{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://prompthero.com/rails/active_storage/representations/proxy/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaEpJaWs0TlROa1pERmlZaTAyTnpNNUxUUXdZakl0T1Raall5MDBPRE0wT1RjMFlXSTBZMkVHT2daRlZBPT0iLCJleHAiOm51bGwsInB1ciI6ImJsb2JfaWQifX0=--ed60df1d7058c4312da9e9da344e6d9671567582/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaDdDRG9MWm05eWJXRjBPZ2wzWldKd09oUnlaWE5wZW1WZmRHOWZiR2x0YVhSYkIya0NBQWd3T2dwellYWmxjbnNKT2hOemRXSnpZVzF3YkdWZmJXOWtaVWtpQjI5dUJqb0dSVlE2Q25OMGNtbHdWRG9PYVc1MFpYSnNZV05sVkRvTWNYVmhiR2wwZVdsZiIsImV4cCI6bnVsbCwicHVyIjoidmFyaWF0aW9uIn19--935666d13f63ed5aca9daa2416340e3a90b6014e/prompthero-prompt-e15d01779e4.png)\n",
    "\n",
    "*Image generated by Stable Diffusion, on my local machine, published on prompthero.*\n",
    "\n",
    "# Building a Neural Network from Scratch in Python\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, we embark on a journey to demystify the inner workings of neural networks by building one from the ground up using Python. Neural networks are the backbone of modern machine learning and artificial intelligence, making this project not only educational but also essential for anyone seeking a deeper understanding of the field.\n",
    "\n",
    "As I'm starting this project for educational purposes and I don't have a strong background on neural networks, I'll be following a video playlist from [sentdex](https://www.youtube.com/@sentdex). I also recommend his [pythonprogramming](https://pythonprogramming.net/) website if you wish to learn python.\n",
    "\n",
    "## Project Goals\n",
    "\n",
    "My primary objectives in this project include:\n",
    "\n",
    "1. **Understanding the Fundamentals:** I'll delve into the core concepts of neural networks, such as neurons, activation functions, forward and backward propagation, and gradient descent.\n",
    "\n",
    "2. **Implementing Key Components:** Building neural networks from scratch will involve creating our own Python classes for layers, loss functions, and optimizers, allowing us to control every aspect of the network.\n",
    "\n",
    "3. **Training a Model:** We will use our homemade neural network to train on a real-world dataset, possibly a simplified version of a popular benchmark dataset like MNIST.\n",
    "\n",
    "4. **Evaluating Performance:** We will assess the model's performance in terms of accuracy, loss, and other relevant metrics.\n",
    "\n",
    "5. **Visualizing the Learning Process:** We aim to create visualizations of how our network learns, which will help us gain insights into the training process.\n",
    "\n",
    "## Technologies Used\n",
    "\n",
    "- **Python:** The primary programming language for this project.\n",
    "- **NumPy:** For numerical operations and matrix calculations.\n",
    "- **Matplotlib:** For data visualization and creating plots.\n",
    "- **Pandas** For data manipulation in tabular format.\n",
    "- **Jupyter Notebook:** For interactive development and documentation.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "The project will be organized into the following main components:\n",
    "\n",
    "- **Data Preparation:** Loading and preprocessing the dataset.\n",
    "- **Neural Network Architecture:** Defining the architecture of our neural network, including the number of layers, neurons, and activation functions.\n",
    "- **Forward and Backward Propagation:** Implementing the forward and backward passes for training the network.\n",
    "- **Loss Functions and Optimizers:** Creating loss functions and optimizers for model training.\n",
    "- **Training and Evaluation:** Training the neural network and evaluating its performance on the dataset.\n",
    "- **Visualizations:** Creating visualizations to gain insights into the model's learning process.\n",
    "\n",
    "## Why Build from Scratch?\n",
    "\n",
    "Building a neural network from scratch provides several benefits:\n",
    "\n",
    "- **Deep Understanding:** You'll gain a deep understanding of the internal workings of neural networks, which is crucial for troubleshooting and optimizing models.\n",
    "- **Flexibility:** You have complete control over every aspect of your model, allowing you to experiment with various architectures and techniques.\n",
    "- **Educational Value:** This project serves as an excellent educational resource for data scientists and aspiring machine learning engineers.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By the end of this project, you will have not only a functional neural network but also a profound understanding of how neural networks operate. You'll be better equipped to tackle more complex machine learning tasks and make informed decisions when working with neural networks in the future.\n",
    "\n",
    "Let's dive into the fascinating world of neural networks and start building one from scratch!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MNIST](https://nvsyashwanth.github.io/machinelearningmaster/assets/images/digitsMNIST/samples.png)\n",
    "\n",
    "# The MNIST Dataset\n",
    "\n",
    "The MNIST dataset is a widely used benchmark dataset in the field of machine learning and computer vision. It consists of a large collection of handwritten digits that are commonly used for training and evaluating machine learning models, particularly in the context of image classification. The MNIST dataset is a popular choice for beginners in deep learning due to its simplicity and accessibility.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The MNIST dataset is composed of the following key characteristics:\n",
    "\n",
    "- **Images:** It contains a total of 70,000 grayscale images of handwritten digits, each measuring 28x28 pixels. These images are represented as 28x28 matrices with pixel values ranging from 0 (black) to 255 (white).\n",
    "\n",
    "- **Labels:** Each image is associated with a label, which corresponds to the digit it represents. The labels are integers from 0 to 9, making MNIST a multi-class classification problem.\n",
    "\n",
    "- **Training and Testing Sets:** The dataset is typically divided into two parts: a training set containing 60,000 images and a testing set with 10,000 images. The training set is used to train machine learning models, while the testing set is used to evaluate their performance.\n",
    "\n",
    "- **Variability:** MNIST exhibits variability in writing styles and quality of handwriting, making it a suitable dataset for assessing the robustness of classification algorithms.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The MNIST dataset serves as a fundamental benchmark for testing the effectiveness of machine learning and deep learning models. It is often used to introduce and evaluate the performance of neural networks, convolutional neural networks (CNNs), and other image classification techniques.\n",
    "\n",
    "## Accessibility\n",
    "\n",
    "The MNIST dataset is publicly available and can be easily accessed and downloaded through various libraries and frameworks in Python, such as TensorFlow and PyTorch. It is also included in popular datasets in machine learning libraries like Scikit-Learn.\n",
    "\n",
    "In this project, we will utilize the MNIST dataset to train our homemade neural network for digit classification. The objective is to achieve a high level of accuracy in recognizing handwritten digits, thus demonstrating the capability of our self-built neural network.\n",
    "\n",
    "Let's start by importing and exploring the MNIST dataset to kick off our project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 12:21:39.568091: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-27 12:21:39.790699: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-27 12:21:39.790717: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-27 12:21:40.718074: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-27 12:21:40.718124: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-27 12:21:40.718129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Importing dataset using train and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 28, 28)\n",
      "Y_train: (60000,)\n",
      "X_test:  (10000, 28, 28)\n",
      "Y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ' + str(train_X.shape))\n",
    "print('Y_train: ' + str(train_y.shape))\n",
    "print('X_test:  '  + str(test_X.shape))\n",
    "print('Y_test:  '  + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_X[0], cmap=plt.get_cmap('gray'))\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_X[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I already have a dataset to test my models, it's time to focus more on how to make everything work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding how Neuron connections work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Basically, what we'll be doing is something like this:\n",
    "inputs = [0, 0, 1, 0, 0] # This would be our input layer aka our dataset = 28 by 28 pixels image.\n",
    "weights = [0.1, 0.4, 2, 0.2, 0] # Every input have a conection to a weight that is adjustable\n",
    "bias = 3 # Another param of the model\n",
    "\n",
    "output2 = inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + inputs[3] * weights[3] + inputs[4] * weights[4] + bias # It'll all resume into this kind of operation\n",
    "\n",
    "# We can also write the above as:\n",
    "output1 = np.dot(weights, inputs) + bias\n",
    "print(output1 == output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is **just a simple neuron with 3 weights and a bias of 3!**\n",
    "\n",
    "Now, lets think of the input about our reality in the mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>126</td>\n",
       "      <td>136</td>\n",
       "      <td>175</td>\n",
       "      <td>26</td>\n",
       "      <td>166</td>\n",
       "      <td>255</td>\n",
       "      <td>247</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>94</td>\n",
       "      <td>154</td>\n",
       "      <td>170</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>225</td>\n",
       "      <td>172</td>\n",
       "      <td>253</td>\n",
       "      <td>242</td>\n",
       "      <td>195</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>238</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>251</td>\n",
       "      <td>93</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>56</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>219</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>198</td>\n",
       "      <td>182</td>\n",
       "      <td>247</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>156</td>\n",
       "      <td>107</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>205</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>253</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>253</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>190</td>\n",
       "      <td>253</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>241</td>\n",
       "      <td>225</td>\n",
       "      <td>160</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>119</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>186</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>150</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>93</td>\n",
       "      <td>252</td>\n",
       "      <td>253</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>253</td>\n",
       "      <td>249</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>130</td>\n",
       "      <td>183</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>207</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>148</td>\n",
       "      <td>229</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>250</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>114</td>\n",
       "      <td>221</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>201</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>66</td>\n",
       "      <td>213</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>198</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>171</td>\n",
       "      <td>219</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>195</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>172</td>\n",
       "      <td>226</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>244</td>\n",
       "      <td>133</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>212</td>\n",
       "      <td>135</td>\n",
       "      <td>132</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3    4    5    6    7    8    9    10   11   12   13   14  \\\n",
       "0    0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "1    0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "2    0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "3    0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "4    0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "5    0   0   0   0    0    0    0    0    0    0    0    0    3   18   18   \n",
       "6    0   0   0   0    0    0    0    0   30   36   94  154  170  253  253   \n",
       "7    0   0   0   0    0    0    0   49  238  253  253  253  253  253  253   \n",
       "8    0   0   0   0    0    0    0   18  219  253  253  253  253  253  198   \n",
       "9    0   0   0   0    0    0    0    0   80  156  107  253  253  205   11   \n",
       "10   0   0   0   0    0    0    0    0    0   14    1  154  253   90    0   \n",
       "11   0   0   0   0    0    0    0    0    0    0    0  139  253  190    2   \n",
       "12   0   0   0   0    0    0    0    0    0    0    0   11  190  253   70   \n",
       "13   0   0   0   0    0    0    0    0    0    0    0    0   35  241  225   \n",
       "14   0   0   0   0    0    0    0    0    0    0    0    0    0   81  240   \n",
       "15   0   0   0   0    0    0    0    0    0    0    0    0    0    0   45   \n",
       "16   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "17   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "18   0   0   0   0    0    0    0    0    0    0    0    0    0    0   46   \n",
       "19   0   0   0   0    0    0    0    0    0    0    0    0   39  148  229   \n",
       "20   0   0   0   0    0    0    0    0    0    0   24  114  221  253  253   \n",
       "21   0   0   0   0    0    0    0    0   23   66  213  253  253  253  253   \n",
       "22   0   0   0   0    0    0   18  171  219  253  253  253  253  195   80   \n",
       "23   0   0   0   0   55  172  226  253  253  253  253  244  133   11    0   \n",
       "24   0   0   0   0  136  253  253  253  212  135  132   16    0    0    0   \n",
       "25   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "26   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "27   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "     15   16   17   18   19   20   21   22   23  24  25  26  27  \n",
       "0     0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "1     0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "2     0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "3     0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "4     0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "5    18  126  136  175   26  166  255  247  127   0   0   0   0  \n",
       "6   253  253  253  225  172  253  242  195   64   0   0   0   0  \n",
       "7   253  253  251   93   82   82   56   39    0   0   0   0   0  \n",
       "8   182  247  241    0    0    0    0    0    0   0   0   0   0  \n",
       "9     0   43  154    0    0    0    0    0    0   0   0   0   0  \n",
       "10    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "11    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "12    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "13  160  108    1    0    0    0    0    0    0   0   0   0   0  \n",
       "14  253  253  119   25    0    0    0    0    0   0   0   0   0  \n",
       "15  186  253  253  150   27    0    0    0    0   0   0   0   0  \n",
       "16   16   93  252  253  187    0    0    0    0   0   0   0   0  \n",
       "17    0    0  249  253  249   64    0    0    0   0   0   0   0  \n",
       "18  130  183  253  253  207    2    0    0    0   0   0   0   0  \n",
       "19  253  253  253  250  182    0    0    0    0   0   0   0   0  \n",
       "20  253  253  201   78    0    0    0    0    0   0   0   0   0  \n",
       "21  198   81    2    0    0    0    0    0    0   0   0   0   0  \n",
       "22    9    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "23    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "24    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "25    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "26    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "27    0    0    0    0    0    0    0    0    0   0   0   0   0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As said before, all inputs are represented by a image, consisting of 28x28 different numbers:\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame(train_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For me, It's now crystal clear what issue some modern models face and why image compressing is so amazing. If I had a higher quality image, my inputs will become more and more complex. I'd have to find a way of representing those images in a more efficient way.\n",
    "\n",
    "But that's definitively a topic for another moment. For this project, I'll just make a functional model with \"arcaic\" techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.449944</td>\n",
       "      <td>-0.279269</td>\n",
       "      <td>0.395052</td>\n",
       "      <td>1.301201</td>\n",
       "      <td>0.762226</td>\n",
       "      <td>0.469672</td>\n",
       "      <td>0.098631</td>\n",
       "      <td>0.834692</td>\n",
       "      <td>0.911249</td>\n",
       "      <td>0.353687</td>\n",
       "      <td>0.929618</td>\n",
       "      <td>-1.048843</td>\n",
       "      <td>-0.187880</td>\n",
       "      <td>0.662278</td>\n",
       "      <td>1.065278</td>\n",
       "      <td>0.075130</td>\n",
       "      <td>0.877792</td>\n",
       "      <td>0.164972</td>\n",
       "      <td>-1.376434</td>\n",
       "      <td>-1.188652</td>\n",
       "      <td>0.044920</td>\n",
       "      <td>1.721831</td>\n",
       "      <td>1.153121</td>\n",
       "      <td>-0.144130</td>\n",
       "      <td>-0.302309</td>\n",
       "      <td>-0.717030</td>\n",
       "      <td>0.141428</td>\n",
       "      <td>-0.155086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.641585</td>\n",
       "      <td>0.319965</td>\n",
       "      <td>0.212015</td>\n",
       "      <td>1.094886</td>\n",
       "      <td>0.062159</td>\n",
       "      <td>1.131337</td>\n",
       "      <td>-0.482603</td>\n",
       "      <td>-2.019054</td>\n",
       "      <td>-0.211842</td>\n",
       "      <td>-0.007306</td>\n",
       "      <td>-0.894274</td>\n",
       "      <td>-0.404592</td>\n",
       "      <td>0.367781</td>\n",
       "      <td>-0.457392</td>\n",
       "      <td>0.314669</td>\n",
       "      <td>-2.082748</td>\n",
       "      <td>-0.094502</td>\n",
       "      <td>0.118080</td>\n",
       "      <td>-1.077732</td>\n",
       "      <td>-0.892119</td>\n",
       "      <td>0.204665</td>\n",
       "      <td>0.621380</td>\n",
       "      <td>0.137087</td>\n",
       "      <td>1.329327</td>\n",
       "      <td>-0.454444</td>\n",
       "      <td>0.193170</td>\n",
       "      <td>1.702389</td>\n",
       "      <td>-0.634235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.502841</td>\n",
       "      <td>-2.245220</td>\n",
       "      <td>0.763748</td>\n",
       "      <td>0.841549</td>\n",
       "      <td>-0.576529</td>\n",
       "      <td>-3.505810</td>\n",
       "      <td>0.836734</td>\n",
       "      <td>-0.608018</td>\n",
       "      <td>1.508059</td>\n",
       "      <td>1.000057</td>\n",
       "      <td>-0.069371</td>\n",
       "      <td>-0.389714</td>\n",
       "      <td>1.161974</td>\n",
       "      <td>0.557201</td>\n",
       "      <td>-0.058146</td>\n",
       "      <td>-0.474881</td>\n",
       "      <td>-0.338424</td>\n",
       "      <td>-0.252586</td>\n",
       "      <td>-2.303380</td>\n",
       "      <td>0.324960</td>\n",
       "      <td>0.671309</td>\n",
       "      <td>1.170797</td>\n",
       "      <td>-0.377535</td>\n",
       "      <td>1.283638</td>\n",
       "      <td>-0.569373</td>\n",
       "      <td>-0.760108</td>\n",
       "      <td>-1.141832</td>\n",
       "      <td>-0.097819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.510024</td>\n",
       "      <td>-1.083654</td>\n",
       "      <td>-1.270947</td>\n",
       "      <td>1.097246</td>\n",
       "      <td>0.273326</td>\n",
       "      <td>-1.023430</td>\n",
       "      <td>-0.232607</td>\n",
       "      <td>-0.149854</td>\n",
       "      <td>0.789136</td>\n",
       "      <td>-0.231188</td>\n",
       "      <td>-0.715735</td>\n",
       "      <td>-0.694082</td>\n",
       "      <td>0.539043</td>\n",
       "      <td>0.602296</td>\n",
       "      <td>0.575306</td>\n",
       "      <td>0.993678</td>\n",
       "      <td>-0.817655</td>\n",
       "      <td>-0.977266</td>\n",
       "      <td>-0.914720</td>\n",
       "      <td>0.366452</td>\n",
       "      <td>-0.940804</td>\n",
       "      <td>0.238042</td>\n",
       "      <td>-0.730606</td>\n",
       "      <td>0.419769</td>\n",
       "      <td>1.186127</td>\n",
       "      <td>0.082407</td>\n",
       "      <td>0.579095</td>\n",
       "      <td>-1.854079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.709381</td>\n",
       "      <td>-0.147959</td>\n",
       "      <td>-1.327209</td>\n",
       "      <td>1.802710</td>\n",
       "      <td>0.304020</td>\n",
       "      <td>0.866459</td>\n",
       "      <td>0.785227</td>\n",
       "      <td>0.903580</td>\n",
       "      <td>-1.732886</td>\n",
       "      <td>1.329472</td>\n",
       "      <td>1.656581</td>\n",
       "      <td>2.159669</td>\n",
       "      <td>-1.757224</td>\n",
       "      <td>0.258530</td>\n",
       "      <td>-1.845509</td>\n",
       "      <td>1.286459</td>\n",
       "      <td>-0.071101</td>\n",
       "      <td>1.126901</td>\n",
       "      <td>-0.041485</td>\n",
       "      <td>-0.690658</td>\n",
       "      <td>0.181141</td>\n",
       "      <td>-0.103857</td>\n",
       "      <td>0.424034</td>\n",
       "      <td>-1.134576</td>\n",
       "      <td>-0.327464</td>\n",
       "      <td>1.159003</td>\n",
       "      <td>0.155672</td>\n",
       "      <td>0.720677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.388106</td>\n",
       "      <td>0.559984</td>\n",
       "      <td>-0.347006</td>\n",
       "      <td>-0.411572</td>\n",
       "      <td>-0.681675</td>\n",
       "      <td>0.531132</td>\n",
       "      <td>0.474848</td>\n",
       "      <td>-0.269613</td>\n",
       "      <td>-1.396361</td>\n",
       "      <td>0.764505</td>\n",
       "      <td>0.094346</td>\n",
       "      <td>-0.745357</td>\n",
       "      <td>0.672427</td>\n",
       "      <td>-1.156287</td>\n",
       "      <td>-0.663436</td>\n",
       "      <td>-1.070311</td>\n",
       "      <td>-1.469880</td>\n",
       "      <td>0.224751</td>\n",
       "      <td>-0.973513</td>\n",
       "      <td>-0.135967</td>\n",
       "      <td>-1.463574</td>\n",
       "      <td>-1.371264</td>\n",
       "      <td>1.268598</td>\n",
       "      <td>-0.798368</td>\n",
       "      <td>1.181123</td>\n",
       "      <td>-0.138578</td>\n",
       "      <td>-0.065409</td>\n",
       "      <td>0.102005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.713463</td>\n",
       "      <td>0.233821</td>\n",
       "      <td>-0.723719</td>\n",
       "      <td>1.552647</td>\n",
       "      <td>-0.760754</td>\n",
       "      <td>1.165866</td>\n",
       "      <td>-1.564368</td>\n",
       "      <td>-0.208207</td>\n",
       "      <td>0.144495</td>\n",
       "      <td>-0.760472</td>\n",
       "      <td>1.343640</td>\n",
       "      <td>-0.194530</td>\n",
       "      <td>-1.276125</td>\n",
       "      <td>-0.332400</td>\n",
       "      <td>1.399900</td>\n",
       "      <td>0.340064</td>\n",
       "      <td>0.298843</td>\n",
       "      <td>-0.696021</td>\n",
       "      <td>-0.429092</td>\n",
       "      <td>0.952773</td>\n",
       "      <td>-2.162159</td>\n",
       "      <td>-0.635005</td>\n",
       "      <td>0.974513</td>\n",
       "      <td>-1.760587</td>\n",
       "      <td>2.240196</td>\n",
       "      <td>0.558241</td>\n",
       "      <td>-0.898130</td>\n",
       "      <td>-0.191293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.685679</td>\n",
       "      <td>0.018153</td>\n",
       "      <td>0.528188</td>\n",
       "      <td>-0.217295</td>\n",
       "      <td>-0.572010</td>\n",
       "      <td>-0.374388</td>\n",
       "      <td>1.842096</td>\n",
       "      <td>-1.761880</td>\n",
       "      <td>-0.747480</td>\n",
       "      <td>-0.140207</td>\n",
       "      <td>-0.206285</td>\n",
       "      <td>-0.886563</td>\n",
       "      <td>-1.099420</td>\n",
       "      <td>1.723384</td>\n",
       "      <td>-0.637163</td>\n",
       "      <td>-1.981298</td>\n",
       "      <td>-0.551982</td>\n",
       "      <td>1.502743</td>\n",
       "      <td>1.869490</td>\n",
       "      <td>0.699687</td>\n",
       "      <td>-0.685399</td>\n",
       "      <td>-0.645577</td>\n",
       "      <td>-2.001380</td>\n",
       "      <td>0.942395</td>\n",
       "      <td>0.627486</td>\n",
       "      <td>1.323623</td>\n",
       "      <td>-0.610883</td>\n",
       "      <td>-0.036616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.654481</td>\n",
       "      <td>0.829764</td>\n",
       "      <td>0.562518</td>\n",
       "      <td>0.343844</td>\n",
       "      <td>-0.129300</td>\n",
       "      <td>-2.106882</td>\n",
       "      <td>0.218494</td>\n",
       "      <td>-1.366817</td>\n",
       "      <td>-0.159712</td>\n",
       "      <td>0.191926</td>\n",
       "      <td>-0.419050</td>\n",
       "      <td>0.118295</td>\n",
       "      <td>-0.819277</td>\n",
       "      <td>-1.140844</td>\n",
       "      <td>0.453219</td>\n",
       "      <td>-0.122616</td>\n",
       "      <td>-0.064203</td>\n",
       "      <td>-1.193752</td>\n",
       "      <td>0.143687</td>\n",
       "      <td>1.206738</td>\n",
       "      <td>1.491497</td>\n",
       "      <td>-2.145219</td>\n",
       "      <td>-0.466879</td>\n",
       "      <td>-0.525107</td>\n",
       "      <td>-0.637520</td>\n",
       "      <td>0.538583</td>\n",
       "      <td>1.769654</td>\n",
       "      <td>-0.146022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.369866</td>\n",
       "      <td>-0.490473</td>\n",
       "      <td>0.643253</td>\n",
       "      <td>-1.030094</td>\n",
       "      <td>-0.627874</td>\n",
       "      <td>-0.872387</td>\n",
       "      <td>0.668128</td>\n",
       "      <td>-0.295445</td>\n",
       "      <td>0.651347</td>\n",
       "      <td>-1.140522</td>\n",
       "      <td>0.121330</td>\n",
       "      <td>0.791913</td>\n",
       "      <td>0.844017</td>\n",
       "      <td>-0.308645</td>\n",
       "      <td>-0.415986</td>\n",
       "      <td>-0.349435</td>\n",
       "      <td>0.099865</td>\n",
       "      <td>-0.415048</td>\n",
       "      <td>0.663897</td>\n",
       "      <td>0.415078</td>\n",
       "      <td>-0.036216</td>\n",
       "      <td>0.188175</td>\n",
       "      <td>-0.047814</td>\n",
       "      <td>0.154670</td>\n",
       "      <td>-0.244221</td>\n",
       "      <td>-0.610407</td>\n",
       "      <td>-0.971988</td>\n",
       "      <td>-0.147728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.868133</td>\n",
       "      <td>0.704426</td>\n",
       "      <td>1.502596</td>\n",
       "      <td>-0.330420</td>\n",
       "      <td>-0.704827</td>\n",
       "      <td>-0.736782</td>\n",
       "      <td>-0.798681</td>\n",
       "      <td>-0.388239</td>\n",
       "      <td>0.936669</td>\n",
       "      <td>0.277927</td>\n",
       "      <td>-1.174798</td>\n",
       "      <td>-0.271890</td>\n",
       "      <td>-0.372675</td>\n",
       "      <td>-0.430632</td>\n",
       "      <td>-0.760700</td>\n",
       "      <td>1.806143</td>\n",
       "      <td>-1.215140</td>\n",
       "      <td>-0.260909</td>\n",
       "      <td>-0.034947</td>\n",
       "      <td>-0.821666</td>\n",
       "      <td>0.363105</td>\n",
       "      <td>1.163359</td>\n",
       "      <td>1.565486</td>\n",
       "      <td>-1.216381</td>\n",
       "      <td>1.586885</td>\n",
       "      <td>1.476563</td>\n",
       "      <td>0.883148</td>\n",
       "      <td>-0.727415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.782045</td>\n",
       "      <td>0.249559</td>\n",
       "      <td>1.446206</td>\n",
       "      <td>1.260611</td>\n",
       "      <td>0.271425</td>\n",
       "      <td>-0.644306</td>\n",
       "      <td>-2.519742</td>\n",
       "      <td>0.133151</td>\n",
       "      <td>-0.563283</td>\n",
       "      <td>0.900099</td>\n",
       "      <td>1.184945</td>\n",
       "      <td>0.983953</td>\n",
       "      <td>-1.981047</td>\n",
       "      <td>0.027499</td>\n",
       "      <td>-0.667246</td>\n",
       "      <td>-2.117161</td>\n",
       "      <td>1.176999</td>\n",
       "      <td>0.411311</td>\n",
       "      <td>-0.366225</td>\n",
       "      <td>-1.400194</td>\n",
       "      <td>0.705921</td>\n",
       "      <td>1.706307</td>\n",
       "      <td>-0.161502</td>\n",
       "      <td>0.174904</td>\n",
       "      <td>-0.599232</td>\n",
       "      <td>-0.526139</td>\n",
       "      <td>-0.840112</td>\n",
       "      <td>-0.606175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.372049</td>\n",
       "      <td>0.863695</td>\n",
       "      <td>0.096357</td>\n",
       "      <td>-1.199094</td>\n",
       "      <td>0.429551</td>\n",
       "      <td>-0.953087</td>\n",
       "      <td>2.323257</td>\n",
       "      <td>0.438644</td>\n",
       "      <td>0.850238</td>\n",
       "      <td>0.642310</td>\n",
       "      <td>-1.943792</td>\n",
       "      <td>-0.477252</td>\n",
       "      <td>-2.395026</td>\n",
       "      <td>3.727112</td>\n",
       "      <td>-0.349052</td>\n",
       "      <td>-0.189351</td>\n",
       "      <td>-0.671965</td>\n",
       "      <td>-0.713809</td>\n",
       "      <td>-0.858651</td>\n",
       "      <td>-0.215227</td>\n",
       "      <td>2.313412</td>\n",
       "      <td>0.922428</td>\n",
       "      <td>1.762308</td>\n",
       "      <td>-0.821442</td>\n",
       "      <td>-2.177231</td>\n",
       "      <td>-1.226803</td>\n",
       "      <td>-0.231533</td>\n",
       "      <td>0.813056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.128504</td>\n",
       "      <td>-0.575379</td>\n",
       "      <td>1.562410</td>\n",
       "      <td>-1.651791</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.427248</td>\n",
       "      <td>-0.403173</td>\n",
       "      <td>-1.336188</td>\n",
       "      <td>0.823418</td>\n",
       "      <td>-0.087102</td>\n",
       "      <td>-0.416537</td>\n",
       "      <td>-0.370387</td>\n",
       "      <td>1.136805</td>\n",
       "      <td>-0.666180</td>\n",
       "      <td>-0.543110</td>\n",
       "      <td>-0.247207</td>\n",
       "      <td>0.427739</td>\n",
       "      <td>1.809831</td>\n",
       "      <td>1.389047</td>\n",
       "      <td>-1.509132</td>\n",
       "      <td>-0.725824</td>\n",
       "      <td>-1.427974</td>\n",
       "      <td>1.444601</td>\n",
       "      <td>0.446701</td>\n",
       "      <td>0.437166</td>\n",
       "      <td>-1.558811</td>\n",
       "      <td>0.325451</td>\n",
       "      <td>-0.873876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.177004</td>\n",
       "      <td>-0.252992</td>\n",
       "      <td>-1.471513</td>\n",
       "      <td>0.641136</td>\n",
       "      <td>-0.609810</td>\n",
       "      <td>-0.057236</td>\n",
       "      <td>-0.672596</td>\n",
       "      <td>-1.834488</td>\n",
       "      <td>0.284761</td>\n",
       "      <td>-0.159169</td>\n",
       "      <td>-0.549312</td>\n",
       "      <td>0.386428</td>\n",
       "      <td>2.019062</td>\n",
       "      <td>0.440660</td>\n",
       "      <td>-1.528360</td>\n",
       "      <td>-1.749939</td>\n",
       "      <td>1.411304</td>\n",
       "      <td>-0.219687</td>\n",
       "      <td>-0.598414</td>\n",
       "      <td>0.043126</td>\n",
       "      <td>-1.595024</td>\n",
       "      <td>-0.084330</td>\n",
       "      <td>-0.147841</td>\n",
       "      <td>-1.185926</td>\n",
       "      <td>-1.063723</td>\n",
       "      <td>0.418060</td>\n",
       "      <td>-0.404967</td>\n",
       "      <td>1.722885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.439413</td>\n",
       "      <td>-0.189354</td>\n",
       "      <td>0.569085</td>\n",
       "      <td>-1.012610</td>\n",
       "      <td>1.082596</td>\n",
       "      <td>0.166017</td>\n",
       "      <td>-0.490193</td>\n",
       "      <td>-2.715622</td>\n",
       "      <td>0.227985</td>\n",
       "      <td>-0.866727</td>\n",
       "      <td>0.021830</td>\n",
       "      <td>1.168356</td>\n",
       "      <td>-1.085184</td>\n",
       "      <td>-0.228109</td>\n",
       "      <td>0.849697</td>\n",
       "      <td>1.583984</td>\n",
       "      <td>0.483241</td>\n",
       "      <td>0.069227</td>\n",
       "      <td>-1.644612</td>\n",
       "      <td>1.083785</td>\n",
       "      <td>-0.264040</td>\n",
       "      <td>-1.189069</td>\n",
       "      <td>0.180464</td>\n",
       "      <td>1.022450</td>\n",
       "      <td>1.075772</td>\n",
       "      <td>0.444104</td>\n",
       "      <td>0.117216</td>\n",
       "      <td>-0.075613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.069430</td>\n",
       "      <td>-0.719419</td>\n",
       "      <td>-0.270607</td>\n",
       "      <td>-0.662052</td>\n",
       "      <td>0.357073</td>\n",
       "      <td>-0.106909</td>\n",
       "      <td>-0.040551</td>\n",
       "      <td>-1.600108</td>\n",
       "      <td>0.407303</td>\n",
       "      <td>-1.078032</td>\n",
       "      <td>-0.035810</td>\n",
       "      <td>0.491723</td>\n",
       "      <td>0.796123</td>\n",
       "      <td>-0.693977</td>\n",
       "      <td>-0.817091</td>\n",
       "      <td>0.098535</td>\n",
       "      <td>-1.621413</td>\n",
       "      <td>-1.103880</td>\n",
       "      <td>1.061463</td>\n",
       "      <td>-2.537070</td>\n",
       "      <td>-0.596982</td>\n",
       "      <td>0.619368</td>\n",
       "      <td>0.737780</td>\n",
       "      <td>-0.043117</td>\n",
       "      <td>2.083177</td>\n",
       "      <td>-0.153688</td>\n",
       "      <td>-0.432864</td>\n",
       "      <td>-0.604908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.052780</td>\n",
       "      <td>-0.792096</td>\n",
       "      <td>1.320846</td>\n",
       "      <td>1.037823</td>\n",
       "      <td>0.266319</td>\n",
       "      <td>1.417459</td>\n",
       "      <td>1.351951</td>\n",
       "      <td>-1.585222</td>\n",
       "      <td>-0.397928</td>\n",
       "      <td>-0.474471</td>\n",
       "      <td>-0.126308</td>\n",
       "      <td>-1.800075</td>\n",
       "      <td>0.614215</td>\n",
       "      <td>1.204032</td>\n",
       "      <td>-0.542056</td>\n",
       "      <td>-1.481645</td>\n",
       "      <td>1.771256</td>\n",
       "      <td>-0.545737</td>\n",
       "      <td>-0.352742</td>\n",
       "      <td>0.121251</td>\n",
       "      <td>-0.006372</td>\n",
       "      <td>0.237486</td>\n",
       "      <td>-1.643921</td>\n",
       "      <td>-0.804570</td>\n",
       "      <td>0.288701</td>\n",
       "      <td>0.421912</td>\n",
       "      <td>-1.227129</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.705381</td>\n",
       "      <td>2.004130</td>\n",
       "      <td>0.728552</td>\n",
       "      <td>-0.462095</td>\n",
       "      <td>-1.450978</td>\n",
       "      <td>0.713055</td>\n",
       "      <td>-0.101708</td>\n",
       "      <td>0.051697</td>\n",
       "      <td>-0.608939</td>\n",
       "      <td>0.645365</td>\n",
       "      <td>-0.574675</td>\n",
       "      <td>-0.922041</td>\n",
       "      <td>0.577357</td>\n",
       "      <td>0.183637</td>\n",
       "      <td>0.478682</td>\n",
       "      <td>-0.600566</td>\n",
       "      <td>-0.739154</td>\n",
       "      <td>1.541035</td>\n",
       "      <td>0.060323</td>\n",
       "      <td>-1.311510</td>\n",
       "      <td>0.984617</td>\n",
       "      <td>1.376215</td>\n",
       "      <td>0.399963</td>\n",
       "      <td>1.929092</td>\n",
       "      <td>-1.364943</td>\n",
       "      <td>-0.164033</td>\n",
       "      <td>-0.287088</td>\n",
       "      <td>0.583896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.695346</td>\n",
       "      <td>-0.174817</td>\n",
       "      <td>1.742120</td>\n",
       "      <td>-0.824529</td>\n",
       "      <td>-0.999778</td>\n",
       "      <td>-0.357111</td>\n",
       "      <td>-0.589318</td>\n",
       "      <td>1.276822</td>\n",
       "      <td>-0.943937</td>\n",
       "      <td>1.847701</td>\n",
       "      <td>0.079550</td>\n",
       "      <td>-0.441492</td>\n",
       "      <td>-0.034094</td>\n",
       "      <td>1.749696</td>\n",
       "      <td>1.115139</td>\n",
       "      <td>-1.983437</td>\n",
       "      <td>-1.108329</td>\n",
       "      <td>0.811892</td>\n",
       "      <td>0.345909</td>\n",
       "      <td>1.387534</td>\n",
       "      <td>0.489247</td>\n",
       "      <td>0.220936</td>\n",
       "      <td>0.324788</td>\n",
       "      <td>1.473250</td>\n",
       "      <td>0.766090</td>\n",
       "      <td>-1.364736</td>\n",
       "      <td>-0.124745</td>\n",
       "      <td>1.831150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.439853</td>\n",
       "      <td>0.389585</td>\n",
       "      <td>-1.482478</td>\n",
       "      <td>1.245913</td>\n",
       "      <td>-0.022340</td>\n",
       "      <td>-1.253345</td>\n",
       "      <td>0.447724</td>\n",
       "      <td>-0.721842</td>\n",
       "      <td>-0.506012</td>\n",
       "      <td>-0.236976</td>\n",
       "      <td>1.430671</td>\n",
       "      <td>0.354459</td>\n",
       "      <td>-1.037301</td>\n",
       "      <td>-0.248292</td>\n",
       "      <td>0.748104</td>\n",
       "      <td>-0.237157</td>\n",
       "      <td>0.334644</td>\n",
       "      <td>-1.151724</td>\n",
       "      <td>0.633037</td>\n",
       "      <td>0.133834</td>\n",
       "      <td>0.964140</td>\n",
       "      <td>0.339075</td>\n",
       "      <td>-0.645803</td>\n",
       "      <td>-0.031706</td>\n",
       "      <td>-0.149283</td>\n",
       "      <td>0.592568</td>\n",
       "      <td>-0.083030</td>\n",
       "      <td>-0.050281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.180272</td>\n",
       "      <td>-1.729927</td>\n",
       "      <td>-1.305057</td>\n",
       "      <td>-0.420737</td>\n",
       "      <td>-1.788589</td>\n",
       "      <td>-1.233307</td>\n",
       "      <td>-0.645265</td>\n",
       "      <td>-1.794381</td>\n",
       "      <td>2.211670</td>\n",
       "      <td>-2.541121</td>\n",
       "      <td>0.394257</td>\n",
       "      <td>0.321631</td>\n",
       "      <td>1.453002</td>\n",
       "      <td>-0.762444</td>\n",
       "      <td>1.060348</td>\n",
       "      <td>-0.652417</td>\n",
       "      <td>0.272714</td>\n",
       "      <td>-0.972853</td>\n",
       "      <td>0.874940</td>\n",
       "      <td>-0.376700</td>\n",
       "      <td>-0.945134</td>\n",
       "      <td>-0.841013</td>\n",
       "      <td>0.565069</td>\n",
       "      <td>-1.266691</td>\n",
       "      <td>0.121152</td>\n",
       "      <td>-1.171481</td>\n",
       "      <td>0.724091</td>\n",
       "      <td>1.019971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.162610</td>\n",
       "      <td>-0.308090</td>\n",
       "      <td>0.247719</td>\n",
       "      <td>-0.048617</td>\n",
       "      <td>-0.320190</td>\n",
       "      <td>-1.624337</td>\n",
       "      <td>1.081114</td>\n",
       "      <td>2.146048</td>\n",
       "      <td>-1.388395</td>\n",
       "      <td>-0.795678</td>\n",
       "      <td>-0.523244</td>\n",
       "      <td>-1.091368</td>\n",
       "      <td>-1.799993</td>\n",
       "      <td>-0.269092</td>\n",
       "      <td>2.188903</td>\n",
       "      <td>-0.918486</td>\n",
       "      <td>-0.616139</td>\n",
       "      <td>1.468503</td>\n",
       "      <td>1.438960</td>\n",
       "      <td>0.493958</td>\n",
       "      <td>-1.460419</td>\n",
       "      <td>-3.388916</td>\n",
       "      <td>0.253746</td>\n",
       "      <td>0.745286</td>\n",
       "      <td>-2.196119</td>\n",
       "      <td>0.333600</td>\n",
       "      <td>1.303926</td>\n",
       "      <td>-0.750414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.998638</td>\n",
       "      <td>1.042916</td>\n",
       "      <td>0.672366</td>\n",
       "      <td>-0.380569</td>\n",
       "      <td>0.741670</td>\n",
       "      <td>-0.517237</td>\n",
       "      <td>-0.736108</td>\n",
       "      <td>-0.044825</td>\n",
       "      <td>-0.191660</td>\n",
       "      <td>-1.695336</td>\n",
       "      <td>0.498145</td>\n",
       "      <td>-0.044262</td>\n",
       "      <td>-0.846778</td>\n",
       "      <td>0.873378</td>\n",
       "      <td>0.398197</td>\n",
       "      <td>0.776083</td>\n",
       "      <td>-1.016593</td>\n",
       "      <td>-0.823698</td>\n",
       "      <td>-0.225134</td>\n",
       "      <td>1.044066</td>\n",
       "      <td>-0.129935</td>\n",
       "      <td>1.011525</td>\n",
       "      <td>0.306917</td>\n",
       "      <td>0.051016</td>\n",
       "      <td>-1.350450</td>\n",
       "      <td>1.499876</td>\n",
       "      <td>1.213555</td>\n",
       "      <td>-2.615243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.301350</td>\n",
       "      <td>0.245717</td>\n",
       "      <td>0.316632</td>\n",
       "      <td>-0.485133</td>\n",
       "      <td>0.663471</td>\n",
       "      <td>0.940989</td>\n",
       "      <td>0.783259</td>\n",
       "      <td>0.499568</td>\n",
       "      <td>2.720175</td>\n",
       "      <td>0.962654</td>\n",
       "      <td>0.329592</td>\n",
       "      <td>1.181477</td>\n",
       "      <td>0.918639</td>\n",
       "      <td>1.681263</td>\n",
       "      <td>1.428473</td>\n",
       "      <td>-0.238602</td>\n",
       "      <td>1.303705</td>\n",
       "      <td>-0.832590</td>\n",
       "      <td>-0.021987</td>\n",
       "      <td>1.117420</td>\n",
       "      <td>-0.352614</td>\n",
       "      <td>-0.137850</td>\n",
       "      <td>-0.602607</td>\n",
       "      <td>-2.419485</td>\n",
       "      <td>0.920276</td>\n",
       "      <td>-1.128594</td>\n",
       "      <td>-0.045118</td>\n",
       "      <td>1.712395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.635234</td>\n",
       "      <td>-0.782012</td>\n",
       "      <td>-0.280645</td>\n",
       "      <td>0.868488</td>\n",
       "      <td>0.854170</td>\n",
       "      <td>0.536594</td>\n",
       "      <td>-1.101759</td>\n",
       "      <td>0.370290</td>\n",
       "      <td>0.983375</td>\n",
       "      <td>0.444872</td>\n",
       "      <td>-0.235834</td>\n",
       "      <td>-0.681621</td>\n",
       "      <td>1.120452</td>\n",
       "      <td>-0.130079</td>\n",
       "      <td>0.291046</td>\n",
       "      <td>0.483265</td>\n",
       "      <td>-1.504844</td>\n",
       "      <td>0.338384</td>\n",
       "      <td>0.304144</td>\n",
       "      <td>-1.566162</td>\n",
       "      <td>-0.877833</td>\n",
       "      <td>-0.602804</td>\n",
       "      <td>-1.492816</td>\n",
       "      <td>-0.701871</td>\n",
       "      <td>-0.282561</td>\n",
       "      <td>-0.176017</td>\n",
       "      <td>-0.100225</td>\n",
       "      <td>-1.495856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.697137</td>\n",
       "      <td>0.870168</td>\n",
       "      <td>-0.363596</td>\n",
       "      <td>-0.430782</td>\n",
       "      <td>1.259144</td>\n",
       "      <td>1.500728</td>\n",
       "      <td>0.335483</td>\n",
       "      <td>-0.226665</td>\n",
       "      <td>1.017423</td>\n",
       "      <td>1.499242</td>\n",
       "      <td>0.169005</td>\n",
       "      <td>0.459365</td>\n",
       "      <td>-0.323489</td>\n",
       "      <td>-0.020706</td>\n",
       "      <td>-1.532835</td>\n",
       "      <td>1.026299</td>\n",
       "      <td>-0.003073</td>\n",
       "      <td>0.309495</td>\n",
       "      <td>-0.824517</td>\n",
       "      <td>0.761319</td>\n",
       "      <td>-1.978394</td>\n",
       "      <td>-1.256146</td>\n",
       "      <td>0.632199</td>\n",
       "      <td>0.409341</td>\n",
       "      <td>-0.833629</td>\n",
       "      <td>-0.682285</td>\n",
       "      <td>-0.318656</td>\n",
       "      <td>-0.752472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.883384</td>\n",
       "      <td>-0.799098</td>\n",
       "      <td>2.175896</td>\n",
       "      <td>0.619733</td>\n",
       "      <td>0.360361</td>\n",
       "      <td>1.240595</td>\n",
       "      <td>-2.027025</td>\n",
       "      <td>2.253597</td>\n",
       "      <td>-1.285287</td>\n",
       "      <td>-3.147054</td>\n",
       "      <td>-0.429824</td>\n",
       "      <td>0.958210</td>\n",
       "      <td>-0.187602</td>\n",
       "      <td>-1.030969</td>\n",
       "      <td>0.956861</td>\n",
       "      <td>-2.206222</td>\n",
       "      <td>0.175713</td>\n",
       "      <td>0.450390</td>\n",
       "      <td>-1.110367</td>\n",
       "      <td>-0.683326</td>\n",
       "      <td>-1.462934</td>\n",
       "      <td>-0.417872</td>\n",
       "      <td>1.254282</td>\n",
       "      <td>0.488594</td>\n",
       "      <td>0.178059</td>\n",
       "      <td>-1.998848</td>\n",
       "      <td>-1.376605</td>\n",
       "      <td>-0.087610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0  -0.449944 -0.279269  0.395052  1.301201  0.762226  0.469672  0.098631   \n",
       "1   1.641585  0.319965  0.212015  1.094886  0.062159  1.131337 -0.482603   \n",
       "2  -0.502841 -2.245220  0.763748  0.841549 -0.576529 -3.505810  0.836734   \n",
       "3   0.510024 -1.083654 -1.270947  1.097246  0.273326 -1.023430 -0.232607   \n",
       "4  -1.709381 -0.147959 -1.327209  1.802710  0.304020  0.866459  0.785227   \n",
       "5  -0.388106  0.559984 -0.347006 -0.411572 -0.681675  0.531132  0.474848   \n",
       "6  -0.713463  0.233821 -0.723719  1.552647 -0.760754  1.165866 -1.564368   \n",
       "7  -0.685679  0.018153  0.528188 -0.217295 -0.572010 -0.374388  1.842096   \n",
       "8   0.654481  0.829764  0.562518  0.343844 -0.129300 -2.106882  0.218494   \n",
       "9   1.369866 -0.490473  0.643253 -1.030094 -0.627874 -0.872387  0.668128   \n",
       "10  0.868133  0.704426  1.502596 -0.330420 -0.704827 -0.736782 -0.798681   \n",
       "11 -0.782045  0.249559  1.446206  1.260611  0.271425 -0.644306 -2.519742   \n",
       "12 -1.372049  0.863695  0.096357 -1.199094  0.429551 -0.953087  2.323257   \n",
       "13 -0.128504 -0.575379  1.562410 -1.651791  0.970370  0.427248 -0.403173   \n",
       "14  0.177004 -0.252992 -1.471513  0.641136 -0.609810 -0.057236 -0.672596   \n",
       "15 -1.439413 -0.189354  0.569085 -1.012610  1.082596  0.166017 -0.490193   \n",
       "16 -0.069430 -0.719419 -0.270607 -0.662052  0.357073 -0.106909 -0.040551   \n",
       "17 -0.052780 -0.792096  1.320846  1.037823  0.266319  1.417459  1.351951   \n",
       "18 -1.705381  2.004130  0.728552 -0.462095 -1.450978  0.713055 -0.101708   \n",
       "19  0.695346 -0.174817  1.742120 -0.824529 -0.999778 -0.357111 -0.589318   \n",
       "20  1.439853  0.389585 -1.482478  1.245913 -0.022340 -1.253345  0.447724   \n",
       "21  1.180272 -1.729927 -1.305057 -0.420737 -1.788589 -1.233307 -0.645265   \n",
       "22 -0.162610 -0.308090  0.247719 -0.048617 -0.320190 -1.624337  1.081114   \n",
       "23 -1.998638  1.042916  0.672366 -0.380569  0.741670 -0.517237 -0.736108   \n",
       "24 -0.301350  0.245717  0.316632 -0.485133  0.663471  0.940989  0.783259   \n",
       "25 -0.635234 -0.782012 -0.280645  0.868488  0.854170  0.536594 -1.101759   \n",
       "26  0.697137  0.870168 -0.363596 -0.430782  1.259144  1.500728  0.335483   \n",
       "27  0.883384 -0.799098  2.175896  0.619733  0.360361  1.240595 -2.027025   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0   0.834692  0.911249  0.353687  0.929618 -1.048843 -0.187880  0.662278   \n",
       "1  -2.019054 -0.211842 -0.007306 -0.894274 -0.404592  0.367781 -0.457392   \n",
       "2  -0.608018  1.508059  1.000057 -0.069371 -0.389714  1.161974  0.557201   \n",
       "3  -0.149854  0.789136 -0.231188 -0.715735 -0.694082  0.539043  0.602296   \n",
       "4   0.903580 -1.732886  1.329472  1.656581  2.159669 -1.757224  0.258530   \n",
       "5  -0.269613 -1.396361  0.764505  0.094346 -0.745357  0.672427 -1.156287   \n",
       "6  -0.208207  0.144495 -0.760472  1.343640 -0.194530 -1.276125 -0.332400   \n",
       "7  -1.761880 -0.747480 -0.140207 -0.206285 -0.886563 -1.099420  1.723384   \n",
       "8  -1.366817 -0.159712  0.191926 -0.419050  0.118295 -0.819277 -1.140844   \n",
       "9  -0.295445  0.651347 -1.140522  0.121330  0.791913  0.844017 -0.308645   \n",
       "10 -0.388239  0.936669  0.277927 -1.174798 -0.271890 -0.372675 -0.430632   \n",
       "11  0.133151 -0.563283  0.900099  1.184945  0.983953 -1.981047  0.027499   \n",
       "12  0.438644  0.850238  0.642310 -1.943792 -0.477252 -2.395026  3.727112   \n",
       "13 -1.336188  0.823418 -0.087102 -0.416537 -0.370387  1.136805 -0.666180   \n",
       "14 -1.834488  0.284761 -0.159169 -0.549312  0.386428  2.019062  0.440660   \n",
       "15 -2.715622  0.227985 -0.866727  0.021830  1.168356 -1.085184 -0.228109   \n",
       "16 -1.600108  0.407303 -1.078032 -0.035810  0.491723  0.796123 -0.693977   \n",
       "17 -1.585222 -0.397928 -0.474471 -0.126308 -1.800075  0.614215  1.204032   \n",
       "18  0.051697 -0.608939  0.645365 -0.574675 -0.922041  0.577357  0.183637   \n",
       "19  1.276822 -0.943937  1.847701  0.079550 -0.441492 -0.034094  1.749696   \n",
       "20 -0.721842 -0.506012 -0.236976  1.430671  0.354459 -1.037301 -0.248292   \n",
       "21 -1.794381  2.211670 -2.541121  0.394257  0.321631  1.453002 -0.762444   \n",
       "22  2.146048 -1.388395 -0.795678 -0.523244 -1.091368 -1.799993 -0.269092   \n",
       "23 -0.044825 -0.191660 -1.695336  0.498145 -0.044262 -0.846778  0.873378   \n",
       "24  0.499568  2.720175  0.962654  0.329592  1.181477  0.918639  1.681263   \n",
       "25  0.370290  0.983375  0.444872 -0.235834 -0.681621  1.120452 -0.130079   \n",
       "26 -0.226665  1.017423  1.499242  0.169005  0.459365 -0.323489 -0.020706   \n",
       "27  2.253597 -1.285287 -3.147054 -0.429824  0.958210 -0.187602 -1.030969   \n",
       "\n",
       "          14        15        16        17        18        19        20  \\\n",
       "0   1.065278  0.075130  0.877792  0.164972 -1.376434 -1.188652  0.044920   \n",
       "1   0.314669 -2.082748 -0.094502  0.118080 -1.077732 -0.892119  0.204665   \n",
       "2  -0.058146 -0.474881 -0.338424 -0.252586 -2.303380  0.324960  0.671309   \n",
       "3   0.575306  0.993678 -0.817655 -0.977266 -0.914720  0.366452 -0.940804   \n",
       "4  -1.845509  1.286459 -0.071101  1.126901 -0.041485 -0.690658  0.181141   \n",
       "5  -0.663436 -1.070311 -1.469880  0.224751 -0.973513 -0.135967 -1.463574   \n",
       "6   1.399900  0.340064  0.298843 -0.696021 -0.429092  0.952773 -2.162159   \n",
       "7  -0.637163 -1.981298 -0.551982  1.502743  1.869490  0.699687 -0.685399   \n",
       "8   0.453219 -0.122616 -0.064203 -1.193752  0.143687  1.206738  1.491497   \n",
       "9  -0.415986 -0.349435  0.099865 -0.415048  0.663897  0.415078 -0.036216   \n",
       "10 -0.760700  1.806143 -1.215140 -0.260909 -0.034947 -0.821666  0.363105   \n",
       "11 -0.667246 -2.117161  1.176999  0.411311 -0.366225 -1.400194  0.705921   \n",
       "12 -0.349052 -0.189351 -0.671965 -0.713809 -0.858651 -0.215227  2.313412   \n",
       "13 -0.543110 -0.247207  0.427739  1.809831  1.389047 -1.509132 -0.725824   \n",
       "14 -1.528360 -1.749939  1.411304 -0.219687 -0.598414  0.043126 -1.595024   \n",
       "15  0.849697  1.583984  0.483241  0.069227 -1.644612  1.083785 -0.264040   \n",
       "16 -0.817091  0.098535 -1.621413 -1.103880  1.061463 -2.537070 -0.596982   \n",
       "17 -0.542056 -1.481645  1.771256 -0.545737 -0.352742  0.121251 -0.006372   \n",
       "18  0.478682 -0.600566 -0.739154  1.541035  0.060323 -1.311510  0.984617   \n",
       "19  1.115139 -1.983437 -1.108329  0.811892  0.345909  1.387534  0.489247   \n",
       "20  0.748104 -0.237157  0.334644 -1.151724  0.633037  0.133834  0.964140   \n",
       "21  1.060348 -0.652417  0.272714 -0.972853  0.874940 -0.376700 -0.945134   \n",
       "22  2.188903 -0.918486 -0.616139  1.468503  1.438960  0.493958 -1.460419   \n",
       "23  0.398197  0.776083 -1.016593 -0.823698 -0.225134  1.044066 -0.129935   \n",
       "24  1.428473 -0.238602  1.303705 -0.832590 -0.021987  1.117420 -0.352614   \n",
       "25  0.291046  0.483265 -1.504844  0.338384  0.304144 -1.566162 -0.877833   \n",
       "26 -1.532835  1.026299 -0.003073  0.309495 -0.824517  0.761319 -1.978394   \n",
       "27  0.956861 -2.206222  0.175713  0.450390 -1.110367 -0.683326 -1.462934   \n",
       "\n",
       "          21        22        23        24        25        26        27  \n",
       "0   1.721831  1.153121 -0.144130 -0.302309 -0.717030  0.141428 -0.155086  \n",
       "1   0.621380  0.137087  1.329327 -0.454444  0.193170  1.702389 -0.634235  \n",
       "2   1.170797 -0.377535  1.283638 -0.569373 -0.760108 -1.141832 -0.097819  \n",
       "3   0.238042 -0.730606  0.419769  1.186127  0.082407  0.579095 -1.854079  \n",
       "4  -0.103857  0.424034 -1.134576 -0.327464  1.159003  0.155672  0.720677  \n",
       "5  -1.371264  1.268598 -0.798368  1.181123 -0.138578 -0.065409  0.102005  \n",
       "6  -0.635005  0.974513 -1.760587  2.240196  0.558241 -0.898130 -0.191293  \n",
       "7  -0.645577 -2.001380  0.942395  0.627486  1.323623 -0.610883 -0.036616  \n",
       "8  -2.145219 -0.466879 -0.525107 -0.637520  0.538583  1.769654 -0.146022  \n",
       "9   0.188175 -0.047814  0.154670 -0.244221 -0.610407 -0.971988 -0.147728  \n",
       "10  1.163359  1.565486 -1.216381  1.586885  1.476563  0.883148 -0.727415  \n",
       "11  1.706307 -0.161502  0.174904 -0.599232 -0.526139 -0.840112 -0.606175  \n",
       "12  0.922428  1.762308 -0.821442 -2.177231 -1.226803 -0.231533  0.813056  \n",
       "13 -1.427974  1.444601  0.446701  0.437166 -1.558811  0.325451 -0.873876  \n",
       "14 -0.084330 -0.147841 -1.185926 -1.063723  0.418060 -0.404967  1.722885  \n",
       "15 -1.189069  0.180464  1.022450  1.075772  0.444104  0.117216 -0.075613  \n",
       "16  0.619368  0.737780 -0.043117  2.083177 -0.153688 -0.432864 -0.604908  \n",
       "17  0.237486 -1.643921 -0.804570  0.288701  0.421912 -1.227129  0.025800  \n",
       "18  1.376215  0.399963  1.929092 -1.364943 -0.164033 -0.287088  0.583896  \n",
       "19  0.220936  0.324788  1.473250  0.766090 -1.364736 -0.124745  1.831150  \n",
       "20  0.339075 -0.645803 -0.031706 -0.149283  0.592568 -0.083030 -0.050281  \n",
       "21 -0.841013  0.565069 -1.266691  0.121152 -1.171481  0.724091  1.019971  \n",
       "22 -3.388916  0.253746  0.745286 -2.196119  0.333600  1.303926 -0.750414  \n",
       "23  1.011525  0.306917  0.051016 -1.350450  1.499876  1.213555 -2.615243  \n",
       "24 -0.137850 -0.602607 -2.419485  0.920276 -1.128594 -0.045118  1.712395  \n",
       "25 -0.602804 -1.492816 -0.701871 -0.282561 -0.176017 -0.100225 -1.495856  \n",
       "26 -1.256146  0.632199  0.409341 -0.833629 -0.682285 -0.318656 -0.752472  \n",
       "27 -0.417872  1.254282  0.488594  0.178059 -1.998848 -1.376605 -0.087610  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to delve more into the subject, lets say I wish to model a neuron for those 28 inputs:\n",
    "inputs = train_X[0]\n",
    "weights = np.random.randn(28, 28) # Just to visualize on a tabular format, later I'll use the 1d format\n",
    "bias = 3 \n",
    "\n",
    "# And to make sure my weights are consistent with what I expected:\n",
    "pd.DataFrame(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3689.7575341109605\n"
     ]
    }
   ],
   "source": [
    "# Now, I'll just connect the input layer with the neuron by making the dot product of both arrays\n",
    "output = np.dot(weights.ravel(), inputs.ravel()) + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4295.88012964 -3035.3042228  -1166.8972121 ]\n"
     ]
    }
   ],
   "source": [
    "# Let's say I'll use 3 neurons connected directly to my inputs. I'd have somenthing like this:\n",
    "inputs = train_X[0]\n",
    "weights = [np.random.randn(784) for _ in range(3)]\n",
    "biases = [np.random.randint(0, 5) for _ in range(3)]\n",
    "\n",
    "# I can connect all neurons by using np.dot\n",
    "outputs = np.dot(weights, inputs.ravel()) + biases\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: weights vector is called first as we need to index the output of the function to EACH neuron. If inputs came first it would throw an error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing in Neural Networks\n",
    "\n",
    "In neural networks, we use something called **batch processing** to train our models effectively.\n",
    "\n",
    "## What is Batch Processing?\n",
    "\n",
    "Instead of throwing all our data at the network at once, we break it into smaller groups called **batches**.\n",
    "\n",
    "## Why Batches?\n",
    "\n",
    "- **Efficiency:** It's easier for computers to handle smaller pieces of data at a time, especially when we have lots of data.\n",
    "\n",
    "- **Generalization:** Batching helps our network learn from a variety of examples in each batch, making it better at understanding the big picture.\n",
    "\n",
    "- **Stability:** The updates to our model are more consistent when we work with batches, reducing randomness.\n",
    "\n",
    "- **Parallelization:** Batches can be processed in parallel, making use of modern hardware like GPUs.\n",
    "\n",
    "- **Regularization:** Batch processing can help prevent our model from overfitting, which means it gets better at handling new data.\n",
    "\n",
    "## Mini-Batch Sizes\n",
    "\n",
    "We choose the size of our batches. Common sizes are 32, 64, or 128, depending on the problem and our computer's power.\n",
    "\n",
    "## Training with Mini-Batches\n",
    "\n",
    "During training, we pass one mini-batch through the network at a time. We calculate how good or bad the model is with that batch and make small improvements.\n",
    "\n",
    "This process repeats for multiple mini-batches until we've gone through all our data, which we call an **epoch**. We usually go through multiple epochs to help the model learn even better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1500.19439501  1565.59758773   731.46116326]\n",
      " [  119.90310521  1797.46219947  -579.60210845]\n",
      " [ 3000.49776816  2958.41594807 -1198.44763616]]\n"
     ]
    }
   ],
   "source": [
    "# Ok, so now I'll think of my input as a collection of inputs (say a batch of 3)\n",
    "inputs = np.array([train_X[i].ravel() for i in range(3)])\n",
    "\n",
    "weights = np.array([np.random.randn(784) for _ in range(3)])\n",
    "biases = np.array([np.random.randint(0, 5) for _ in range(3)])\n",
    "\n",
    "# Note that I'm still using 3 neurons. The shape of my weights array is (3, 784) while my inputs shape is the same. If I simply dot product them, an error will be raised.\n",
    "# To avoid that, and make sure our math is right, the right thing to do is to transpose my weights matrix!\n",
    "outputs = np.dot(inputs, weights.T) + biases\n",
    "print(outputs) # This should be a 3x3 matrix = output of my neurons for all 3 inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a second neuron layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.013884    1.09169055 -0.42076207]\n",
      " [-1.48607014 -0.8685229  -0.36561622]\n",
      " [-0.58231852  1.25673517  1.35533535]]\n"
     ]
    }
   ],
   "source": [
    "# Just as I've done before, but more simpler: I have 3 inputs on 3 different batches. I'll just initialize 3 more neurons and biases and calculate the dot product of them.\n",
    "middle_inputs = outputs.copy()\n",
    "\n",
    "weights = np.array([np.random.randn(3) for _ in range(3)])\n",
    "biases = np.array([np.random.randint(0, 2) for _ in range(3)])\n",
    "\n",
    "# My middle inputs shape is (3,3). My weights shape is (3,). We'll keep using the same notation.\n",
    "outputs = np.dot(middle_inputs, weights.T) + biases\n",
    "print(weights) # Which is a 3x3 matrix relative to outputs of the second layer of neurons for the 3 inputs that I'm using!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object-Oriented Neural Network\n",
    "\n",
    "In this project, I'm adopting an object-oriented approach to build and manage our neural network. This involves creating Python objects to represent various components of the neural network, such as layers, weights, biases, and the network itself.\n",
    "\n",
    "## Why Object-Oriented Design?\n",
    "\n",
    "1. **Modularity:** Object-oriented design allows us to create modular and reusable components. Each part of the network can be encapsulated within its own object, making it easier to manage and understand the code.\n",
    "\n",
    "2. **Abstraction:** Objects help in abstracting the complexity of the network. We can define high-level interfaces for each component, hiding intricate details while providing a clean API.\n",
    "\n",
    "3. **Readability:** Object-oriented code tends to be more readable and self-explanatory. This is particularly important when working on complex projects like neural networks.\n",
    "\n",
    "4. **Ease of Debugging:** With clear separation of responsibilities, it's easier to locate and fix issues within specific components of the network.\n",
    "\n",
    "5. **Scalability:** An object-oriented design makes it easier to extend the network by adding new layers, activation functions, and optimizers. This flexibility is crucial when experimenting with different architectures.\n",
    "\n",
    "By creating objects for our neural network components, I aim to make this codebase more organized, maintainable, and scalable. It also helps in abstracting away the intricate details of the network, making it more accessible for future development and collaboration.\n",
    "\n",
    "In the upcoming sections, we will dive into the implementation of these objects, building a neural network from the ground up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm starting to see a conection here. This will be the foundation of creating Layers:\n",
    "class LayerDense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The power of using OOP right now is shown by making everything I did so far on these few lines:\n",
    "np.random.seed(0)\n",
    "\n",
    "first_layer = LayerDense(784, 3) # I have 28*28=784 inputs for 3 neurons\n",
    "second_layer = LayerDense(3, 3) # The output of the first layer is the input of the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 14.80753073  -3.68685917 -32.94804846]\n",
      " [  8.77757203 -41.83978917 -78.26822642]\n",
      " [-17.18374287  34.25673045  81.8390243 ]]\n"
     ]
    }
   ],
   "source": [
    "# And it becomes clear what I need to do now:\n",
    "X = np.array([train_X[i].ravel() for i in range(3)])\n",
    "first_layer.forward(X)\n",
    "second_layer.forward(first_layer.output)\n",
    "print(second_layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Layer Activation Functions\n",
    "\n",
    "In the neural network project, hidden layers play a critical role in learning and extracting complex patterns from data. To introduce non-linearity into the network, activation functions are used within these hidden layers.\n",
    "\n",
    "## What Are Activation Functions?\n",
    "\n",
    "Activation functions are mathematical functions that determine whether a neuron should \"fire\" (become active) based on the weighted sum of its inputs. In other words, they introduce non-linear relationships into the network, allowing it to model more complex patterns.\n",
    "\n",
    "## Why Hidden Layer Activation Functions?\n",
    "\n",
    "- **Non-Linearity:** Hidden layers need to capture non-linear relationships within the data. Activation functions enable neurons to produce non-linear output, which is crucial for solving complex problems.\n",
    "\n",
    "- **Feature Learning:** Different activation functions have different properties, allowing the network to learn and represent features at various scales and complexities.\n",
    "\n",
    "- **Gradient Flow:** Activation functions also help in maintaining a good gradient flow during backpropagation, aiding in efficient and stable training.\n",
    "\n",
    "## Common Activation Functions\n",
    "\n",
    "There are several common activation functions used in hidden layers, including:\n",
    "\n",
    "- **Rectified Linear Unit (ReLU):** A simple and widely used activation function that outputs the input for positive values and zero for negative values.\n",
    "\n",
    "- **Sigmoid:** An S-shaped function that squashes input values into a range between 0 and 1. It's often used in binary classification problems.\n",
    "\n",
    "- **Hyperbolic Tangent (tanh):** Similar to the sigmoid function but with a range between -1 and 1. It helps with zero-centered outputs.\n",
    "\n",
    "- **Leaky ReLU:** An extension of ReLU that allows small negative values to pass through, addressing the \"dying ReLU\" problem.\n",
    "\n",
    "The choice of activation function depends on the problem at hand, and experimentation often helps in determining which one works best for a particular task.\n",
    "\n",
    "In the upcoming sections, activation functions will be implemented within the hidden layers to add non-linearity and enable the project's neural network to learn and represent intricate patterns in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Rectified Linear Activation Function (RELU) is amazing, I'll be implementing that bellow:\n",
    "class activation_ReLU():\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 38.75324019 -43.2537871  -65.89990683]\n",
      " [ 13.27605731 -44.66599785 -74.05566156]\n",
      " [ 14.82255911  -9.31238909  13.88044815]]\n"
     ]
    }
   ],
   "source": [
    "# I pretty much just add the activation function on the output of the first layer:\n",
    "X = np.array([train_X[i].ravel() for i in range(3)])\n",
    "np.random.seed(0)\n",
    "\n",
    "first_layer = LayerDense(784, 3) \n",
    "second_layer = LayerDense(3, 3) \n",
    "\n",
    "activation_1 = activation_ReLU()\n",
    "\n",
    "# And it becomes clear what I need to do now:\n",
    "first_layer.forward(X)\n",
    "activation_1.forward(first_layer.output)\n",
    "\n",
    "\n",
    "second_layer.forward(activation_1.output)\n",
    "print(second_layer.output) # And that is DONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the \"skeleton\" of our model up and running, I'll be looking at the output layer of the model next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Layer and Softmax Activation\n",
    "\n",
    "In the neural network project, the **output layer** is where the network makes its final predictions or classifications. It's a critical part of the network that produces the results we want.\n",
    "\n",
    "## The Role of Softmax Activation\n",
    "\n",
    "To make these predictions, the output layer often employs the **softmax activation function**. The softmax function takes a set of scores or values and transforms them into a probability distribution. \n",
    "\n",
    "- **Normalization:** The softmax function normalizes the scores so that they all add up to 1. This makes it easier to interpret the output as probabilities.\n",
    "\n",
    "- **Multi-Class Classification:** Softmax is commonly used in multi-class classification problems, where there are more than two classes to choose from.\n",
    "\n",
    "The output layer with softmax activation helps our neural network provide clear and interpretable results, making it a key component in many machine learning tasks.\n",
    "\n",
    "In the next sections, we will explore how to implement the output layer and softmax activation function to ensure our neural network produces meaningful and accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 2.42549605e-36 3.54581225e-46]\n",
      " [1.00000000e+00 6.85622677e-26 1.18118086e-38]\n",
      " [7.19525864e-01 2.37340352e-11 2.80474136e-01]]\n"
     ]
    }
   ],
   "source": [
    "# The idea on this activation function is to first exponenciate the result of the last layer and second to normalize it so it falls into a 0 and 1 interval.\n",
    "\n",
    "# Overflow prevention\n",
    "values_test = second_layer.output - np.max(second_layer.output)\n",
    "\n",
    "# First we exponenciate:\n",
    "values_test = np.exp(values_test)\n",
    "\n",
    "# Second we normalize:\n",
    "values_test = values_test / np.sum(values_test, axis = 1, keepdims=True)\n",
    "\n",
    "# We have the following output\n",
    "print(values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now let's translate whats up there into a new class\n",
    "class ActivationSoftMax():\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis = 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 2.42549605e-36 3.54581225e-46]\n",
      " [1.00000000e+00 6.85622677e-26 1.18118086e-38]\n",
      " [7.19525864e-01 2.37340352e-11 2.80474136e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Again, it's just build one on top of the other\n",
    "X = np.array([train_X[i].ravel() for i in range(3)])\n",
    "np.random.seed(0)\n",
    "\n",
    "first_layer = LayerDense(784, 3) \n",
    "second_layer = LayerDense(3, 3) \n",
    "\n",
    "activation_1 = activation_ReLU()\n",
    "activation_softmax = ActivationSoftMax()\n",
    "\n",
    "first_layer.forward(X) # Inputs go to first layer\n",
    "activation_1.forward(first_layer.output) # Then to activation function\n",
    "second_layer.forward(activation_1.output) # Then to second layer\n",
    "\n",
    "# Finally, activation by softmax\n",
    "activation_softmax.forward(second_layer.output)\n",
    "print(activation_softmax.output) # And that is DONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Cross-Entropy Loss\n",
    "\n",
    "In the project's context of classification, we need a way to measure how wrong or right our neural network's predictions are. One of the most widely used loss functions for this purpose is **Categorical Cross-Entropy**.\n",
    "\n",
    "## What is Categorical Cross-Entropy?\n",
    "\n",
    "Categorical Cross-Entropy is a loss function designed for multi-class classification problems, where data can belong to one of several possible classes. Its primary purpose is to quantify the difference between the predicted probability distribution and the actual distribution (one-hot encoded labels).\n",
    "\n",
    "## How It Works\n",
    "\n",
    "Here's how Categorical Cross-Entropy works:\n",
    "\n",
    "- For each example, the network produces a probability distribution over all the possible classes using the softmax function in the output layer.\n",
    "\n",
    "- The actual label is represented as a one-hot encoded vector, where only one element is 1 (the correct class) and the rest are 0.\n",
    "\n",
    "- Categorical Cross-Entropy measures the dissimilarity between the predicted distribution and the actual label by summing up the differences.\n",
    "\n",
    "- It penalizes incorrect predictions more severely, encouraging the network to become more accurate in its classifications.\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "In essence, Categorical Cross-Entropy quantifies how \"surprised\" the network is by the actual class compared to its prediction. If the prediction is perfectly accurate, the loss is zero. The higher the loss, the more different the prediction is from the actual label.\n",
    "\n",
    "## Training Objective\n",
    "\n",
    "During the training process, our goal is to minimize the Categorical Cross-Entropy loss. This means finding the network's parameters (weights and biases) that result in the most accurate predictions.\n",
    "\n",
    "In the following sections, we will implement Categorical Cross-Entropy as the loss function for our neural network and use it to guide the training process, helping the network learn and make better classifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99752638e-01 2.42489607e-36 3.54493515e-46 2.15916838e-04\n",
      "  7.92027664e-26 3.14446724e-05 4.35590025e-11 2.12793591e-43\n",
      "  1.12855400e-60 6.08625859e-35]\n",
      " [3.18529382e-33 3.19146442e-28 2.83064991e-12 1.42726252e-20\n",
      "  1.28391390e-20 8.92143604e-04 1.69019781e-08 4.68454040e-02\n",
      "  1.36311083e-22 9.52262436e-01]\n",
      " [2.82193134e-12 7.09890074e-22 1.82950591e-17 1.02951965e-23\n",
      "  4.36150060e-11 1.06366587e-11 3.89786604e-16 1.42263235e-01\n",
      "  8.57736714e-01 5.06745797e-08]]\n"
     ]
    }
   ],
   "source": [
    "# First, I'll have to adapt my model as my objective is to give classifications to 10 different outcomes (all numbers)\n",
    "\n",
    "# Won't change what I had essencially\n",
    "X = np.array([train_X[i].ravel() for i in range(3)])\n",
    "np.random.seed(0)\n",
    "\n",
    "first_layer = LayerDense(784, 3) \n",
    "second_layer = LayerDense(3, 10) # Only now I will have 10 outputs from the second layer\n",
    "\n",
    "activation_1 = activation_ReLU()\n",
    "activation_softmax = ActivationSoftMax()\n",
    "\n",
    "first_layer.forward(X)\n",
    "activation_1.forward(first_layer.output) \n",
    "second_layer.forward(activation_1.output) \n",
    "\n",
    "# Finally, activation by softmax\n",
    "activation_softmax.forward(second_layer.output)\n",
    "print(activation_softmax.output) # And that is DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([train_y[i] for i in range(3)]) # This is my y vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.36728099 74.82676353 23.85561985]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array(activation_softmax.output)\n",
    "\n",
    "# I wish to get only the outputs related to the correct classification, hence:\n",
    "print(-np.log(softmax_outputs[range(len(softmax_outputs)), y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.34988812436632\n"
     ]
    }
   ],
   "source": [
    "# Which is:\n",
    "neg_loss = -np.log(softmax_outputs[range(len(softmax_outputs)), y])\n",
    "average_loss = np.mean(neg_loss)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue is due to happen on this case. If confidence of predicted value is 0, log(0) -> inf. And if there is an infinite \"value\", the mean of an infinite value will be infinite.\n",
    "\n",
    "Because of this, it is interesting to clip our confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.34988812436632\n"
     ]
    }
   ],
   "source": [
    "# Something of the sorts:\n",
    "predictions = np.clip(softmax_outputs, 1e-50, 1 - 1e-50)\n",
    "neg_loss = -np.log(predictions[range(len(softmax_outputs)), y])\n",
    "average_loss = np.mean(neg_loss)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement it as a class\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        batch_loss = np.mean(sample_losses)\n",
    "        return batch_loss\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Now a workaround if y_true is one hot encoded or not:\n",
    "        if len(y_true.shape) == 1:\n",
    "            confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        else:\n",
    "            confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        negative_log_likelihood = -np.log(confidences)\n",
    "        return negative_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1948777644199895\n"
     ]
    }
   ],
   "source": [
    "# IT IS BUILD TIME!\n",
    "\n",
    "X = np.array([train_X[i].ravel() for i in range(3)])\n",
    "y = np.array([train_y[i] for i in range(3)])\n",
    "\n",
    "first_layer = LayerDense(784, 3) \n",
    "second_layer = LayerDense(3, 10)\n",
    "\n",
    "activation_1 = activation_ReLU()\n",
    "activation_softmax = ActivationSoftMax()\n",
    "\n",
    "first_layer.forward(X)\n",
    "activation_1.forward(first_layer.output) \n",
    "second_layer.forward(activation_1.output) \n",
    "\n",
    "activation_softmax.forward(second_layer.output)\n",
    "\n",
    "# Calculate the loss:\n",
    "cross_entropy_loss = CrossEntropyLoss()\n",
    "\n",
    "loss_value = cross_entropy_loss.calculate(activation_softmax.output, y)\n",
    "print(loss_value) # And that is DONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization in Neural Networks\n",
    "\n",
    "Optimization is a crucial step in training neural networks to make accurate predictions or classifications. Key concepts include **backpropagation** and **gradient descent**, which are fundamental to improving the network's performance.\n",
    "\n",
    "## 1. Backpropagation\n",
    "\n",
    "**Backpropagation** is the process of computing the gradients of the loss function with respect to the network's parameters. It's a crucial step in the training process and works in reverse through the network's layers. Here are the key points:\n",
    "\n",
    "- **Forward Pass:** During the forward pass, input data is propagated through the network to make predictions.\n",
    "\n",
    "- **Loss Calculation:** The loss (error) between the predictions and the actual target values is calculated.\n",
    "\n",
    "- **Backward Pass:** In the backward pass, the gradients of the loss with respect to the network's parameters are calculated. This helps us understand how changes in each parameter affect the loss.\n",
    "\n",
    "- **Gradient Descent:** The gradients obtained from backpropagation guide the update of network parameters in the direction that reduces the loss. Gradient descent is a common optimization algorithm used for this purpose.\n",
    "\n",
    "## 2. Gradient Descent\n",
    "\n",
    "**Gradient Descent** is an iterative optimization algorithm that minimizes the loss function by adjusting the network's parameters in small steps. Key points to understand:\n",
    "\n",
    "- **Learning Rate:** The learning rate is a hyperparameter that determines the step size during parameter updates. It affects the speed and stability of training.\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD):** In standard gradient descent, all training examples are used in each iteration. In SGD, only a random subset (mini-batch) of examples is used, which speeds up training.\n",
    "\n",
    "- **Adaptive Methods:** Variants of gradient descent, such as Adam and RMSprop, adjust the learning rate during training to improve convergence.\n",
    "\n",
    "## 3. Mini-Batch Training\n",
    "\n",
    "Instead of processing all data at once, neural networks are often trained on smaller subsets called **mini-batches**. Mini-batch training provides several benefits, including improved efficiency, regularization, and better gradient estimates.\n",
    "\n",
    "## 4. Local Minima\n",
    "\n",
    "Optimization in neural networks can get stuck in local minima, where the loss function stops improving. Techniques like **momentum** and **learning rate schedules** are used to escape these local minima and reach better solutions.\n",
    "\n",
    "In summary, optimization in neural networks involves backpropagation to compute gradients and gradient descent to update parameters. Training typically uses mini-batches to improve efficiency. Handling local minima and choosing appropriate optimization techniques are also important for successful training.\n",
    "\n",
    "In the following sections, we will delve into the implementation of these optimization concepts to train our neural network effectively.\n",
    "\n",
    "\n",
    "## Calculus and Optimizing the Neural network\n",
    "\n",
    "Before I start experimenting with backpropagation, one thing I must really test is my calculus knowledge right now.\n",
    "\n",
    "The main objective here is quite simple: where should my optimizer adjust that will give conctrete results on reducing my loss function?\n",
    "\n",
    "The question is quite basic if I look at a single layer of a neural network and worry about how a specific output is being propagated.\n",
    "\n",
    "I know that on the low level of the model, what is being done is the following operation:\n",
    "\n",
    "$y_i = x_i * weight_i$\n",
    "\n",
    "But it's complexity goes further than that, as we sum the result of every x*weight:\n",
    "\n",
    "$\\sum y_i = \\sum x_i * w_i = w_i$\n",
    "\n",
    "As w is the result of the dot product of inputs and weights, we add the bias and the outcome is the output of the neuron:\n",
    "\n",
    "$\\sum x_i * w_i + bias$\n",
    "\n",
    "This is feeded into the ReLu function, which is basically a $max(x, 0)$ function:\n",
    "\n",
    "$z = ReLu(\\sum x_i * w_i + bias)$\n",
    "\n",
    "### The parcial derivatives\n",
    "\n",
    "Now that I've defined z as the output of the Rectified Linear Unit RELU, I'll delve into the partial derivatives of this function.\n",
    "\n",
    "First, the partial derivative of the ReLu function is defined as:\n",
    "\n",
    "$\\frac{\\partial}{\\partial z}f(z) = 1 (x > 0)$\n",
    "\n",
    "Also, $z = (\\sum x_i * w_i + bias)$ which means, by the chain rule:\n",
    "\n",
    "$\\frac{\\partial}{\\partial z}f(z) = \\frac{\\partial}{\\partial z}f(z) * \\frac{\\partial}{\\partial w}f(w + bias)$\n",
    "\n",
    "As w is a partial derivative of $\\sum y_i$, and partial derivative of a sum will always result on 1, we can simplify the expression by:\n",
    "\n",
    "$\\frac{\\partial}{\\partial z}f(z) = 1 * \\frac{\\partial}{\\partial z}f(z)$\n",
    "\n",
    "The result above is the result of the fist **backpropagation!** Let's keep up now by going further into the functions and calculate de partial derivative of the output of the neuron:\n",
    "\n",
    "$\\frac{\\partial}{\\partial x_i w_i}f(\\sum (x_i w_i) + bias) = 1$\n",
    "\n",
    "And we should also consider the partial derivative of the bias on this case:\n",
    "\n",
    "$\\frac{\\partial}{\\partial bias}f(\\sum (x_i w_i) + bias) = 1$\n",
    "\n",
    "The last function inside a layer is plainly simple:\n",
    "\n",
    "$\\frac{\\partial}{\\partial x_i}f(x_i * w_i) = w_i$\n",
    "\n",
    "$\\frac{\\partial}{\\partial w_i}f(x_i * w_i) = x_i$\n",
    "\n",
    "All of these combined make possible to calculate the gradient of the whole layer:\n",
    "\n",
    "$dx = \\frac{\\partial}{\\partial x_i}f(ReLu(z))$ \n",
    "\n",
    "$dw = \\frac{\\partial}{\\partial w_i}f(ReLu(z))$ \n",
    "\n",
    "$db = \\frac{\\partial}{\\partial b_i}f(ReLu(z))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything I have so far...\n",
    "class LayerDense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class activation_ReLU():\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "class ActivationSoftMax():\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis = 1, keepdims=True)\n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        batch_loss = np.mean(sample_losses)\n",
    "        return batch_loss\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Now a workaround if y_true is one hot encoded or not:\n",
    "        if len(y_true.shape) == 1:\n",
    "            confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        else:\n",
    "            confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        negative_log_likelihood = -np.log(confidences)\n",
    "        return negative_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing I'll have to add is a backward method for LayerDense class:\n",
    "class LayerDense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues): #dvalues are calculated on the ReLu activation phase\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.biases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "# And now for the ReLu class:\n",
    "class activation_ReLU():\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # This is the simple derivative where we nullify all values that were negative before the ReLu activation:\n",
    "        self.dinputs[inputs < 0 ] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of the Categorical Cross Entropy Loss function\n",
    "\n",
    "The Categorical Cross Entropy Loss function is defined by:\n",
    "\n",
    "$L_i = -\\sum y_{i, j} log(\\hat y_{i, j})$\n",
    "\n",
    "It's parcial derivative is:\n",
    "\n",
    "$\\frac{\\partial L_i}{\\partial \\hat y_{i, j}} = -\\sum y_{i, j} \\frac{\\partial}{\\partial \\hat y_{i, j}} log(\\hat y_{i, j}) = -\\sum y_{i, j} \\frac{1}{\\hat y_{i, j}} = - \\sum \\frac{y_{i, j}}{\\hat y_{i, j}}$\n",
    "\n",
    "As only one argument of this sum will be diffent than 0 (we will only consider the true label category):\n",
    "\n",
    "$\\frac{\\partial L_i}{\\partial \\hat y_{i, j}} = - \\frac{y_{i, j}}{\\hat y_{i, j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating this equation into a method:\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Now a workaround if y_true is one hot encoded or not:\n",
    "        if len(y_true.shape) == 1:\n",
    "            confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        else:\n",
    "            confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        negative_log_likelihood = -np.log(confidences)\n",
    "        return negative_log_likelihood\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        n_samples = len(dvalues)\n",
    "\n",
    "        # Number of labels\n",
    "        n_labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(n_labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = - y_true / dvalues\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / n_samples # This is important so when I implement the optimizer the gradient sum doesn't explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of the SoftMax activation function\n",
    "\n",
    "The SoftMax activation function is defined by:\n",
    "\n",
    "$S_{i, j} = \\frac{e^{z_{i j}}}{\\sum e^{z_{i l}}}$\n",
    "\n",
    "It's derivative is a bit complicated, and is as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_{i, j}}{\\partial z_{i, k}} = \\frac{\\partial \\frac{e^{z_{i j}}}{\\sum e^{z_{i l}}}}{\\partial z_{i, k}} = \\frac{\\frac{\\partial}{\\partial z_{i, k}}e^{z_{i j}}\\sum e^{z_{i l}}- e^{z_{i j}}\\frac{\\partial}{\\partial z_{i, k}} \\sum e^{z_{i l}}}{[\\sum e^{z_{i l}}]^2}\n",
    "$$\n",
    "\n",
    "As $\\frac{\\partial}{\\partial x} e^x = e^x$, and if $j = k \\rightarrow \\frac{\\partial}{\\partial z_{i, k}} \\sum e^{z_{i l}} = e^{z_{i l}}$. So:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_{i, j}}{\\partial z_{i, k}} = \\frac{e^{z_{i j}} \\sum e^{z_{i l}}- e^{z_{i j}} e^{z_{i l}}}{[\\sum e^{z_{i l}}]^2} = \\frac{e^{z_{i j}} (\\sum e^{z_{i l}} - e^{z_{i l}})}{\\sum e^{z_{i l}} \\sum e^{z_{i l}}}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_{i, j}}{\\partial z_{i, k}} = \\frac{e^{z_{i j}}}{\\sum e^{z_{i l}}} \\cdot \\frac{\\sum e^{z_{i l}} - e^{z_{i l}}}{\\sum e^{z_{i l}}} = S_{i, j} \\cdot (1 - S_{i, k})\n",
    "$$\n",
    "\n",
    "But if $j \\neq k \\rightarrow \\frac{\\partial}{\\partial z_{i, k}} \\sum e^{z_{i l}} = 0$. Then (skipping to end of simplification):\n",
    "\n",
    "$$\n",
    "-S_{i, j} \\cdot S_{i, k}\n",
    "$$\n",
    "\n",
    "\n",
    "But as there are 2 different scenarios to code now, it's not very interesting speedwise. We can morph both scenarios by using the following Kronecker delta:\n",
    "\n",
    "If $i = j \\rightarrow \\delta _{j,k} = 1$, else: $\\delta _{j,k} = 0$ \n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_{i, j}}{\\partial z_{i, k}} = S_{i, j} \\cdot (\\delta _{j,k} - S_{i, k}) = S_{i, j}\\delta _{j,k} - S_{i, j} S_{i, k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the softmax function is: [[0.7 0.1 0.2]]\n",
      "This is the output when delta = 1 and 0 otherwise:\n",
      "[[0.7 0.  0. ]\n",
      " [0.  0.1 0. ]\n",
      " [0.  0.  0.2]]\n",
      "This is the product of the right side of the equation:\n",
      "[[0.49 0.07 0.14]\n",
      " [0.07 0.01 0.02]\n",
      " [0.14 0.02 0.04]]\n",
      "This is the resulting gradient of the softmax function:\n",
      "[[ 0.21 -0.07 -0.14]\n",
      " [-0.07  0.09 -0.02]\n",
      " [-0.14 -0.02  0.16]]\n"
     ]
    }
   ],
   "source": [
    "# I'll try to reproduce a book problem on this now\n",
    "\n",
    "# Say we have the following\n",
    "softmax_output = np.array([0.7, 0.1, 0.2]).reshape(-1, 1)\n",
    "print(f\"The output of the softmax function is: {softmax_output.T}\")\n",
    "\n",
    "# Below is the same as softmax_output * np.eye(softmax_output.shape[0]) but faster\n",
    "print(\"This is the output when delta = 1 and 0 otherwise:\")\n",
    "print(np.diagflat(softmax_output))\n",
    "\n",
    "print(\"This is the product of the right side of the equation:\"),\n",
    "print(np.dot(softmax_output, softmax_output.T))\n",
    "\n",
    "print(\"This is the resulting gradient of the softmax function:\")\n",
    "print(np.diagflat(softmax_output) - np.dot(softmax_output, softmax_output.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix result of the equation and the array solution provided by the code is called the\n",
    "Jacobian matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.17315525e-62 -2.32094331e-30 -4.40044283e-42 -3.78565136e-67\n",
      "  -3.25928993e-25  8.95330501e-02 -3.36217519e-47 -8.95330501e-02\n",
      "  -6.02343956e-13 -1.36078008e-28]\n",
      " [-6.76665981e-08 -1.36219860e-06  6.49922806e-02 -2.04837552e-06\n",
      "  -2.72416100e-04 -3.68144946e-06 -5.50163737e-08 -5.02513102e-04\n",
      "  -6.08142136e-02 -3.39592306e-03]\n",
      " [-3.03576608e-18 -3.03576608e-18 -3.03576608e-18 -3.03576608e-18\n",
      "  -1.30104261e-18 -1.30104261e-18 -1.30104261e-18 -1.30104261e-18\n",
      "  -1.73472348e-18 -3.46944695e-18]]\n"
     ]
    }
   ],
   "source": [
    "# Again, I'll repeat the process to the previous output of the batch of 3 inputs I had:\n",
    "\n",
    "softmax_output = activation_softmax.output\n",
    "# print(softmax_output)\n",
    "\n",
    "dinputs = np.empty_like(softmax_output) # This creates the vector without the need to initialize its values.\n",
    "\n",
    "# Because it's now on a batch format, I'll loop over it:\n",
    "for index, (single_output, single_dvalues) in enumerate(zip(softmax_output, softmax_output)):\n",
    "    single_output = single_output.reshape(-1, 1)\n",
    "    jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "    dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "print(dinputs) # This is the jacobian matrix for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's code this solution on the SoftMax activation class\n",
    "class ActivationSoftMax():\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis = 1, keepdims=True)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            \n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, there is a efficient combination of the SoftMax activation and the loss function that improves a lot computing time. I won't delve into the mathematics of this derivative, but I'll implement it's solution either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "# Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = ActivationSoftMax()\n",
    "        self.loss = CrossEntropyLoss()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this is all I have so far:\n",
    "\n",
    "class LayerDense():\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues): #dvalues are calculated on the ReLu activation phase\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class activation_ReLU():\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs < 0 ] = 0\n",
    "    \n",
    "class ActivationSoftMax():\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis = 1, keepdims=True)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            \n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Now a workaround if y_true is one hot encoded or not:\n",
    "        if len(y_true.shape) == 1:\n",
    "            confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        else:\n",
    "            confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        negative_log_likelihood = -np.log(confidences)\n",
    "        return negative_log_likelihood\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        n_samples = len(dvalues)\n",
    "\n",
    "        # Number of labels\n",
    "        n_labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(n_labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = - y_true / dvalues\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / n_samples # This is important so when I implement the optimizer the gradient sum doesn't explode\n",
    "\n",
    "# Softmax classifier - combined Softmax activation and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "# Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = ActivationSoftMax()\n",
    "        self.loss = CrossEntropyLoss()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: combined loss and activation:\n",
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n",
      "Gradients: separate loss and activation:\n",
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n"
     ]
    }
   ],
   "source": [
    "# Lets test if both gradient calculations result in the same thing\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],[0.1, 0.5, 0.4],[0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# The gradient using the \"faster\" form\n",
    "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "softmax_loss.backward(softmax_outputs, class_targets)\n",
    "dvalues1 = softmax_loss.dinputs \n",
    "\n",
    "# Using the conventional form\n",
    "activation = ActivationSoftMax()\n",
    "activation.output = softmax_outputs\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "loss.backward(softmax_outputs, class_targets)\n",
    "activation.backward(loss.dinputs)\n",
    "dvalues2 = activation.dinputs\n",
    "\n",
    "\n",
    "print('Gradients: combined loss and activation:')\n",
    "print(dvalues1)\n",
    "print('Gradients: separate loss and activation:')\n",
    "print(dvalues2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[0.         0.         0.20950808]]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-1.01672813e+02  7.95956995e-02  8.77309725e-25  4.60940505e-15\n",
      "  -1.06293647e+02 -1.58669433e+02  1.16071382e-22  5.19450792e-34\n",
      "   3.66556297e+02  1.07464968e-13]]\n",
      "[[-3.33333238e-01  2.55097512e-04  2.86827222e-27  1.50081181e-17\n",
      "  -3.33333333e-01 -3.33333333e-01  3.79242245e-25  1.70113798e-36\n",
      "   9.99744807e-01  3.49599909e-16]]\n"
     ]
    }
   ],
   "source": [
    "# And it's time to test on real data!\n",
    "X = np.array([train_X[i].ravel() for i in range(3)])\n",
    "y = np.array([train_y[i] for i in range(3)])\n",
    "\n",
    "first_layer = LayerDense(784, 3) \n",
    "second_layer = LayerDense(3, 10)\n",
    "\n",
    "activation_1 = activation_ReLU()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform first forward pass into first layer\n",
    "first_layer.forward(X)\n",
    "activation_1.forward(first_layer.output) # Activate neurons based on ReLu activation function\n",
    "\n",
    "# Perform second forward pass into second layer\n",
    "second_layer.forward(activation_1.output) \n",
    "\n",
    "# Perform forward pass into activation/loss function:\n",
    "loss = loss_activation.forward(second_layer.output, y)\n",
    "\n",
    "# print(loss_activation.output[0]) # And that is done with forward pass.\n",
    "# print(\"loss:\", loss)\n",
    "\n",
    "# Now on the backward pass:\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "second_layer.backward(loss_activation.dinputs)\n",
    "activation_1.backward(second_layer.dinputs)\n",
    "first_layer.backward(activation_1.dinputs)\n",
    "\n",
    "# Print gradients\n",
    "print(first_layer.dweights)\n",
    "print(first_layer.dbiases)\n",
    "print(second_layer.dweights)\n",
    "print(second_layer.dbiases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy from model is \n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a popular optimization algorithm used in training neural networks. It's a variant of the gradient descent algorithm that offers several advantages, especially for large datasets. Here's a brief overview:\n",
    "\n",
    "## Key Features:\n",
    "\n",
    "1. **Minibatch Processing:** Instead of using the entire dataset in each iteration, SGD randomly selects a small subset (minibatch) of data samples. This speeds up training and often results in faster convergence.\n",
    "\n",
    "2. **Stochastic Updates:** In each iteration, SGD updates the model's parameters (weights and biases) based on the gradient computed for the current minibatch. This introduces randomness, which can help escape local minima.\n",
    "\n",
    "3. **Noisy but Efficient:** The stochastic nature of SGD introduces noise in parameter updates, but this noise can have a regularizing effect and help the model generalize better.\n",
    "\n",
    "4. **Learning Rate:** The learning rate in SGD determines the step size for parameter updates. It's a critical hyperparameter to fine-tune.\n",
    "\n",
    "5. **Convergence:** SGD may not converge to the global minimum but often finds a good solution in a reasonable time.\n",
    "\n",
    "## Benefits:\n",
    "\n",
    "- Well-suited for large datasets.\n",
    "- Faster convergence due to minibatch processing.\n",
    "- Effective regularization through noise.\n",
    "- Scalable to deep neural networks.\n",
    "\n",
    "## Challenges:\n",
    "\n",
    "- Noisy updates may require careful learning rate tuning.\n",
    "- May not converge to the global minimum.\n",
    "\n",
    "SGD is the foundation for many advanced optimization algorithms in deep learning, making it a crucial tool for training neural networks efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.083, loss: 14.533\n",
      "epoch: 100, acc: 0.257, loss: 1.965\n",
      "epoch: 200, acc: 0.312, loss: 1.826\n",
      "epoch: 300, acc: 0.357, loss: 1.715\n",
      "epoch: 400, acc: 0.371, loss: 1.668\n",
      "epoch: 500, acc: 0.377, loss: 1.643\n",
      "epoch: 600, acc: 0.384, loss: 1.619\n",
      "epoch: 700, acc: 0.393, loss: 1.586\n",
      "epoch: 800, acc: 0.402, loss: 1.557\n",
      "epoch: 900, acc: 0.402, loss: 1.552\n"
     ]
    }
   ],
   "source": [
    "# And we use this class as another building block inside a loop.\n",
    "X = np.array([train_X[i].ravel() for i in range(5000)])\n",
    "y = np.array([train_y[i] for i in range(5000)])\n",
    "\n",
    "# Create layers\n",
    "first_layer = LayerDense(784, 37) \n",
    "second_layer = LayerDense(37, 10)\n",
    "\n",
    "# Initialize activation/loss\n",
    "activation_1 = activation_ReLU()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate = 0.01)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(0, 1000):\n",
    "\n",
    "    # Perform first forward pass into first layer\n",
    "    first_layer.forward(X)\n",
    "    activation_1.forward(first_layer.output) # Activate neurons based on ReLu activation function\n",
    "\n",
    "    # Perform second forward pass into second layer\n",
    "    second_layer.forward(activation_1.output) \n",
    "\n",
    "    # Perform forward pass into activation/loss function:\n",
    "    loss = loss_activation.forward(second_layer.output, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f}')\n",
    "\n",
    "    # Now on the backward pass:\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    second_layer.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(second_layer.dinputs)\n",
    "    first_layer.backward(activation_1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(first_layer)\n",
    "    optimizer.update_params(second_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are already starting to take shape. I'll keep on progressing on how to improve with what I have so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Decay in Optimization\n",
    "\n",
    "Learning rate decay is a strategy used in optimization algorithms, such as Stochastic Gradient Descent (SGD), to gradually reduce the learning rate during training. This approach helps address specific challenges in the training process. Here's a brief overview:\n",
    "\n",
    "## Purpose of Learning Rate Decay:\n",
    "\n",
    "- **Faster Convergence:** Initially, a larger learning rate allows the optimizer to take bigger steps, which can speed up convergence in the early stages of training.\n",
    "\n",
    "- **Stability:** As training progresses, reducing the learning rate can stabilize the training process, preventing overshooting and oscillations.\n",
    "\n",
    "- **Refinement:** Smaller learning rates in later stages allow for fine-tuning model parameters for better accuracy.\n",
    "\n",
    "## Common Decay Schedules:\n",
    "\n",
    "1. **Step Decay:** The learning rate is reduced at specific intervals or epochs. For example, you can halve the learning rate after every \"n\" epochs.\n",
    "\n",
    "2. **Exponential Decay:** The learning rate is reduced exponentially over time, often by a fixed factor at each step.\n",
    "\n",
    "3. **Time-Based Decay:** The learning rate decreases with time according to a predefined function or schedule.\n",
    "\n",
    "4. **Performance-Based Decay:** The learning rate is adjusted based on the model's performance, decreasing when improvement stalls.\n",
    "\n",
    "## Benefits:\n",
    "\n",
    "- **Improved Convergence:** Learning rate decay can help find a better minimum in the loss function.\n",
    "\n",
    "- **Robustness:** It makes training less sensitive to the initial learning rate choice.\n",
    "\n",
    "## Challenges:\n",
    "\n",
    "- Selecting the right decay schedule and parameters can be a hyperparameter tuning challenge.\n",
    "\n",
    "- Over-aggressive decay may lead to slow convergence or getting stuck in local minima.\n",
    "\n",
    "Learning rate decay is a valuable tool for optimizing deep learning models, and its effectiveness depends on the specific problem, architecture, and dataset. It's a strategy that balances fast initial progress with careful fine-tuning as training advances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.044, loss: 15.186 lr: 0.01\n",
      "epoch: 100, acc: 0.280, loss: 1.937 lr: 0.005025125628140704\n",
      "epoch: 200, acc: 0.391, loss: 1.645 lr: 0.0033444816053511705\n",
      "epoch: 300, acc: 0.460, loss: 1.607 lr: 0.002506265664160401\n",
      "epoch: 400, acc: 0.483, loss: 1.417 lr: 0.002004008016032064\n",
      "epoch: 500, acc: 0.513, loss: 1.327 lr: 0.001669449081803005\n",
      "epoch: 600, acc: 0.524, loss: 1.280 lr: 0.0014306151645207437\n",
      "epoch: 700, acc: 0.533, loss: 1.262 lr: 0.0012515644555694619\n",
      "epoch: 800, acc: 0.545, loss: 1.217 lr: 0.0011123470522803114\n",
      "epoch: 900, acc: 0.549, loss: 1.193 lr: 0.001001001001001001\n",
      "epoch: 1000, acc: 0.552, loss: 1.177 lr: 0.0009099181073703366\n"
     ]
    }
   ],
   "source": [
    "# And we use this class as another building block inside a loop.\n",
    "X = np.array([train_X[i].ravel() for i in range(5000)])\n",
    "y = np.array([train_y[i] for i in range(5000)])\n",
    "\n",
    "# Create layers\n",
    "first_layer = LayerDense(784, 37) \n",
    "second_layer = LayerDense(37, 10)\n",
    "\n",
    "# Initialize activation/loss\n",
    "activation_1 = activation_ReLU()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate = 0.01, decay=0.01)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(0, 1001):\n",
    "\n",
    "    # Perform first forward pass into first layer\n",
    "    first_layer.forward(X)\n",
    "    activation_1.forward(first_layer.output) # Activate neurons based on ReLu activation function\n",
    "\n",
    "    # Perform second forward pass into second layer\n",
    "    second_layer.forward(activation_1.output) \n",
    "\n",
    "    # Perform forward pass into activation/loss function:\n",
    "    loss = loss_activation.forward(second_layer.output, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f} ' + f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Now on the backward pass:\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    second_layer.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(second_layer.dinputs)\n",
    "    first_layer.backward(activation_1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(first_layer)\n",
    "    optimizer.update_params(second_layer)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum in Optimization\n",
    "\n",
    "Momentum is a technique used in optimization algorithms, such as Stochastic Gradient Descent (SGD), to improve convergence and overcome challenges like local minima. Here's a brief overview:\n",
    "\n",
    "- **Purpose:** Momentum adds a moving average of past gradients to the parameter updates. This helps maintain direction and accelerates convergence.\n",
    "\n",
    "- **Effect:** Momentum smooths the optimization process, reducing oscillations, and improving convergence speed, especially in the presence of noisy gradients.\n",
    "\n",
    "- **Hyperparameter:** The momentum hyperparameter (commonly denoted as ) controls the impact of past gradients. Typical values range between 0.8 and 0.99.\n",
    "\n",
    "- **Benefits:** Momentum is particularly effective in training deep neural networks and escaping local minima.\n",
    "\n",
    "- **Challenges:** The choice of the momentum value requires tuning, and too high a value can lead to overshooting.\n",
    "\n",
    "Momentum is a valuable tool in optimizing neural networks, aiding faster convergence and better solutions by utilizing historical gradient information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay = 0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                # If momentums don't exist insite layer, create them.\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else: # This will become the version of optimizer without momentum    \n",
    "            layer.weight_updates = -self.learning_rate * layer.dweights\n",
    "            layer.bias_updates = -self.learning_rate * layer.dbiases\n",
    "        \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.092, loss: 14.379 lr: 0.01\n",
      "epoch: 100, acc: 0.318, loss: 1.866 lr: 0.005025125628140704\n",
      "epoch: 200, acc: 0.355, loss: 1.765 lr: 0.0033444816053511705\n",
      "epoch: 300, acc: 0.369, loss: 1.718 lr: 0.002506265664160401\n",
      "epoch: 400, acc: 0.374, loss: 1.695 lr: 0.002004008016032064\n",
      "epoch: 500, acc: 0.381, loss: 1.674 lr: 0.001669449081803005\n",
      "epoch: 600, acc: 0.386, loss: 1.656 lr: 0.0014306151645207437\n",
      "epoch: 700, acc: 0.389, loss: 1.645 lr: 0.0012515644555694619\n",
      "epoch: 800, acc: 0.393, loss: 1.635 lr: 0.0011123470522803114\n",
      "epoch: 900, acc: 0.395, loss: 1.627 lr: 0.001001001001001001\n",
      "epoch: 1000, acc: 0.399, loss: 1.620 lr: 0.0009099181073703366\n"
     ]
    }
   ],
   "source": [
    "# And we use this class as another building block inside a loop.\n",
    "X = np.array([train_X[i].ravel() for i in range(5000)])\n",
    "y = np.array([train_y[i] for i in range(5000)])\n",
    "\n",
    "# Create layers\n",
    "first_layer = LayerDense(784, 37) \n",
    "second_layer = LayerDense(37, 10)\n",
    "\n",
    "# Initialize activation/loss\n",
    "activation_1 = activation_ReLU()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate = 0.01, decay=0.01, momentum=0.05)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(0, 1001):\n",
    "\n",
    "    # Perform first forward pass into first layer\n",
    "    first_layer.forward(X)\n",
    "    activation_1.forward(first_layer.output) # Activate neurons based on ReLu activation function\n",
    "\n",
    "    # Perform second forward pass into second layer\n",
    "    second_layer.forward(activation_1.output) \n",
    "\n",
    "    # Perform forward pass into activation/loss function:\n",
    "    loss = loss_activation.forward(second_layer.output, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f} ' + f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Now on the backward pass:\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    second_layer.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(second_layer.dinputs)\n",
    "    first_layer.backward(activation_1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(first_layer)\n",
    "    optimizer.update_params(second_layer)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad in Optimization\n",
    "\n",
    "AdaGrad, short for Adaptive Gradient Descent, is an optimization technique that adapts the learning rate for each parameter during training. Here's a brief overview:\n",
    "\n",
    "- **Adaptive Learning Rates:** AdaGrad adjusts the learning rate individually for each parameter based on historical gradient information. Parameters that have received large gradients in the past are assigned smaller learning rates, while those with small gradients get larger learning rates.\n",
    "\n",
    "- **Purpose:** AdaGrad addresses the challenge of selecting a fixed learning rate by automatically scaling it for each parameter. This can lead to faster convergence and robustness in training.\n",
    "\n",
    "- **Effect:** AdaGrad is effective in dealing with sparse data and non-convex optimization problems. It tends to converge quickly in the early stages of training.\n",
    "\n",
    "- **Hyperparameters:** AdaGrad introduces a hyperparameter called  (epsilon) to prevent division by zero. Typical values for  are small, e.g., 1e-7.\n",
    "\n",
    "- **Challenges:** Over time, the accumulated squared gradients may become very large, causing the learning rates to diminish to the point where learning stalls. This is known as the \"learning rate decay\" problem.\n",
    "\n",
    "- **Variants:** Modifications like RMSprop and Adam address some of the limitations of AdaGrad.\n",
    "\n",
    "AdaGrad is a valuable optimization technique, particularly in cases where choosing an appropriate learning rate can be challenging, and when dealing with data with varying scales and sparse gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adagrad:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay = 0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Update layer weights and biases\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.056, loss: 15.125 lr: 0.01\n",
      "epoch: 100, acc: 0.849, loss: 1.622 lr: 0.005025125628140704\n",
      "epoch: 200, acc: 0.891, loss: 1.106 lr: 0.0033444816053511705\n",
      "epoch: 300, acc: 0.907, loss: 0.886 lr: 0.002506265664160401\n",
      "epoch: 400, acc: 0.918, loss: 0.764 lr: 0.002004008016032064\n",
      "epoch: 500, acc: 0.926, loss: 0.687 lr: 0.001669449081803005\n",
      "epoch: 600, acc: 0.933, loss: 0.636 lr: 0.0014306151645207437\n",
      "epoch: 700, acc: 0.935, loss: 0.596 lr: 0.0012515644555694619\n",
      "epoch: 800, acc: 0.936, loss: 0.561 lr: 0.0011123470522803114\n",
      "epoch: 900, acc: 0.939, loss: 0.530 lr: 0.001001001001001001\n",
      "epoch: 1000, acc: 0.942, loss: 0.503 lr: 0.0009099181073703366\n"
     ]
    }
   ],
   "source": [
    "# And we use this class as another building block inside a loop.\n",
    "X = np.array([train_X[i].ravel() for i in range(5000)])\n",
    "y = np.array([train_y[i] for i in range(5000)])\n",
    "\n",
    "# Create layers\n",
    "first_layer = LayerDense(784, 37) \n",
    "second_layer = LayerDense(37, 10)\n",
    "\n",
    "# Initialize activation/loss\n",
    "activation_1 = activation_ReLU()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = Optimizer_Adagrad(learning_rate = 0.01, decay=0.01)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(0, 1001):\n",
    "\n",
    "    # Perform first forward pass into first layer\n",
    "    first_layer.forward(X)\n",
    "    activation_1.forward(first_layer.output) # Activate neurons based on ReLu activation function\n",
    "\n",
    "    # Perform second forward pass into second layer\n",
    "    second_layer.forward(activation_1.output) \n",
    "\n",
    "    # Perform forward pass into activation/loss function:\n",
    "    loss = loss_activation.forward(second_layer.output, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f} ' + f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Now on the backward pass:\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    second_layer.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(second_layer.dinputs)\n",
    "    first_layer.backward(activation_1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(first_layer)\n",
    "    optimizer.update_params(second_layer)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp in Optimization\n",
    "\n",
    "RMSProp, short for Root Mean Square Propagation, is an optimization algorithm used in training neural networks. It's designed to address some of the limitations of earlier optimization techniques like AdaGrad. Here's an overview:\n",
    "\n",
    "- **Adaptive Learning Rates:** RMSProp adapts the learning rates individually for each parameter during training. It dynamically scales the learning rates to improve convergence.\n",
    "\n",
    "- **Moving Average of Squared Gradients:** RMSProp maintains a moving average of the squared gradients of each parameter. This moving average serves as a scaling factor for the learning rate.\n",
    "\n",
    "- **Purpose:** RMSProp helps mitigate the learning rate decay problem encountered in AdaGrad. It is effective in handling non-stationary and noisy data.\n",
    "\n",
    "- **Hyperparameters:** RMSProp introduces a decay factor (commonly denoted as ) to control the contribution of past gradients in the moving average. A typical value for  is around 0.9. It also includes a small  (epsilon) value to prevent division by zero.\n",
    "\n",
    "- **Benefits:** RMSProp provides faster convergence compared to AdaGrad, and its adaptive learning rates make it suitable for a wide range of deep learning tasks.\n",
    "\n",
    "- **Challenges:** Proper tuning of the hyperparameters ( and ) is important to ensure optimal performance.\n",
    "\n",
    "RMSProp is a valuable optimization algorithm, offering a balance between adaptive learning rates and stability, making it a popular choice for training deep neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main difference between Adagrad and Rmsprop is on the equation of cached gradients\n",
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with rho * cached_gradients + (1 - rho) * gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Update layer weights and biases\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.062, loss: 14.876 lr: 0.01\n",
      "epoch: 100, acc: 0.703, loss: 3.706 lr: 5.0251256281407036e-05\n",
      "epoch: 200, acc: 0.753, loss: 2.864 lr: 2.5062656641604008e-05\n",
      "epoch: 300, acc: 0.784, loss: 2.453 lr: 1.669449081803005e-05\n",
      "epoch: 400, acc: 0.801, loss: 2.206 lr: 1.251564455569462e-05\n",
      "epoch: 500, acc: 0.810, loss: 2.031 lr: 1.001001001001001e-05\n",
      "epoch: 600, acc: 0.817, loss: 1.891 lr: 8.340283569641367e-06\n",
      "epoch: 700, acc: 0.825, loss: 1.776 lr: 7.147962830593281e-06\n",
      "epoch: 800, acc: 0.831, loss: 1.681 lr: 6.253908692933083e-06\n",
      "epoch: 900, acc: 0.835, loss: 1.598 lr: 5.558643690939411e-06\n",
      "epoch: 1000, acc: 0.840, loss: 1.526 lr: 5.002501250625313e-06\n"
     ]
    }
   ],
   "source": [
    "# And we use this class as another building block inside a loop.\n",
    "X = np.array([train_X[i].ravel() for i in range(5000)])\n",
    "y = np.array([train_y[i] for i in range(5000)])\n",
    "\n",
    "# Create layers\n",
    "first_layer = LayerDense(784, 37) \n",
    "second_layer = LayerDense(37, 10)\n",
    "\n",
    "# Initialize activation/loss\n",
    "activation_1 = activation_ReLU()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = Optimizer_RMSprop(learning_rate = 0.01, decay=2, rho=0.7)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(0, 1001):\n",
    "\n",
    "    # Perform first forward pass into first layer\n",
    "    first_layer.forward(X)\n",
    "    activation_1.forward(first_layer.output) # Activate neurons based on ReLu activation function\n",
    "\n",
    "    # Perform second forward pass into second layer\n",
    "    second_layer.forward(activation_1.output) \n",
    "\n",
    "    # Perform forward pass into activation/loss function:\n",
    "    loss = loss_activation.forward(second_layer.output, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f} ' + f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Now on the backward pass:\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    second_layer.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(second_layer.dinputs)\n",
    "    first_layer.backward(activation_1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(first_layer)\n",
    "    optimizer.update_params(second_layer)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam (Adaptive Moment Estimation) in Optimization\n",
    "\n",
    "Adam, short for Adaptive Moment Estimation, is a popular optimization algorithm for training neural networks. It combines the benefits of both momentum and RMSProp to accelerate convergence and improve optimization. Here's a brief overview:\n",
    "\n",
    "- **Combination of Momentum and RMSProp:** Adam maintains two moving averages, one for the gradients (like momentum) and one for the squared gradients (like RMSProp). These moving averages are used to adaptively adjust the learning rates for each parameter.\n",
    "\n",
    "- **Purpose:** Adam aims to achieve fast convergence and stable training by adapting the learning rates based on the historical gradients. It is effective for a wide range of deep learning tasks.\n",
    "\n",
    "- **Hyperparameters:** Adam includes parameters such as 1 (momentum factor), 2 (squared gradient decay), and  (epsilon) to prevent division by zero. Typical values for 1 and 2 are around 0.9 and 0.999, respectively.\n",
    "\n",
    "- **Bias Correction:** Adam applies bias correction to the moving averages to address their initialization at zero. This correction helps in stabilizing training.\n",
    "\n",
    "- **Benefits:** Adam combines the strengths of momentum and RMSProp, providing fast convergence, adaptability, and robustness to a variety of tasks.\n",
    "\n",
    "- **Challenges:** Careful tuning of hyperparameters, especially 1 and 2, is crucial for optimal performance. Adam is sensitive to the choice of these values.\n",
    "\n",
    "Adam is a widely used optimization algorithm in deep learning, offering a balance between adaptive learning rates and stability, making it a popular choice for training deep neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam is basically a RMSprop built in with a momentum functionality. Which means:\n",
    "# The main difference between Adagrad and Rmsprop is on the equation of cached gradients\n",
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7, beta1=0.9, beta2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentums with current gradients\n",
    "        layer.weight_momentums = self.beta1 * layer.weight_momentums + (1 - self.beta1) * layer.dweights**2\n",
    "        layer.bias_momentums = self.beta1 * layer.bias_momentums + (1 - self.beta1) * layer.dbiases**2\n",
    "\n",
    "        # Corrected momentums\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update cached gradients\n",
    "        layer.weight_cache = self.beta2 * layer.weight_cache + (1 - self.beta2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta2 * layer.bias_cache + (1 - self.beta2) * layer.dbiases**2\n",
    "\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "\n",
    "\n",
    "        # Update layer weights and biases\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.091, loss: 14.315 lr: 0.0001\n",
      "epoch: 100, acc: 0.167, loss: 12.175 lr: 5.0251256281407036e-05\n",
      "epoch: 200, acc: 0.178, loss: 11.369 lr: 3.3444816053511705e-05\n",
      "epoch: 300, acc: 0.183, loss: 10.553 lr: 2.506265664160401e-05\n",
      "epoch: 400, acc: 0.183, loss: 9.926 lr: 2.004008016032064e-05\n",
      "epoch: 500, acc: 0.182, loss: 9.341 lr: 1.669449081803005e-05\n",
      "epoch: 600, acc: 0.183, loss: 8.855 lr: 1.4306151645207439e-05\n",
      "epoch: 700, acc: 0.180, loss: 8.455 lr: 1.251564455569462e-05\n",
      "epoch: 800, acc: 0.180, loss: 8.114 lr: 1.1123470522803115e-05\n",
      "epoch: 900, acc: 0.182, loss: 7.822 lr: 1.001001001001001e-05\n",
      "epoch: 1000, acc: 0.186, loss: 7.579 lr: 9.099181073703366e-06\n"
     ]
    }
   ],
   "source": [
    "# And we use this class as another building block inside a loop.\n",
    "X = np.array([train_X[i].ravel() for i in range(5000)])\n",
    "y = np.array([train_y[i] for i in range(5000)])\n",
    "\n",
    "# Create layers\n",
    "first_layer = LayerDense(784, 37) \n",
    "second_layer = LayerDense(37, 10)\n",
    "\n",
    "# Initialize activation/loss\n",
    "activation_1 = activation_ReLU()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate = 0.0001, decay=0.01, beta1=0.9, beta2=0.999)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(0, 1001):\n",
    "\n",
    "    # Perform first forward pass into first layer\n",
    "    first_layer.forward(X)\n",
    "    activation_1.forward(first_layer.output) # Activate neurons based on ReLu activation function\n",
    "\n",
    "    # Perform second forward pass into second layer\n",
    "    second_layer.forward(activation_1.output) \n",
    "\n",
    "    # Perform forward pass into activation/loss function:\n",
    "    loss = loss_activation.forward(second_layer.output, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f} ' + f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Now on the backward pass:\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    second_layer.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(second_layer.dinputs)\n",
    "    first_layer.backward(activation_1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(first_layer)\n",
    "    optimizer.update_params(second_layer)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm having major issues on selecting the best set of hyperparameters for the Adam optimizer. Rms and Adagrad worked best into optimizing the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing in Out of Sample data\n",
    "\n",
    "In this section I'll test my NN model on some out of sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.062, loss: 14.881 lr: 0.01\n",
      "epoch: 100, acc: 0.721, loss: 1.806 lr: 0.005025125628140704\n",
      "epoch: 200, acc: 0.744, loss: 1.299 lr: 0.0033444816053511705\n",
      "epoch: 300, acc: 0.756, loss: 1.106 lr: 0.002506265664160401\n",
      "epoch: 400, acc: 0.768, loss: 1.001 lr: 0.002004008016032064\n",
      "epoch: 500, acc: 0.774, loss: 0.934 lr: 0.001669449081803005\n",
      "epoch: 600, acc: 0.779, loss: 0.889 lr: 0.0014306151645207437\n",
      "epoch: 700, acc: 0.783, loss: 0.854 lr: 0.0012515644555694619\n",
      "epoch: 800, acc: 0.789, loss: 0.827 lr: 0.0011123470522803114\n",
      "epoch: 900, acc: 0.794, loss: 0.803 lr: 0.001001001001001001\n",
      "epoch: 1000, acc: 0.796, loss: 0.784 lr: 0.0009099181073703366\n"
     ]
    }
   ],
   "source": [
    "# And we use this class as another building block inside a loop.\n",
    "X = np.array([train_X[i].ravel() for i in range(10000)])\n",
    "y = np.array([train_y[i] for i in range(10000)])\n",
    "\n",
    "# Create layers\n",
    "first_layer = LayerDense(784, 37) \n",
    "second_layer = LayerDense(37, 10)\n",
    "\n",
    "# Initialize activation/loss\n",
    "activation_1 = activation_ReLU()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = Optimizer_Adagrad(learning_rate = 0.01, decay=0.01)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(0, 1001):\n",
    "\n",
    "    # Perform first forward pass into first layer\n",
    "    first_layer.forward(X)\n",
    "    activation_1.forward(first_layer.output) # Activate neurons based on ReLu activation function\n",
    "\n",
    "    # Perform second forward pass into second layer\n",
    "    second_layer.forward(activation_1.output) \n",
    "\n",
    "    # Perform forward pass into activation/loss function:\n",
    "    loss = loss_activation.forward(second_layer.output, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f} ' + f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Now on the backward pass:\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    second_layer.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(second_layer.dinputs)\n",
    "    first_layer.backward(activation_1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(first_layer)\n",
    "    optimizer.update_params(second_layer)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.700, loss: 1.605\n"
     ]
    }
   ],
   "source": [
    "X = np.array([test_X[i].ravel() for i in range(1000)])\n",
    "y = np.array([test_y[i] for i in range(1000)])\n",
    "\n",
    "# Perform first forward pass into first layer\n",
    "first_layer.forward(X)\n",
    "activation_1.forward(first_layer.output) \n",
    "\n",
    "# Perform second forward pass into second layer\n",
    "second_layer.forward(activation_1.output) \n",
    "\n",
    "# Perform forward pass into activation/loss function:\n",
    "loss = loss_activation.forward(second_layer.output, y)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}') # Seem's like the model is overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing some inputs and predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 10, figsize=(20, 3))\n",
    "\n",
    "for i in range(10):\n",
    "    # Display model prediction\n",
    "    axes[i].set_title(f\"Prediction: {predictions[i]:.2f}\")\n",
    "    \n",
    "    # Display the image\n",
    "    axes[i].imshow(X[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "    axes[i].axis('off')  # Turn off axis labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the NN is doing a good job on very distinct numbers, but it's having an issue on similar ones (like 9 and 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.094, loss: 3.251 lr: 0.01\n",
      "epoch: 100, acc: 0.924, loss: 0.271 lr: 0.005025125628140704\n",
      "epoch: 200, acc: 0.939, loss: 0.220 lr: 0.0033444816053511705\n",
      "epoch: 300, acc: 0.944, loss: 0.196 lr: 0.002506265664160401\n",
      "epoch: 400, acc: 0.949, loss: 0.182 lr: 0.002004008016032064\n",
      "epoch: 500, acc: 0.952, loss: 0.172 lr: 0.001669449081803005\n",
      "epoch: 600, acc: 0.954, loss: 0.164 lr: 0.0014306151645207437\n",
      "epoch: 700, acc: 0.956, loss: 0.159 lr: 0.0012515644555694619\n",
      "epoch: 800, acc: 0.958, loss: 0.154 lr: 0.0011123470522803114\n",
      "epoch: 900, acc: 0.959, loss: 0.150 lr: 0.001001001001001001\n",
      "epoch: 1000, acc: 0.961, loss: 0.146 lr: 0.0009099181073703366\n"
     ]
    }
   ],
   "source": [
    "# As it seems, I can improve model performance by scaling input data. I'll do it now and check:\n",
    "\n",
    "# And we use this class as another building block inside a loop.\n",
    "X = np.array([train_X[i].ravel() for i in range(10000)])\n",
    "# Scaling my input layer:\n",
    "X = (X - np.mean(X, axis = 1, keepdims=True)) / np.std(X, axis = 1, keepdims=True)\n",
    "\n",
    "\n",
    "y = np.array([train_y[i] for i in range(10000)])\n",
    "\n",
    "# Create layers\n",
    "first_layer = LayerDense(784, 37) \n",
    "second_layer = LayerDense(37, 10)\n",
    "\n",
    "# Initialize activation/loss\n",
    "activation_1 = activation_ReLU()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = Optimizer_Adagrad(learning_rate = 0.01, decay=0.01)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(0, 1001):\n",
    "\n",
    "    # Perform first forward pass into first layer\n",
    "    first_layer.forward(X)\n",
    "    activation_1.forward(first_layer.output) # Activate neurons based on ReLu activation function\n",
    "\n",
    "    # Perform second forward pass into second layer\n",
    "    second_layer.forward(activation_1.output) \n",
    "\n",
    "    # Perform forward pass into activation/loss function:\n",
    "    loss = loss_activation.forward(second_layer.output, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f} ' + f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Now on the backward pass:\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    second_layer.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(second_layer.dinputs)\n",
    "    first_layer.backward(activation_1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(first_layer)\n",
    "    optimizer.update_params(second_layer)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.915, loss: 0.271\n"
     ]
    }
   ],
   "source": [
    "X = np.array([test_X[i].ravel() for i in range(1000)])\n",
    "X = (X - np.mean(X, axis = 1, keepdims=True)) / np.std(X, axis = 1, keepdims=True)\n",
    "y = np.array([test_y[i] for i in range(1000)])\n",
    "\n",
    "# Perform first forward pass into first layer\n",
    "first_layer.forward(X)\n",
    "activation_1.forward(first_layer.output) \n",
    "\n",
    "# Perform second forward pass into second layer\n",
    "second_layer.forward(activation_1.output) \n",
    "\n",
    "# Perform forward pass into activation/loss function:\n",
    "loss = loss_activation.forward(second_layer.output, y)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}') # Seem's like the model is not overfitting anymore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 10, figsize=(20, 3))\n",
    "\n",
    "for i in range(10):\n",
    "    # Display model prediction\n",
    "    axes[i].set_title(f\"Prediction: {predictions[i]:.2f}\")\n",
    "    \n",
    "    # Display the image\n",
    "    axes[i].imshow(X[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "    axes[i].axis('off')  # Turn off axis labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In the journey to build a neural network from scratch, I embarked on a quest to understand the intricate workings of deep learning, and the result is a remarkable achievementa neural network model capable of predicting with an impressive 91.5% accuracy. This accomplishment was attained using only the fundamental library of `numpy`, highlighting the power of pure, foundational techniques.\n",
    "\n",
    "Throughout this project, I delved into a multitude of concepts, algorithms, and techniques, each contributing to the grand tapestry of neural network development. From data preprocessing and the MNIST dataset exploration to the inner workings of forward and backward propagation, I unraveled the secrets behind the black box of deep learning.\n",
    "\n",
    "Here are some of the key takeaways from this project:\n",
    "\n",
    "## Data Preprocessing Matters\n",
    "I learned the crucial role that data preprocessing plays in the success of a neural network. Scaling and normalizing the input data, as well as transforming labels, had a significant impact on the model's performance.\n",
    "\n",
    "## Building Blocks of a Neural Network\n",
    "From creating custom layers, activation functions, loss functions, and optimizers, I assembled the essential components that form the building blocks of any neural network.\n",
    "\n",
    "## Optimization Techniques\n",
    "I explored various optimization techniques, including gradient descent, momentum, RMSprop, AdaGrad, and Adam. Each optimizer had its strengths and weaknesses, emphasizing the importance of choosing the right optimization strategy for a given problem.\n",
    "\n",
    "## Deep Insights into Backpropagation\n",
    "The heart of neural network training lies in backpropagation. Understanding how gradients flow through the network and how to update weights and biases with minimal error was pivotal in achieving high accuracy.\n",
    "\n",
    "## The Impact of Hyperparameters\n",
    "Fine-tuning hyperparameters, such as learning rates and decay rates, proved to be a crucial part of optimizing the model's training process. The choice of these values can make or break the convergence of the network.\n",
    "\n",
    "## Scaling for Success\n",
    "I recognized the transformative power of scaling data to a range of -1 to 1 or 0 to 1, aligning the data's distribution with the expectations of neural network architectures. This simple yet impactful step played a significant role in reaching a 91.5% accuracy.\n",
    "\n",
    "In summary, this project has been a voyage of discovery and learning. From the basic principles of neural networks to advanced optimization techniques, I navigated through the complex landscape of deep learning. Achieving a 91.5% accuracy rate with only `numpy` as the tool of choice is not just an accomplishment; it's a testament to the depth of understanding and mastery gained during this project.\n",
    "\n",
    "As I conclude this journey, I'm reminded that the world of deep learning is ever-evolving, with endless opportunities for exploration and innovation. I look forward to applying these newfound skills and insights to even more challenging problems in the future. The adventure is far from over, and the possibilities are limitless.\n",
    "\n",
    "This project is a testament to the power of curiosity, determination, and the pursuit of knowledge. It's not just the end; it's a new beginning in the world of neural networks and artificial intelligence.\n",
    "\n",
    "\n",
    "# Continuing the Journey\n",
    "\n",
    "This project marks a significant milestone in my exploration of neural networks and deep learning. It's a foundation that I plan to build upon in the future. As I wrap up this project, I'm not bidding farewell to this fascinating world but rather setting the stage for the next chapter.\n",
    "\n",
    "In the coming days, I'm committed to expanding and refining the knowledge and expertise I've gained here. The world of artificial intelligence and deep learning is ever-evolving, with new challenges and opportunities emerging continuously. My intention is to stay at the forefront of these advancements and apply the skills I've acquired to tackle even more complex and impactful projects.\n",
    "\n",
    "With every line of code, every experiment, and every neural network I design, I'll be pushing the boundaries of what's possible and delving deeper into the mysteries of artificial intelligence. My journey doesn't end here; it's an ongoing adventure that promises new insights and breakthroughs.\n",
    "\n",
    "I'm excited about the future, and I'm looking forward to the projects and challenges that lie ahead. The knowledge and experience gained from this project are the stepping stones for what's to come. I invite you to stay tuned for the next chapter, where I'll continue to unravel the secrets of deep learning and bring them to life in innovative ways.\n",
    "\n",
    "Thank you for being a part of this journey. The best is yet to come!\n",
    "\n",
    "Onward and upward!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbfbd03a87395819c26610a7c68c8cab9bc3d3efa5727999ef7714fdb2cb4f26"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit ('3.10.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
