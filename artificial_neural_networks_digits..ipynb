{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://prompthero.com/rails/active_storage/representations/proxy/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaEpJaWs0TlROa1pERmlZaTAyTnpNNUxUUXdZakl0T1Raall5MDBPRE0wT1RjMFlXSTBZMkVHT2daRlZBPT0iLCJleHAiOm51bGwsInB1ciI6ImJsb2JfaWQifX0=--ed60df1d7058c4312da9e9da344e6d9671567582/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaDdDRG9MWm05eWJXRjBPZ2wzWldKd09oUnlaWE5wZW1WZmRHOWZiR2x0YVhSYkIya0NBQWd3T2dwellYWmxjbnNKT2hOemRXSnpZVzF3YkdWZmJXOWtaVWtpQjI5dUJqb0dSVlE2Q25OMGNtbHdWRG9PYVc1MFpYSnNZV05sVkRvTWNYVmhiR2wwZVdsZiIsImV4cCI6bnVsbCwicHVyIjoidmFyaWF0aW9uIn19--935666d13f63ed5aca9daa2416340e3a90b6014e/prompthero-prompt-e15d01779e4.png)\n",
    "\n",
    "*Image generated by Stable Diffusion, on my local machine, published on prompthero.*\n",
    "\n",
    "# Building a Neural Network from Scratch in Python\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, we embark on a journey to demystify the inner workings of neural networks by building one from the ground up using Python. Neural networks are the backbone of modern machine learning and artificial intelligence, making this project not only educational but also essential for anyone seeking a deeper understanding of the field.\n",
    "\n",
    "As I'm starting this project for educational purposes and I don't have a strong background on neural networks, I'll be following a video playlist from [sentdex](https://www.youtube.com/@sentdex). I also recommend his [pythonprogramming](https://pythonprogramming.net/) website if you wish to learn python.\n",
    "\n",
    "## Project Goals\n",
    "\n",
    "My primary objectives in this project include:\n",
    "\n",
    "1. **Understanding the Fundamentals:** I'll delve into the core concepts of neural networks, such as neurons, activation functions, forward and backward propagation, and gradient descent.\n",
    "\n",
    "2. **Implementing Key Components:** Building neural networks from scratch will involve creating our own Python classes for layers, loss functions, and optimizers, allowing us to control every aspect of the network.\n",
    "\n",
    "3. **Training a Model:** We will use our homemade neural network to train on a real-world dataset, possibly a simplified version of a popular benchmark dataset like MNIST.\n",
    "\n",
    "4. **Evaluating Performance:** We will assess the model's performance in terms of accuracy, loss, and other relevant metrics.\n",
    "\n",
    "5. **Visualizing the Learning Process:** We aim to create visualizations of how our network learns, which will help us gain insights into the training process.\n",
    "\n",
    "## Technologies Used\n",
    "\n",
    "- **Python:** The primary programming language for this project.\n",
    "- **NumPy:** For numerical operations and matrix calculations.\n",
    "- **Matplotlib:** For data visualization and creating plots.\n",
    "- **Pandas** For data manipulation in tabular format.\n",
    "- **Jupyter Notebook:** For interactive development and documentation.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "The project will be organized into the following main components:\n",
    "\n",
    "- **Data Preparation:** Loading and preprocessing the dataset.\n",
    "- **Neural Network Architecture:** Defining the architecture of our neural network, including the number of layers, neurons, and activation functions.\n",
    "- **Forward and Backward Propagation:** Implementing the forward and backward passes for training the network.\n",
    "- **Loss Functions and Optimizers:** Creating loss functions and optimizers for model training.\n",
    "- **Training and Evaluation:** Training the neural network and evaluating its performance on the dataset.\n",
    "- **Visualizations:** Creating visualizations to gain insights into the model's learning process.\n",
    "\n",
    "## Why Build from Scratch?\n",
    "\n",
    "Building a neural network from scratch provides several benefits:\n",
    "\n",
    "- **Deep Understanding:** You'll gain a deep understanding of the internal workings of neural networks, which is crucial for troubleshooting and optimizing models.\n",
    "- **Flexibility:** You have complete control over every aspect of your model, allowing you to experiment with various architectures and techniques.\n",
    "- **Educational Value:** This project serves as an excellent educational resource for data scientists and aspiring machine learning engineers.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By the end of this project, you will have not only a functional neural network but also a profound understanding of how neural networks operate. You'll be better equipped to tackle more complex machine learning tasks and make informed decisions when working with neural networks in the future.\n",
    "\n",
    "Let's dive into the fascinating world of neural networks and start building one from scratch!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MNIST](https://nvsyashwanth.github.io/machinelearningmaster/assets/images/digitsMNIST/samples.png)\n",
    "\n",
    "# The MNIST Dataset\n",
    "\n",
    "The MNIST dataset is a widely used benchmark dataset in the field of machine learning and computer vision. It consists of a large collection of handwritten digits that are commonly used for training and evaluating machine learning models, particularly in the context of image classification. The MNIST dataset is a popular choice for beginners in deep learning due to its simplicity and accessibility.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The MNIST dataset is composed of the following key characteristics:\n",
    "\n",
    "- **Images:** It contains a total of 70,000 grayscale images of handwritten digits, each measuring 28x28 pixels. These images are represented as 28x28 matrices with pixel values ranging from 0 (black) to 255 (white).\n",
    "\n",
    "- **Labels:** Each image is associated with a label, which corresponds to the digit it represents. The labels are integers from 0 to 9, making MNIST a multi-class classification problem.\n",
    "\n",
    "- **Training and Testing Sets:** The dataset is typically divided into two parts: a training set containing 60,000 images and a testing set with 10,000 images. The training set is used to train machine learning models, while the testing set is used to evaluate their performance.\n",
    "\n",
    "- **Variability:** MNIST exhibits variability in writing styles and quality of handwriting, making it a suitable dataset for assessing the robustness of classification algorithms.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The MNIST dataset serves as a fundamental benchmark for testing the effectiveness of machine learning and deep learning models. It is often used to introduce and evaluate the performance of neural networks, convolutional neural networks (CNNs), and other image classification techniques.\n",
    "\n",
    "## Accessibility\n",
    "\n",
    "The MNIST dataset is publicly available and can be easily accessed and downloaded through various libraries and frameworks in Python, such as TensorFlow and PyTorch. It is also included in popular datasets in machine learning libraries like Scikit-Learn.\n",
    "\n",
    "In this project, we will utilize the MNIST dataset to train our homemade neural network for digit classification. The objective is to achieve a high level of accuracy in recognizing handwritten digits, thus demonstrating the capability of our self-built neural network.\n",
    "\n",
    "Let's start by importing and exploring the MNIST dataset to kick off our project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 23:11:47.451283: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-26 23:11:47.522950: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-26 23:11:47.522967: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-26 23:11:48.136327: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-26 23:11:48.136372: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-26 23:11:48.136376: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data() # Importing dataset using train and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 28, 28)\n",
      "Y_train: (60000,)\n",
      "X_test:  (10000, 28, 28)\n",
      "Y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ' + str(train_X.shape))\n",
    "print('Y_train: ' + str(train_y.shape))\n",
    "print('X_test:  '  + str(test_X.shape))\n",
    "print('Y_test:  '  + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_X[0], cmap=plt.get_cmap('gray'))\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_X[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I already have a dataset to test my models, it's time to focus more on how to make everything work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding how Neuron connections work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Basically, what we'll be doing is something like this:\n",
    "inputs = [0, 0, 1, 0, 0] # This would be our input layer aka our dataset = 28 by 28 pixels image.\n",
    "weights = [0.1, 0.4, 2, 0.2, 0] # Every input have a conection to a weight that is adjustable\n",
    "bias = 3 # Another param of the model\n",
    "\n",
    "output2 = inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + inputs[3] * weights[3] + inputs[4] * weights[4] + bias # It'll all resume into this kind of operation\n",
    "\n",
    "# We can also write the above as:\n",
    "output1 = np.dot(weights, inputs) + bias\n",
    "print(output1 == output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is **just a simple neuron with 3 weights and a bias of 3!**\n",
    "\n",
    "Now, lets think of the input about our reality in the mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>126</td>\n",
       "      <td>136</td>\n",
       "      <td>175</td>\n",
       "      <td>26</td>\n",
       "      <td>166</td>\n",
       "      <td>255</td>\n",
       "      <td>247</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>94</td>\n",
       "      <td>154</td>\n",
       "      <td>170</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>225</td>\n",
       "      <td>172</td>\n",
       "      <td>253</td>\n",
       "      <td>242</td>\n",
       "      <td>195</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>238</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>251</td>\n",
       "      <td>93</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>56</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>219</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>198</td>\n",
       "      <td>182</td>\n",
       "      <td>247</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>156</td>\n",
       "      <td>107</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>205</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>253</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>253</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>190</td>\n",
       "      <td>253</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>241</td>\n",
       "      <td>225</td>\n",
       "      <td>160</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>119</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>186</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>150</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>93</td>\n",
       "      <td>252</td>\n",
       "      <td>253</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>253</td>\n",
       "      <td>249</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>130</td>\n",
       "      <td>183</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>207</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>148</td>\n",
       "      <td>229</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>250</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>114</td>\n",
       "      <td>221</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>201</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>66</td>\n",
       "      <td>213</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>198</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>171</td>\n",
       "      <td>219</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>195</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>172</td>\n",
       "      <td>226</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>244</td>\n",
       "      <td>133</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>212</td>\n",
       "      <td>135</td>\n",
       "      <td>132</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3    4    5    6    7    8    9    10   11   12   13   14  \\\n",
       "0    0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "1    0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "2    0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "3    0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "4    0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "5    0   0   0   0    0    0    0    0    0    0    0    0    3   18   18   \n",
       "6    0   0   0   0    0    0    0    0   30   36   94  154  170  253  253   \n",
       "7    0   0   0   0    0    0    0   49  238  253  253  253  253  253  253   \n",
       "8    0   0   0   0    0    0    0   18  219  253  253  253  253  253  198   \n",
       "9    0   0   0   0    0    0    0    0   80  156  107  253  253  205   11   \n",
       "10   0   0   0   0    0    0    0    0    0   14    1  154  253   90    0   \n",
       "11   0   0   0   0    0    0    0    0    0    0    0  139  253  190    2   \n",
       "12   0   0   0   0    0    0    0    0    0    0    0   11  190  253   70   \n",
       "13   0   0   0   0    0    0    0    0    0    0    0    0   35  241  225   \n",
       "14   0   0   0   0    0    0    0    0    0    0    0    0    0   81  240   \n",
       "15   0   0   0   0    0    0    0    0    0    0    0    0    0    0   45   \n",
       "16   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "17   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "18   0   0   0   0    0    0    0    0    0    0    0    0    0    0   46   \n",
       "19   0   0   0   0    0    0    0    0    0    0    0    0   39  148  229   \n",
       "20   0   0   0   0    0    0    0    0    0    0   24  114  221  253  253   \n",
       "21   0   0   0   0    0    0    0    0   23   66  213  253  253  253  253   \n",
       "22   0   0   0   0    0    0   18  171  219  253  253  253  253  195   80   \n",
       "23   0   0   0   0   55  172  226  253  253  253  253  244  133   11    0   \n",
       "24   0   0   0   0  136  253  253  253  212  135  132   16    0    0    0   \n",
       "25   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "26   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "27   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "     15   16   17   18   19   20   21   22   23  24  25  26  27  \n",
       "0     0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "1     0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "2     0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "3     0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "4     0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "5    18  126  136  175   26  166  255  247  127   0   0   0   0  \n",
       "6   253  253  253  225  172  253  242  195   64   0   0   0   0  \n",
       "7   253  253  251   93   82   82   56   39    0   0   0   0   0  \n",
       "8   182  247  241    0    0    0    0    0    0   0   0   0   0  \n",
       "9     0   43  154    0    0    0    0    0    0   0   0   0   0  \n",
       "10    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "11    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "12    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "13  160  108    1    0    0    0    0    0    0   0   0   0   0  \n",
       "14  253  253  119   25    0    0    0    0    0   0   0   0   0  \n",
       "15  186  253  253  150   27    0    0    0    0   0   0   0   0  \n",
       "16   16   93  252  253  187    0    0    0    0   0   0   0   0  \n",
       "17    0    0  249  253  249   64    0    0    0   0   0   0   0  \n",
       "18  130  183  253  253  207    2    0    0    0   0   0   0   0  \n",
       "19  253  253  253  250  182    0    0    0    0   0   0   0   0  \n",
       "20  253  253  201   78    0    0    0    0    0   0   0   0   0  \n",
       "21  198   81    2    0    0    0    0    0    0   0   0   0   0  \n",
       "22    9    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "23    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "24    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "25    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "26    0    0    0    0    0    0    0    0    0   0   0   0   0  \n",
       "27    0    0    0    0    0    0    0    0    0   0   0   0   0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As said before, all inputs are represented by a image, consisting of 28x28 different numbers:\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame(train_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For me, It's now crystal clear what issue some modern models face and why image compressing is so amazing. If I had a higher quality image, my inputs will become more and more complex. I'd have to find a way of representing those images in a more efficient way.\n",
    "\n",
    "But that's definitively a topic for another moment. For this project, I'll just make a functional model with \"arcaic\" techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.180770</td>\n",
       "      <td>-0.725617</td>\n",
       "      <td>0.445365</td>\n",
       "      <td>0.070050</td>\n",
       "      <td>0.828574</td>\n",
       "      <td>-0.299864</td>\n",
       "      <td>1.350800</td>\n",
       "      <td>-0.062062</td>\n",
       "      <td>-0.984203</td>\n",
       "      <td>1.574879</td>\n",
       "      <td>-0.504978</td>\n",
       "      <td>-1.528970</td>\n",
       "      <td>-0.307319</td>\n",
       "      <td>0.430492</td>\n",
       "      <td>-0.575498</td>\n",
       "      <td>-0.146121</td>\n",
       "      <td>-0.626692</td>\n",
       "      <td>-0.535901</td>\n",
       "      <td>-0.551767</td>\n",
       "      <td>-1.287639</td>\n",
       "      <td>-2.573967</td>\n",
       "      <td>1.392734</td>\n",
       "      <td>-1.165491</td>\n",
       "      <td>-1.160244</td>\n",
       "      <td>0.931404</td>\n",
       "      <td>-0.193218</td>\n",
       "      <td>-0.826734</td>\n",
       "      <td>1.016017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.189212</td>\n",
       "      <td>1.160284</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>2.221204</td>\n",
       "      <td>-0.144592</td>\n",
       "      <td>0.265980</td>\n",
       "      <td>-0.405230</td>\n",
       "      <td>-1.442757</td>\n",
       "      <td>0.919349</td>\n",
       "      <td>0.628743</td>\n",
       "      <td>1.877818</td>\n",
       "      <td>0.341939</td>\n",
       "      <td>1.879802</td>\n",
       "      <td>-1.804209</td>\n",
       "      <td>1.509601</td>\n",
       "      <td>0.578200</td>\n",
       "      <td>1.820134</td>\n",
       "      <td>-1.647458</td>\n",
       "      <td>-0.071796</td>\n",
       "      <td>-1.706009</td>\n",
       "      <td>1.915033</td>\n",
       "      <td>0.576252</td>\n",
       "      <td>-0.312639</td>\n",
       "      <td>-1.531277</td>\n",
       "      <td>-1.311846</td>\n",
       "      <td>-0.484592</td>\n",
       "      <td>0.962071</td>\n",
       "      <td>-0.829936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.150172</td>\n",
       "      <td>0.563057</td>\n",
       "      <td>1.951360</td>\n",
       "      <td>1.171090</td>\n",
       "      <td>-1.098743</td>\n",
       "      <td>1.223695</td>\n",
       "      <td>-0.924246</td>\n",
       "      <td>-0.119900</td>\n",
       "      <td>-1.184617</td>\n",
       "      <td>-0.701804</td>\n",
       "      <td>-0.206351</td>\n",
       "      <td>0.134746</td>\n",
       "      <td>-0.797138</td>\n",
       "      <td>-0.592275</td>\n",
       "      <td>0.056639</td>\n",
       "      <td>-0.055050</td>\n",
       "      <td>-0.078291</td>\n",
       "      <td>1.043392</td>\n",
       "      <td>-0.129741</td>\n",
       "      <td>-0.859656</td>\n",
       "      <td>2.065681</td>\n",
       "      <td>-0.354718</td>\n",
       "      <td>0.054670</td>\n",
       "      <td>-0.323433</td>\n",
       "      <td>0.586366</td>\n",
       "      <td>-1.078199</td>\n",
       "      <td>1.216436</td>\n",
       "      <td>-0.414490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.973851</td>\n",
       "      <td>0.426298</td>\n",
       "      <td>1.498135</td>\n",
       "      <td>-0.513920</td>\n",
       "      <td>-1.712537</td>\n",
       "      <td>-0.149118</td>\n",
       "      <td>0.679096</td>\n",
       "      <td>-1.413578</td>\n",
       "      <td>0.407482</td>\n",
       "      <td>0.809017</td>\n",
       "      <td>1.555142</td>\n",
       "      <td>0.582634</td>\n",
       "      <td>-0.421997</td>\n",
       "      <td>1.873648</td>\n",
       "      <td>-0.480006</td>\n",
       "      <td>-1.025328</td>\n",
       "      <td>0.834600</td>\n",
       "      <td>-0.534857</td>\n",
       "      <td>1.319629</td>\n",
       "      <td>0.347435</td>\n",
       "      <td>-1.092914</td>\n",
       "      <td>-0.563440</td>\n",
       "      <td>-0.579150</td>\n",
       "      <td>-0.464330</td>\n",
       "      <td>-0.239029</td>\n",
       "      <td>-0.455215</td>\n",
       "      <td>0.454378</td>\n",
       "      <td>-0.797240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.142586</td>\n",
       "      <td>-0.217872</td>\n",
       "      <td>-0.772235</td>\n",
       "      <td>-1.055171</td>\n",
       "      <td>-0.721575</td>\n",
       "      <td>2.416938</td>\n",
       "      <td>-0.348178</td>\n",
       "      <td>-0.866582</td>\n",
       "      <td>-0.680252</td>\n",
       "      <td>0.076620</td>\n",
       "      <td>1.575977</td>\n",
       "      <td>-0.151005</td>\n",
       "      <td>-1.697484</td>\n",
       "      <td>-1.136693</td>\n",
       "      <td>1.974582</td>\n",
       "      <td>0.934887</td>\n",
       "      <td>0.492387</td>\n",
       "      <td>0.299708</td>\n",
       "      <td>-0.164821</td>\n",
       "      <td>-0.199286</td>\n",
       "      <td>-2.334906</td>\n",
       "      <td>1.207757</td>\n",
       "      <td>-0.513191</td>\n",
       "      <td>0.345445</td>\n",
       "      <td>-0.154082</td>\n",
       "      <td>-0.515849</td>\n",
       "      <td>2.296197</td>\n",
       "      <td>-0.860005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.521443</td>\n",
       "      <td>-0.437600</td>\n",
       "      <td>-1.062279</td>\n",
       "      <td>0.854550</td>\n",
       "      <td>0.200017</td>\n",
       "      <td>1.391941</td>\n",
       "      <td>-0.347696</td>\n",
       "      <td>1.113370</td>\n",
       "      <td>-0.421756</td>\n",
       "      <td>-1.190892</td>\n",
       "      <td>-0.443061</td>\n",
       "      <td>-0.926813</td>\n",
       "      <td>1.291118</td>\n",
       "      <td>1.129925</td>\n",
       "      <td>-0.632991</td>\n",
       "      <td>-0.108827</td>\n",
       "      <td>0.713042</td>\n",
       "      <td>-1.342310</td>\n",
       "      <td>-0.280886</td>\n",
       "      <td>-0.335637</td>\n",
       "      <td>-0.291654</td>\n",
       "      <td>-0.559678</td>\n",
       "      <td>1.157100</td>\n",
       "      <td>-0.849495</td>\n",
       "      <td>0.100137</td>\n",
       "      <td>-0.738119</td>\n",
       "      <td>0.118803</td>\n",
       "      <td>1.640540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.333707</td>\n",
       "      <td>-0.360498</td>\n",
       "      <td>0.780280</td>\n",
       "      <td>2.137184</td>\n",
       "      <td>0.768863</td>\n",
       "      <td>0.565805</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>0.627454</td>\n",
       "      <td>-0.608304</td>\n",
       "      <td>-1.067726</td>\n",
       "      <td>0.518428</td>\n",
       "      <td>-0.121007</td>\n",
       "      <td>0.861916</td>\n",
       "      <td>1.252288</td>\n",
       "      <td>1.941582</td>\n",
       "      <td>-1.072269</td>\n",
       "      <td>2.335880</td>\n",
       "      <td>0.780074</td>\n",
       "      <td>-1.008134</td>\n",
       "      <td>2.471164</td>\n",
       "      <td>0.078486</td>\n",
       "      <td>-2.125618</td>\n",
       "      <td>-0.701389</td>\n",
       "      <td>-0.567883</td>\n",
       "      <td>0.471079</td>\n",
       "      <td>-0.633024</td>\n",
       "      <td>-1.782633</td>\n",
       "      <td>-0.625173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.219664</td>\n",
       "      <td>-0.501804</td>\n",
       "      <td>-0.719543</td>\n",
       "      <td>1.372761</td>\n",
       "      <td>-0.949213</td>\n",
       "      <td>-0.002983</td>\n",
       "      <td>1.740195</td>\n",
       "      <td>-0.615821</td>\n",
       "      <td>-0.240422</td>\n",
       "      <td>0.327880</td>\n",
       "      <td>-1.478137</td>\n",
       "      <td>-0.177940</td>\n",
       "      <td>0.615955</td>\n",
       "      <td>-0.176535</td>\n",
       "      <td>0.504721</td>\n",
       "      <td>0.666974</td>\n",
       "      <td>0.705058</td>\n",
       "      <td>-0.106926</td>\n",
       "      <td>-1.420229</td>\n",
       "      <td>-0.684839</td>\n",
       "      <td>-0.969005</td>\n",
       "      <td>-0.099692</td>\n",
       "      <td>0.688492</td>\n",
       "      <td>-1.181868</td>\n",
       "      <td>1.597547</td>\n",
       "      <td>0.367169</td>\n",
       "      <td>0.980986</td>\n",
       "      <td>0.567836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.415216</td>\n",
       "      <td>0.018498</td>\n",
       "      <td>0.195470</td>\n",
       "      <td>-1.122029</td>\n",
       "      <td>-0.501876</td>\n",
       "      <td>-1.203879</td>\n",
       "      <td>-0.021424</td>\n",
       "      <td>-0.720352</td>\n",
       "      <td>-0.922835</td>\n",
       "      <td>0.939161</td>\n",
       "      <td>0.761670</td>\n",
       "      <td>0.257282</td>\n",
       "      <td>0.652921</td>\n",
       "      <td>0.489220</td>\n",
       "      <td>-0.574361</td>\n",
       "      <td>-0.149573</td>\n",
       "      <td>-0.917420</td>\n",
       "      <td>-0.192925</td>\n",
       "      <td>2.384782</td>\n",
       "      <td>-0.258051</td>\n",
       "      <td>0.739488</td>\n",
       "      <td>-0.512682</td>\n",
       "      <td>1.380393</td>\n",
       "      <td>-0.821698</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.395070</td>\n",
       "      <td>0.819332</td>\n",
       "      <td>0.733975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.125175</td>\n",
       "      <td>-0.148910</td>\n",
       "      <td>-0.452866</td>\n",
       "      <td>1.438826</td>\n",
       "      <td>1.617654</td>\n",
       "      <td>-0.502696</td>\n",
       "      <td>0.786557</td>\n",
       "      <td>-1.026812</td>\n",
       "      <td>0.582041</td>\n",
       "      <td>0.220147</td>\n",
       "      <td>1.253764</td>\n",
       "      <td>-1.287540</td>\n",
       "      <td>0.338014</td>\n",
       "      <td>-0.315344</td>\n",
       "      <td>-1.317225</td>\n",
       "      <td>1.958934</td>\n",
       "      <td>-0.048866</td>\n",
       "      <td>0.814357</td>\n",
       "      <td>0.498720</td>\n",
       "      <td>0.394151</td>\n",
       "      <td>-0.388839</td>\n",
       "      <td>-0.698090</td>\n",
       "      <td>0.139059</td>\n",
       "      <td>-0.508969</td>\n",
       "      <td>-0.181753</td>\n",
       "      <td>-0.667118</td>\n",
       "      <td>-0.838526</td>\n",
       "      <td>-1.399885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.226035</td>\n",
       "      <td>0.405911</td>\n",
       "      <td>0.974983</td>\n",
       "      <td>2.333066</td>\n",
       "      <td>-1.175539</td>\n",
       "      <td>1.231637</td>\n",
       "      <td>0.483249</td>\n",
       "      <td>2.037303</td>\n",
       "      <td>-0.408995</td>\n",
       "      <td>0.705700</td>\n",
       "      <td>0.271625</td>\n",
       "      <td>-0.499419</td>\n",
       "      <td>0.177552</td>\n",
       "      <td>-0.885727</td>\n",
       "      <td>0.750649</td>\n",
       "      <td>0.763695</td>\n",
       "      <td>-1.908094</td>\n",
       "      <td>-0.232801</td>\n",
       "      <td>-0.345197</td>\n",
       "      <td>-0.775872</td>\n",
       "      <td>0.065632</td>\n",
       "      <td>-1.273640</td>\n",
       "      <td>0.555991</td>\n",
       "      <td>-0.009064</td>\n",
       "      <td>0.463395</td>\n",
       "      <td>-1.451687</td>\n",
       "      <td>0.168665</td>\n",
       "      <td>0.448957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.043556</td>\n",
       "      <td>-0.501340</td>\n",
       "      <td>-1.588891</td>\n",
       "      <td>-0.182021</td>\n",
       "      <td>-0.992399</td>\n",
       "      <td>0.387006</td>\n",
       "      <td>0.496970</td>\n",
       "      <td>0.298482</td>\n",
       "      <td>1.135593</td>\n",
       "      <td>1.414546</td>\n",
       "      <td>-0.954487</td>\n",
       "      <td>0.415973</td>\n",
       "      <td>0.896623</td>\n",
       "      <td>-0.396031</td>\n",
       "      <td>0.020741</td>\n",
       "      <td>1.266128</td>\n",
       "      <td>0.438592</td>\n",
       "      <td>0.016628</td>\n",
       "      <td>0.948499</td>\n",
       "      <td>0.077433</td>\n",
       "      <td>0.622783</td>\n",
       "      <td>-1.115289</td>\n",
       "      <td>-0.901342</td>\n",
       "      <td>-0.361003</td>\n",
       "      <td>-0.023129</td>\n",
       "      <td>-0.865550</td>\n",
       "      <td>0.271549</td>\n",
       "      <td>0.486801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.009091</td>\n",
       "      <td>-0.522603</td>\n",
       "      <td>0.889518</td>\n",
       "      <td>0.104475</td>\n",
       "      <td>-1.350172</td>\n",
       "      <td>-1.142404</td>\n",
       "      <td>1.170929</td>\n",
       "      <td>-1.085567</td>\n",
       "      <td>0.575436</td>\n",
       "      <td>-0.807623</td>\n",
       "      <td>0.233768</td>\n",
       "      <td>0.017791</td>\n",
       "      <td>0.272069</td>\n",
       "      <td>0.603105</td>\n",
       "      <td>-2.039149</td>\n",
       "      <td>0.782383</td>\n",
       "      <td>-0.538083</td>\n",
       "      <td>0.172495</td>\n",
       "      <td>0.534780</td>\n",
       "      <td>2.383925</td>\n",
       "      <td>-0.977100</td>\n",
       "      <td>0.456526</td>\n",
       "      <td>-1.430741</td>\n",
       "      <td>0.950596</td>\n",
       "      <td>0.539873</td>\n",
       "      <td>0.192685</td>\n",
       "      <td>-0.795366</td>\n",
       "      <td>0.935720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.509212</td>\n",
       "      <td>-3.227276</td>\n",
       "      <td>-0.594590</td>\n",
       "      <td>-1.233097</td>\n",
       "      <td>-1.175759</td>\n",
       "      <td>0.874742</td>\n",
       "      <td>0.270732</td>\n",
       "      <td>0.158609</td>\n",
       "      <td>0.910339</td>\n",
       "      <td>0.490613</td>\n",
       "      <td>0.134973</td>\n",
       "      <td>-1.159512</td>\n",
       "      <td>-0.309426</td>\n",
       "      <td>0.313730</td>\n",
       "      <td>-0.637796</td>\n",
       "      <td>-0.933803</td>\n",
       "      <td>-0.305091</td>\n",
       "      <td>1.369307</td>\n",
       "      <td>-1.012252</td>\n",
       "      <td>-0.656330</td>\n",
       "      <td>0.968510</td>\n",
       "      <td>0.096117</td>\n",
       "      <td>-1.754314</td>\n",
       "      <td>-1.072766</td>\n",
       "      <td>0.581635</td>\n",
       "      <td>0.234207</td>\n",
       "      <td>0.488491</td>\n",
       "      <td>-1.723799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.423349</td>\n",
       "      <td>0.545921</td>\n",
       "      <td>-0.684972</td>\n",
       "      <td>1.005203</td>\n",
       "      <td>0.079570</td>\n",
       "      <td>-0.467714</td>\n",
       "      <td>-0.799008</td>\n",
       "      <td>-0.398534</td>\n",
       "      <td>-1.598085</td>\n",
       "      <td>-0.411175</td>\n",
       "      <td>0.517853</td>\n",
       "      <td>0.603685</td>\n",
       "      <td>0.096737</td>\n",
       "      <td>1.372294</td>\n",
       "      <td>-1.996855</td>\n",
       "      <td>0.476501</td>\n",
       "      <td>-1.465718</td>\n",
       "      <td>1.192161</td>\n",
       "      <td>-1.126743</td>\n",
       "      <td>1.609494</td>\n",
       "      <td>1.025262</td>\n",
       "      <td>-0.283920</td>\n",
       "      <td>-0.613009</td>\n",
       "      <td>0.352166</td>\n",
       "      <td>0.643955</td>\n",
       "      <td>0.315603</td>\n",
       "      <td>-0.230871</td>\n",
       "      <td>-1.241714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.032699</td>\n",
       "      <td>-0.850797</td>\n",
       "      <td>0.317651</td>\n",
       "      <td>-0.786134</td>\n",
       "      <td>-0.813722</td>\n",
       "      <td>-1.277778</td>\n",
       "      <td>1.197083</td>\n",
       "      <td>-0.281339</td>\n",
       "      <td>1.059263</td>\n",
       "      <td>0.633299</td>\n",
       "      <td>-0.168190</td>\n",
       "      <td>-0.131297</td>\n",
       "      <td>0.616278</td>\n",
       "      <td>-0.459717</td>\n",
       "      <td>0.391004</td>\n",
       "      <td>0.524558</td>\n",
       "      <td>0.630045</td>\n",
       "      <td>0.446741</td>\n",
       "      <td>0.082156</td>\n",
       "      <td>-1.018555</td>\n",
       "      <td>-0.126039</td>\n",
       "      <td>0.712315</td>\n",
       "      <td>-0.480034</td>\n",
       "      <td>-2.123426</td>\n",
       "      <td>0.182868</td>\n",
       "      <td>0.012342</td>\n",
       "      <td>-0.277380</td>\n",
       "      <td>0.989977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.084942</td>\n",
       "      <td>-0.245630</td>\n",
       "      <td>0.845521</td>\n",
       "      <td>0.322377</td>\n",
       "      <td>-0.171662</td>\n",
       "      <td>0.857747</td>\n",
       "      <td>0.613239</td>\n",
       "      <td>0.704851</td>\n",
       "      <td>-1.383253</td>\n",
       "      <td>0.202632</td>\n",
       "      <td>-0.504668</td>\n",
       "      <td>-0.741093</td>\n",
       "      <td>2.125393</td>\n",
       "      <td>0.838561</td>\n",
       "      <td>-2.145634</td>\n",
       "      <td>0.982320</td>\n",
       "      <td>0.526839</td>\n",
       "      <td>0.628083</td>\n",
       "      <td>-0.539414</td>\n",
       "      <td>-0.416067</td>\n",
       "      <td>-0.834330</td>\n",
       "      <td>-0.498684</td>\n",
       "      <td>-0.182776</td>\n",
       "      <td>0.416571</td>\n",
       "      <td>-0.328783</td>\n",
       "      <td>0.434364</td>\n",
       "      <td>0.338480</td>\n",
       "      <td>0.562978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.281826</td>\n",
       "      <td>-1.015635</td>\n",
       "      <td>-0.408268</td>\n",
       "      <td>0.269101</td>\n",
       "      <td>-0.605697</td>\n",
       "      <td>-1.578929</td>\n",
       "      <td>-0.977601</td>\n",
       "      <td>-0.132463</td>\n",
       "      <td>0.867124</td>\n",
       "      <td>1.502101</td>\n",
       "      <td>0.805789</td>\n",
       "      <td>0.702485</td>\n",
       "      <td>-0.455438</td>\n",
       "      <td>1.308410</td>\n",
       "      <td>0.270048</td>\n",
       "      <td>1.097327</td>\n",
       "      <td>0.202092</td>\n",
       "      <td>-2.579915</td>\n",
       "      <td>-0.405765</td>\n",
       "      <td>-0.667236</td>\n",
       "      <td>0.056007</td>\n",
       "      <td>-1.137569</td>\n",
       "      <td>-0.256704</td>\n",
       "      <td>0.336281</td>\n",
       "      <td>0.541181</td>\n",
       "      <td>-0.275403</td>\n",
       "      <td>-0.178412</td>\n",
       "      <td>1.690404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.467774</td>\n",
       "      <td>-0.083130</td>\n",
       "      <td>1.048290</td>\n",
       "      <td>-0.778082</td>\n",
       "      <td>1.408869</td>\n",
       "      <td>0.980099</td>\n",
       "      <td>-0.543859</td>\n",
       "      <td>0.048549</td>\n",
       "      <td>0.417971</td>\n",
       "      <td>-0.265624</td>\n",
       "      <td>-2.312240</td>\n",
       "      <td>1.630341</td>\n",
       "      <td>-0.673165</td>\n",
       "      <td>-0.013387</td>\n",
       "      <td>1.347557</td>\n",
       "      <td>0.950354</td>\n",
       "      <td>-1.960883</td>\n",
       "      <td>-0.125966</td>\n",
       "      <td>1.225044</td>\n",
       "      <td>0.638099</td>\n",
       "      <td>1.112254</td>\n",
       "      <td>1.135668</td>\n",
       "      <td>-3.053464</td>\n",
       "      <td>1.818821</td>\n",
       "      <td>-0.523055</td>\n",
       "      <td>-1.043691</td>\n",
       "      <td>0.668290</td>\n",
       "      <td>0.254054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.020631</td>\n",
       "      <td>0.533391</td>\n",
       "      <td>-0.073973</td>\n",
       "      <td>0.615957</td>\n",
       "      <td>0.404321</td>\n",
       "      <td>0.644357</td>\n",
       "      <td>-0.441596</td>\n",
       "      <td>0.207147</td>\n",
       "      <td>1.513872</td>\n",
       "      <td>-0.585399</td>\n",
       "      <td>0.253028</td>\n",
       "      <td>0.998528</td>\n",
       "      <td>-1.322388</td>\n",
       "      <td>0.228185</td>\n",
       "      <td>-0.544082</td>\n",
       "      <td>1.244309</td>\n",
       "      <td>-0.288184</td>\n",
       "      <td>0.603995</td>\n",
       "      <td>1.735645</td>\n",
       "      <td>-0.025411</td>\n",
       "      <td>-1.306812</td>\n",
       "      <td>1.461042</td>\n",
       "      <td>-0.938360</td>\n",
       "      <td>-1.437472</td>\n",
       "      <td>-0.296852</td>\n",
       "      <td>-0.772534</td>\n",
       "      <td>-0.503186</td>\n",
       "      <td>0.724818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.363973</td>\n",
       "      <td>0.023827</td>\n",
       "      <td>2.233755</td>\n",
       "      <td>-0.974193</td>\n",
       "      <td>0.520030</td>\n",
       "      <td>-0.402411</td>\n",
       "      <td>-0.273280</td>\n",
       "      <td>-0.376537</td>\n",
       "      <td>0.814402</td>\n",
       "      <td>-0.510793</td>\n",
       "      <td>0.414496</td>\n",
       "      <td>-0.153126</td>\n",
       "      <td>-0.524134</td>\n",
       "      <td>-0.573348</td>\n",
       "      <td>-0.612014</td>\n",
       "      <td>-0.693744</td>\n",
       "      <td>0.951710</td>\n",
       "      <td>2.242447</td>\n",
       "      <td>0.954628</td>\n",
       "      <td>1.170238</td>\n",
       "      <td>-0.032041</td>\n",
       "      <td>-0.399675</td>\n",
       "      <td>0.403676</td>\n",
       "      <td>-0.016116</td>\n",
       "      <td>0.074470</td>\n",
       "      <td>-0.499230</td>\n",
       "      <td>2.213183</td>\n",
       "      <td>0.522925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.074350</td>\n",
       "      <td>-0.041128</td>\n",
       "      <td>0.533089</td>\n",
       "      <td>-0.721251</td>\n",
       "      <td>-1.169157</td>\n",
       "      <td>-0.079839</td>\n",
       "      <td>-0.578904</td>\n",
       "      <td>0.727098</td>\n",
       "      <td>-0.106033</td>\n",
       "      <td>0.292085</td>\n",
       "      <td>0.924381</td>\n",
       "      <td>-2.517114</td>\n",
       "      <td>0.358028</td>\n",
       "      <td>1.678995</td>\n",
       "      <td>-0.380467</td>\n",
       "      <td>0.573286</td>\n",
       "      <td>-0.868450</td>\n",
       "      <td>-0.413685</td>\n",
       "      <td>0.602902</td>\n",
       "      <td>0.099522</td>\n",
       "      <td>0.460444</td>\n",
       "      <td>-0.013024</td>\n",
       "      <td>0.521336</td>\n",
       "      <td>-0.579375</td>\n",
       "      <td>1.888840</td>\n",
       "      <td>0.424897</td>\n",
       "      <td>-0.203791</td>\n",
       "      <td>1.190353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.672046</td>\n",
       "      <td>0.887812</td>\n",
       "      <td>0.055773</td>\n",
       "      <td>-0.254458</td>\n",
       "      <td>-0.098458</td>\n",
       "      <td>-0.472286</td>\n",
       "      <td>0.600059</td>\n",
       "      <td>1.009487</td>\n",
       "      <td>0.968074</td>\n",
       "      <td>0.240159</td>\n",
       "      <td>-0.205657</td>\n",
       "      <td>-1.569409</td>\n",
       "      <td>0.464536</td>\n",
       "      <td>-0.195910</td>\n",
       "      <td>1.112564</td>\n",
       "      <td>-2.077295</td>\n",
       "      <td>1.287946</td>\n",
       "      <td>-0.624643</td>\n",
       "      <td>-0.700934</td>\n",
       "      <td>-0.691068</td>\n",
       "      <td>0.761464</td>\n",
       "      <td>0.626980</td>\n",
       "      <td>-0.235405</td>\n",
       "      <td>0.613879</td>\n",
       "      <td>-0.351539</td>\n",
       "      <td>0.061484</td>\n",
       "      <td>1.513309</td>\n",
       "      <td>0.394492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.367356</td>\n",
       "      <td>-0.507407</td>\n",
       "      <td>-1.232345</td>\n",
       "      <td>1.303871</td>\n",
       "      <td>-0.009421</td>\n",
       "      <td>1.238967</td>\n",
       "      <td>-0.203153</td>\n",
       "      <td>-0.908616</td>\n",
       "      <td>-0.859695</td>\n",
       "      <td>0.395858</td>\n",
       "      <td>0.192023</td>\n",
       "      <td>-0.830690</td>\n",
       "      <td>0.210129</td>\n",
       "      <td>0.140116</td>\n",
       "      <td>0.425387</td>\n",
       "      <td>-1.289742</td>\n",
       "      <td>-2.473316</td>\n",
       "      <td>0.203753</td>\n",
       "      <td>-1.373570</td>\n",
       "      <td>-0.182070</td>\n",
       "      <td>-1.725644</td>\n",
       "      <td>0.185051</td>\n",
       "      <td>-0.295145</td>\n",
       "      <td>-2.074042</td>\n",
       "      <td>0.392135</td>\n",
       "      <td>-0.828411</td>\n",
       "      <td>-0.196392</td>\n",
       "      <td>0.521529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.558252</td>\n",
       "      <td>1.067067</td>\n",
       "      <td>-0.834748</td>\n",
       "      <td>-1.677283</td>\n",
       "      <td>1.999532</td>\n",
       "      <td>-0.541800</td>\n",
       "      <td>-0.188803</td>\n",
       "      <td>-2.173582</td>\n",
       "      <td>0.068015</td>\n",
       "      <td>-1.059265</td>\n",
       "      <td>1.008204</td>\n",
       "      <td>-1.218093</td>\n",
       "      <td>2.527404</td>\n",
       "      <td>0.371860</td>\n",
       "      <td>-0.022023</td>\n",
       "      <td>-2.610980</td>\n",
       "      <td>0.646358</td>\n",
       "      <td>-0.884718</td>\n",
       "      <td>0.532801</td>\n",
       "      <td>2.431324</td>\n",
       "      <td>0.489454</td>\n",
       "      <td>-0.044299</td>\n",
       "      <td>1.671273</td>\n",
       "      <td>-2.413034</td>\n",
       "      <td>0.152091</td>\n",
       "      <td>0.411253</td>\n",
       "      <td>-0.537083</td>\n",
       "      <td>0.590947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.507594</td>\n",
       "      <td>-2.795331</td>\n",
       "      <td>-0.164369</td>\n",
       "      <td>0.477207</td>\n",
       "      <td>0.042006</td>\n",
       "      <td>-0.813541</td>\n",
       "      <td>0.718524</td>\n",
       "      <td>-0.671183</td>\n",
       "      <td>-0.234588</td>\n",
       "      <td>0.208351</td>\n",
       "      <td>2.042142</td>\n",
       "      <td>0.527180</td>\n",
       "      <td>0.629408</td>\n",
       "      <td>-0.596282</td>\n",
       "      <td>-2.062655</td>\n",
       "      <td>0.127407</td>\n",
       "      <td>-0.801511</td>\n",
       "      <td>-0.673290</td>\n",
       "      <td>0.931951</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.575650</td>\n",
       "      <td>-0.307206</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>-0.029967</td>\n",
       "      <td>-0.566937</td>\n",
       "      <td>-1.385455</td>\n",
       "      <td>-0.441937</td>\n",
       "      <td>-1.333633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.493760</td>\n",
       "      <td>1.403247</td>\n",
       "      <td>0.153765</td>\n",
       "      <td>0.075028</td>\n",
       "      <td>0.360050</td>\n",
       "      <td>0.126612</td>\n",
       "      <td>0.105941</td>\n",
       "      <td>-0.437306</td>\n",
       "      <td>0.865868</td>\n",
       "      <td>1.162350</td>\n",
       "      <td>1.364443</td>\n",
       "      <td>-0.317638</td>\n",
       "      <td>-0.763270</td>\n",
       "      <td>-1.501744</td>\n",
       "      <td>0.885348</td>\n",
       "      <td>0.178920</td>\n",
       "      <td>1.429005</td>\n",
       "      <td>-0.375020</td>\n",
       "      <td>-0.292138</td>\n",
       "      <td>-2.309278</td>\n",
       "      <td>-0.824794</td>\n",
       "      <td>-0.897320</td>\n",
       "      <td>-0.221581</td>\n",
       "      <td>1.618991</td>\n",
       "      <td>0.973615</td>\n",
       "      <td>0.445378</td>\n",
       "      <td>-0.342946</td>\n",
       "      <td>1.593434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.637190</td>\n",
       "      <td>-0.147183</td>\n",
       "      <td>0.918084</td>\n",
       "      <td>1.232294</td>\n",
       "      <td>1.182894</td>\n",
       "      <td>-0.517310</td>\n",
       "      <td>0.624076</td>\n",
       "      <td>0.311839</td>\n",
       "      <td>-0.345652</td>\n",
       "      <td>-1.503745</td>\n",
       "      <td>-0.473981</td>\n",
       "      <td>0.103016</td>\n",
       "      <td>-0.641502</td>\n",
       "      <td>-0.154010</td>\n",
       "      <td>1.032721</td>\n",
       "      <td>0.012797</td>\n",
       "      <td>0.136196</td>\n",
       "      <td>1.029510</td>\n",
       "      <td>-0.254672</td>\n",
       "      <td>1.829062</td>\n",
       "      <td>1.264786</td>\n",
       "      <td>-0.522155</td>\n",
       "      <td>0.167175</td>\n",
       "      <td>1.334904</td>\n",
       "      <td>0.907921</td>\n",
       "      <td>1.882873</td>\n",
       "      <td>0.253658</td>\n",
       "      <td>1.953373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.180770 -0.725617  0.445365  0.070050  0.828574 -0.299864  1.350800   \n",
       "1   0.189212  1.160284  0.190138  2.221204 -0.144592  0.265980 -0.405230   \n",
       "2   0.150172  0.563057  1.951360  1.171090 -1.098743  1.223695 -0.924246   \n",
       "3  -0.973851  0.426298  1.498135 -0.513920 -1.712537 -0.149118  0.679096   \n",
       "4  -0.142586 -0.217872 -0.772235 -1.055171 -0.721575  2.416938 -0.348178   \n",
       "5   0.521443 -0.437600 -1.062279  0.854550  0.200017  1.391941 -0.347696   \n",
       "6   0.333707 -0.360498  0.780280  2.137184  0.768863  0.565805  0.010483   \n",
       "7  -1.219664 -0.501804 -0.719543  1.372761 -0.949213 -0.002983  1.740195   \n",
       "8  -0.415216  0.018498  0.195470 -1.122029 -0.501876 -1.203879 -0.021424   \n",
       "9   1.125175 -0.148910 -0.452866  1.438826  1.617654 -0.502696  0.786557   \n",
       "10  0.226035  0.405911  0.974983  2.333066 -1.175539  1.231637  0.483249   \n",
       "11 -0.043556 -0.501340 -1.588891 -0.182021 -0.992399  0.387006  0.496970   \n",
       "12 -1.009091 -0.522603  0.889518  0.104475 -1.350172 -1.142404  1.170929   \n",
       "13 -1.509212 -3.227276 -0.594590 -1.233097 -1.175759  0.874742  0.270732   \n",
       "14  0.423349  0.545921 -0.684972  1.005203  0.079570 -0.467714 -0.799008   \n",
       "15 -0.032699 -0.850797  0.317651 -0.786134 -0.813722 -1.277778  1.197083   \n",
       "16 -0.084942 -0.245630  0.845521  0.322377 -0.171662  0.857747  0.613239   \n",
       "17 -1.281826 -1.015635 -0.408268  0.269101 -0.605697 -1.578929 -0.977601   \n",
       "18  0.467774 -0.083130  1.048290 -0.778082  1.408869  0.980099 -0.543859   \n",
       "19  0.020631  0.533391 -0.073973  0.615957  0.404321  0.644357 -0.441596   \n",
       "20  1.363973  0.023827  2.233755 -0.974193  0.520030 -0.402411 -0.273280   \n",
       "21  0.074350 -0.041128  0.533089 -0.721251 -1.169157 -0.079839 -0.578904   \n",
       "22 -0.672046  0.887812  0.055773 -0.254458 -0.098458 -0.472286  0.600059   \n",
       "23 -0.367356 -0.507407 -1.232345  1.303871 -0.009421  1.238967 -0.203153   \n",
       "24 -0.558252  1.067067 -0.834748 -1.677283  1.999532 -0.541800 -0.188803   \n",
       "25 -0.507594 -2.795331 -0.164369  0.477207  0.042006 -0.813541  0.718524   \n",
       "26  0.493760  1.403247  0.153765  0.075028  0.360050  0.126612  0.105941   \n",
       "27 -0.637190 -0.147183  0.918084  1.232294  1.182894 -0.517310  0.624076   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0  -0.062062 -0.984203  1.574879 -0.504978 -1.528970 -0.307319  0.430492   \n",
       "1  -1.442757  0.919349  0.628743  1.877818  0.341939  1.879802 -1.804209   \n",
       "2  -0.119900 -1.184617 -0.701804 -0.206351  0.134746 -0.797138 -0.592275   \n",
       "3  -1.413578  0.407482  0.809017  1.555142  0.582634 -0.421997  1.873648   \n",
       "4  -0.866582 -0.680252  0.076620  1.575977 -0.151005 -1.697484 -1.136693   \n",
       "5   1.113370 -0.421756 -1.190892 -0.443061 -0.926813  1.291118  1.129925   \n",
       "6   0.627454 -0.608304 -1.067726  0.518428 -0.121007  0.861916  1.252288   \n",
       "7  -0.615821 -0.240422  0.327880 -1.478137 -0.177940  0.615955 -0.176535   \n",
       "8  -0.720352 -0.922835  0.939161  0.761670  0.257282  0.652921  0.489220   \n",
       "9  -1.026812  0.582041  0.220147  1.253764 -1.287540  0.338014 -0.315344   \n",
       "10  2.037303 -0.408995  0.705700  0.271625 -0.499419  0.177552 -0.885727   \n",
       "11  0.298482  1.135593  1.414546 -0.954487  0.415973  0.896623 -0.396031   \n",
       "12 -1.085567  0.575436 -0.807623  0.233768  0.017791  0.272069  0.603105   \n",
       "13  0.158609  0.910339  0.490613  0.134973 -1.159512 -0.309426  0.313730   \n",
       "14 -0.398534 -1.598085 -0.411175  0.517853  0.603685  0.096737  1.372294   \n",
       "15 -0.281339  1.059263  0.633299 -0.168190 -0.131297  0.616278 -0.459717   \n",
       "16  0.704851 -1.383253  0.202632 -0.504668 -0.741093  2.125393  0.838561   \n",
       "17 -0.132463  0.867124  1.502101  0.805789  0.702485 -0.455438  1.308410   \n",
       "18  0.048549  0.417971 -0.265624 -2.312240  1.630341 -0.673165 -0.013387   \n",
       "19  0.207147  1.513872 -0.585399  0.253028  0.998528 -1.322388  0.228185   \n",
       "20 -0.376537  0.814402 -0.510793  0.414496 -0.153126 -0.524134 -0.573348   \n",
       "21  0.727098 -0.106033  0.292085  0.924381 -2.517114  0.358028  1.678995   \n",
       "22  1.009487  0.968074  0.240159 -0.205657 -1.569409  0.464536 -0.195910   \n",
       "23 -0.908616 -0.859695  0.395858  0.192023 -0.830690  0.210129  0.140116   \n",
       "24 -2.173582  0.068015 -1.059265  1.008204 -1.218093  2.527404  0.371860   \n",
       "25 -0.671183 -0.234588  0.208351  2.042142  0.527180  0.629408 -0.596282   \n",
       "26 -0.437306  0.865868  1.162350  1.364443 -0.317638 -0.763270 -1.501744   \n",
       "27  0.311839 -0.345652 -1.503745 -0.473981  0.103016 -0.641502 -0.154010   \n",
       "\n",
       "          14        15        16        17        18        19        20  \\\n",
       "0  -0.575498 -0.146121 -0.626692 -0.535901 -0.551767 -1.287639 -2.573967   \n",
       "1   1.509601  0.578200  1.820134 -1.647458 -0.071796 -1.706009  1.915033   \n",
       "2   0.056639 -0.055050 -0.078291  1.043392 -0.129741 -0.859656  2.065681   \n",
       "3  -0.480006 -1.025328  0.834600 -0.534857  1.319629  0.347435 -1.092914   \n",
       "4   1.974582  0.934887  0.492387  0.299708 -0.164821 -0.199286 -2.334906   \n",
       "5  -0.632991 -0.108827  0.713042 -1.342310 -0.280886 -0.335637 -0.291654   \n",
       "6   1.941582 -1.072269  2.335880  0.780074 -1.008134  2.471164  0.078486   \n",
       "7   0.504721  0.666974  0.705058 -0.106926 -1.420229 -0.684839 -0.969005   \n",
       "8  -0.574361 -0.149573 -0.917420 -0.192925  2.384782 -0.258051  0.739488   \n",
       "9  -1.317225  1.958934 -0.048866  0.814357  0.498720  0.394151 -0.388839   \n",
       "10  0.750649  0.763695 -1.908094 -0.232801 -0.345197 -0.775872  0.065632   \n",
       "11  0.020741  1.266128  0.438592  0.016628  0.948499  0.077433  0.622783   \n",
       "12 -2.039149  0.782383 -0.538083  0.172495  0.534780  2.383925 -0.977100   \n",
       "13 -0.637796 -0.933803 -0.305091  1.369307 -1.012252 -0.656330  0.968510   \n",
       "14 -1.996855  0.476501 -1.465718  1.192161 -1.126743  1.609494  1.025262   \n",
       "15  0.391004  0.524558  0.630045  0.446741  0.082156 -1.018555 -0.126039   \n",
       "16 -2.145634  0.982320  0.526839  0.628083 -0.539414 -0.416067 -0.834330   \n",
       "17  0.270048  1.097327  0.202092 -2.579915 -0.405765 -0.667236  0.056007   \n",
       "18  1.347557  0.950354 -1.960883 -0.125966  1.225044  0.638099  1.112254   \n",
       "19 -0.544082  1.244309 -0.288184  0.603995  1.735645 -0.025411 -1.306812   \n",
       "20 -0.612014 -0.693744  0.951710  2.242447  0.954628  1.170238 -0.032041   \n",
       "21 -0.380467  0.573286 -0.868450 -0.413685  0.602902  0.099522  0.460444   \n",
       "22  1.112564 -2.077295  1.287946 -0.624643 -0.700934 -0.691068  0.761464   \n",
       "23  0.425387 -1.289742 -2.473316  0.203753 -1.373570 -0.182070 -1.725644   \n",
       "24 -0.022023 -2.610980  0.646358 -0.884718  0.532801  2.431324  0.489454   \n",
       "25 -2.062655  0.127407 -0.801511 -0.673290  0.931951 -0.464249 -0.575650   \n",
       "26  0.885348  0.178920  1.429005 -0.375020 -0.292138 -2.309278 -0.824794   \n",
       "27  1.032721  0.012797  0.136196  1.029510 -0.254672  1.829062  1.264786   \n",
       "\n",
       "          21        22        23        24        25        26        27  \n",
       "0   1.392734 -1.165491 -1.160244  0.931404 -0.193218 -0.826734  1.016017  \n",
       "1   0.576252 -0.312639 -1.531277 -1.311846 -0.484592  0.962071 -0.829936  \n",
       "2  -0.354718  0.054670 -0.323433  0.586366 -1.078199  1.216436 -0.414490  \n",
       "3  -0.563440 -0.579150 -0.464330 -0.239029 -0.455215  0.454378 -0.797240  \n",
       "4   1.207757 -0.513191  0.345445 -0.154082 -0.515849  2.296197 -0.860005  \n",
       "5  -0.559678  1.157100 -0.849495  0.100137 -0.738119  0.118803  1.640540  \n",
       "6  -2.125618 -0.701389 -0.567883  0.471079 -0.633024 -1.782633 -0.625173  \n",
       "7  -0.099692  0.688492 -1.181868  1.597547  0.367169  0.980986  0.567836  \n",
       "8  -0.512682  1.380393 -0.821698  0.000989  0.395070  0.819332  0.733975  \n",
       "9  -0.698090  0.139059 -0.508969 -0.181753 -0.667118 -0.838526 -1.399885  \n",
       "10 -1.273640  0.555991 -0.009064  0.463395 -1.451687  0.168665  0.448957  \n",
       "11 -1.115289 -0.901342 -0.361003 -0.023129 -0.865550  0.271549  0.486801  \n",
       "12  0.456526 -1.430741  0.950596  0.539873  0.192685 -0.795366  0.935720  \n",
       "13  0.096117 -1.754314 -1.072766  0.581635  0.234207  0.488491 -1.723799  \n",
       "14 -0.283920 -0.613009  0.352166  0.643955  0.315603 -0.230871 -1.241714  \n",
       "15  0.712315 -0.480034 -2.123426  0.182868  0.012342 -0.277380  0.989977  \n",
       "16 -0.498684 -0.182776  0.416571 -0.328783  0.434364  0.338480  0.562978  \n",
       "17 -1.137569 -0.256704  0.336281  0.541181 -0.275403 -0.178412  1.690404  \n",
       "18  1.135668 -3.053464  1.818821 -0.523055 -1.043691  0.668290  0.254054  \n",
       "19  1.461042 -0.938360 -1.437472 -0.296852 -0.772534 -0.503186  0.724818  \n",
       "20 -0.399675  0.403676 -0.016116  0.074470 -0.499230  2.213183  0.522925  \n",
       "21 -0.013024  0.521336 -0.579375  1.888840  0.424897 -0.203791  1.190353  \n",
       "22  0.626980 -0.235405  0.613879 -0.351539  0.061484  1.513309  0.394492  \n",
       "23  0.185051 -0.295145 -2.074042  0.392135 -0.828411 -0.196392  0.521529  \n",
       "24 -0.044299  1.671273 -2.413034  0.152091  0.411253 -0.537083  0.590947  \n",
       "25 -0.307206  0.890909 -0.029967 -0.566937 -1.385455 -0.441937 -1.333633  \n",
       "26 -0.897320 -0.221581  1.618991  0.973615  0.445378 -0.342946  1.593434  \n",
       "27 -0.522155  0.167175  1.334904  0.907921  1.882873  0.253658  1.953373  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to delve more into the subject, lets say I wish to model a neuron for those 28 inputs:\n",
    "inputs = train_X[0]\n",
    "weights = np.random.randn(28, 28) # Just to visualize on a tabular format, later I'll use the 1d format\n",
    "bias = 3 \n",
    "\n",
    "# And to make sure my weights are consistent with what I expected:\n",
    "pd.DataFrame(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267.95702502978804\n"
     ]
    }
   ],
   "source": [
    "# Now, I'll just connect the input layer with the neuron by making the dot product of both arrays\n",
    "output = np.dot(weights.ravel(), inputs.ravel()) + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3098.26071729  3861.55890338 -1793.36606002]\n"
     ]
    }
   ],
   "source": [
    "# Let's say I'll use 3 neurons connected directly to my inputs. I'd have somenthing like this:\n",
    "inputs = train_X[0]\n",
    "weights = [np.random.randn(784) for _ in range(3)]\n",
    "biases = [np.random.randint(0, 5) for _ in range(3)]\n",
    "\n",
    "# I can connect all neurons by using np.dot\n",
    "outputs = np.dot(weights, inputs.ravel()) + biases\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: weights vector is called first as we need to index the output of the function to EACH neuron. If inputs came first it would throw an error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing in Neural Networks\n",
    "\n",
    "In neural networks, we use something called **batch processing** to train our models effectively.\n",
    "\n",
    "## What is Batch Processing?\n",
    "\n",
    "Instead of throwing all our data at the network at once, we break it into smaller groups called **batches**.\n",
    "\n",
    "## Why Batches?\n",
    "\n",
    "- **Efficiency:** It's easier for computers to handle smaller pieces of data at a time, especially when we have lots of data.\n",
    "\n",
    "- **Generalization:** Batching helps our network learn from a variety of examples in each batch, making it better at understanding the big picture.\n",
    "\n",
    "- **Stability:** The updates to our model are more consistent when we work with batches, reducing randomness.\n",
    "\n",
    "- **Parallelization:** Batches can be processed in parallel, making use of modern hardware like GPUs.\n",
    "\n",
    "- **Regularization:** Batch processing can help prevent our model from overfitting, which means it gets better at handling new data.\n",
    "\n",
    "## Mini-Batch Sizes\n",
    "\n",
    "We choose the size of our batches. Common sizes are 32, 64, or 128, depending on the problem and our computer's power.\n",
    "\n",
    "## Training with Mini-Batches\n",
    "\n",
    "During training, we pass one mini-batch through the network at a time. We calculate how good or bad the model is with that batch and make small improvements.\n",
    "\n",
    "This process repeats for multiple mini-batches until we've gone through all our data, which we call an **epoch**. We usually go through multiple epochs to help the model learn even better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4205.9126391   -332.83101273  -476.62503581]\n",
      " [ 2188.26624927 -2576.92427954  -933.74877862]\n",
      " [ 1864.33274405 -2076.93273269  2397.88119824]]\n"
     ]
    }
   ],
   "source": [
    "# Ok, so now I'll think of my input as a collection of inputs (say a batch of 3)\n",
    "inputs = np.array([train_X[i].ravel() for i in range(3)])\n",
    "\n",
    "weights = np.array([np.random.randn(784) for _ in range(3)])\n",
    "biases = np.array([np.random.randint(0, 5) for _ in range(3)])\n",
    "\n",
    "# Note that I'm still using 3 neurons. The shape of my weights array is (3, 784) while my inputs shape is the same. If I simply dot product them, an error will be raised.\n",
    "# To avoid that, and make sure our math is right, the right thing to do is to transpose my weights matrix!\n",
    "outputs = np.dot(inputs, weights.T) + biases\n",
    "print(outputs) # This should be a 3x3 matrix = output of my neurons for all 3 inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a second neuron layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41331748  0.10499436 -0.9354814 ]\n",
      " [ 1.14937615 -0.32419323  0.31262418]\n",
      " [ 0.43616783  0.05571527 -0.20884038]]\n"
     ]
    }
   ],
   "source": [
    "# Just as I've done before, but more simpler: I have 3 inputs on 3 different batches. I'll just initialize 3 more neurons and biases and calculate the dot product of them.\n",
    "middle_inputs = outputs.copy()\n",
    "\n",
    "weights = np.array([np.random.randn(3) for _ in range(3)])\n",
    "biases = np.array([np.random.randint(0, 2) for _ in range(3)])\n",
    "\n",
    "# My middle inputs shape is (3,3). My weights shape is (3,). We'll keep using the same notation.\n",
    "outputs = np.dot(middle_inputs, weights.T) + biases\n",
    "print(weights) # Which is a 3x3 matrix relative to outputs of the second layer of neurons for the 3 inputs that I'm using!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object-Oriented Neural Network\n",
    "\n",
    "In this project, I'm adopting an object-oriented approach to build and manage our neural network. This involves creating Python objects to represent various components of the neural network, such as layers, weights, biases, and the network itself.\n",
    "\n",
    "## Why Object-Oriented Design?\n",
    "\n",
    "1. **Modularity:** Object-oriented design allows us to create modular and reusable components. Each part of the network can be encapsulated within its own object, making it easier to manage and understand the code.\n",
    "\n",
    "2. **Abstraction:** Objects help in abstracting the complexity of the network. We can define high-level interfaces for each component, hiding intricate details while providing a clean API.\n",
    "\n",
    "3. **Readability:** Object-oriented code tends to be more readable and self-explanatory. This is particularly important when working on complex projects like neural networks.\n",
    "\n",
    "4. **Ease of Debugging:** With clear separation of responsibilities, it's easier to locate and fix issues within specific components of the network.\n",
    "\n",
    "5. **Scalability:** An object-oriented design makes it easier to extend the network by adding new layers, activation functions, and optimizers. This flexibility is crucial when experimenting with different architectures.\n",
    "\n",
    "By creating objects for our neural network components, I aim to make this codebase more organized, maintainable, and scalable. It also helps in abstracting away the intricate details of the network, making it more accessible for future development and collaboration.\n",
    "\n",
    "In the upcoming sections, we will dive into the implementation of these objects, building a neural network from the ground up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm starting to see a conection here. This will be the foundation of creating Layers:\n",
    "class LayerDense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The power of using OOP right now is shown by making everything I did so far on these few lines:\n",
    "np.random.seed(0)\n",
    "\n",
    "first_layer = LayerDense(784, 3) # I have 28*28=784 inputs for 3 neurons\n",
    "second_layer = LayerDense(3, 3) # The output of the first layer is the input of the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 14.80753073  -3.68685917 -32.94804846]\n",
      " [  8.77757203 -41.83978917 -78.26822642]\n",
      " [-17.18374287  34.25673045  81.8390243 ]]\n"
     ]
    }
   ],
   "source": [
    "# And it becomes clear what I need to do now:\n",
    "X = np.array([train_X[i].ravel() for i in range(3)])\n",
    "first_layer.forward(X)\n",
    "second_layer.forward(first_layer.output)\n",
    "print(second_layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Layer Activation Functions\n",
    "\n",
    "In the neural network project, hidden layers play a critical role in learning and extracting complex patterns from data. To introduce non-linearity into the network, activation functions are used within these hidden layers.\n",
    "\n",
    "## What Are Activation Functions?\n",
    "\n",
    "Activation functions are mathematical functions that determine whether a neuron should \"fire\" (become active) based on the weighted sum of its inputs. In other words, they introduce non-linear relationships into the network, allowing it to model more complex patterns.\n",
    "\n",
    "## Why Hidden Layer Activation Functions?\n",
    "\n",
    "- **Non-Linearity:** Hidden layers need to capture non-linear relationships within the data. Activation functions enable neurons to produce non-linear output, which is crucial for solving complex problems.\n",
    "\n",
    "- **Feature Learning:** Different activation functions have different properties, allowing the network to learn and represent features at various scales and complexities.\n",
    "\n",
    "- **Gradient Flow:** Activation functions also help in maintaining a good gradient flow during backpropagation, aiding in efficient and stable training.\n",
    "\n",
    "## Common Activation Functions\n",
    "\n",
    "There are several common activation functions used in hidden layers, including:\n",
    "\n",
    "- **Rectified Linear Unit (ReLU):** A simple and widely used activation function that outputs the input for positive values and zero for negative values.\n",
    "\n",
    "- **Sigmoid:** An S-shaped function that squashes input values into a range between 0 and 1. It's often used in binary classification problems.\n",
    "\n",
    "- **Hyperbolic Tangent (tanh):** Similar to the sigmoid function but with a range between -1 and 1. It helps with zero-centered outputs.\n",
    "\n",
    "- **Leaky ReLU:** An extension of ReLU that allows small negative values to pass through, addressing the \"dying ReLU\" problem.\n",
    "\n",
    "The choice of activation function depends on the problem at hand, and experimentation often helps in determining which one works best for a particular task.\n",
    "\n",
    "In the upcoming sections, activation functions will be implemented within the hidden layers to add non-linearity and enable the project's neural network to learn and represent intricate patterns in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Rectified Linear Activation Function (RELU) is amazing, I'll be implementing that bellow:\n",
    "class activation_ReLU():\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 38.75324019 -43.2537871  -65.89990683]\n",
      " [ 13.27605731 -44.66599785 -74.05566156]\n",
      " [ 14.82255911  -9.31238909  13.88044815]]\n"
     ]
    }
   ],
   "source": [
    "# I pretty much just add the activation function on the output of the first layer:\n",
    "X = np.array([train_X[i].ravel() for i in range(3)])\n",
    "np.random.seed(0)\n",
    "\n",
    "first_layer = LayerDense(784, 3) \n",
    "second_layer = LayerDense(3, 3) \n",
    "\n",
    "activation_1 = activation_ReLU()\n",
    "\n",
    "# And it becomes clear what I need to do now:\n",
    "first_layer.forward(X)\n",
    "activation_1.forward(first_layer.output)\n",
    "\n",
    "\n",
    "second_layer.forward(activation_1.output)\n",
    "print(second_layer.output) # And that is DONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the \"skeleton\" of our model up and running, I'll be looking at the output layer of the model next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Layer and Softmax Activation\n",
    "\n",
    "In the neural network project, the **output layer** is where the network makes its final predictions or classifications. It's a critical part of the network that produces the results we want.\n",
    "\n",
    "## The Role of Softmax Activation\n",
    "\n",
    "To make these predictions, the output layer often employs the **softmax activation function**. The softmax function takes a set of scores or values and transforms them into a probability distribution. \n",
    "\n",
    "- **Normalization:** The softmax function normalizes the scores so that they all add up to 1. This makes it easier to interpret the output as probabilities.\n",
    "\n",
    "- **Multi-Class Classification:** Softmax is commonly used in multi-class classification problems, where there are more than two classes to choose from.\n",
    "\n",
    "The output layer with softmax activation helps our neural network provide clear and interpretable results, making it a key component in many machine learning tasks.\n",
    "\n",
    "In the next sections, we will explore how to implement the output layer and softmax activation function to ensure our neural network produces meaningful and accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 2.42549605e-36 3.54581225e-46]\n",
      " [1.00000000e+00 6.85622677e-26 1.18118086e-38]\n",
      " [7.19525864e-01 2.37340352e-11 2.80474136e-01]]\n"
     ]
    }
   ],
   "source": [
    "# The idea on this activation function is to first exponenciate the result of the last layer and second to normalize it so it falls into a 0 and 1 interval.\n",
    "\n",
    "# Overflow prevention\n",
    "values_test = second_layer.output - np.max(second_layer.output)\n",
    "\n",
    "# First we exponenciate:\n",
    "values_test = np.exp(values_test)\n",
    "\n",
    "# Second we normalize:\n",
    "values_test = values_test / np.sum(values_test, axis = 1, keepdims=True)\n",
    "\n",
    "# We have the following output\n",
    "print(values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now let's translate whats up there into a new class\n",
    "class ActivationSoftMax():\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis = 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 2.42549605e-36 3.54581225e-46]\n",
      " [1.00000000e+00 6.85622677e-26 1.18118086e-38]\n",
      " [7.19525864e-01 2.37340352e-11 2.80474136e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Again, it's just build one on top of the other\n",
    "X = np.array([train_X[i].ravel() for i in range(3)])\n",
    "np.random.seed(0)\n",
    "\n",
    "first_layer = LayerDense(784, 3) \n",
    "second_layer = LayerDense(3, 3) \n",
    "\n",
    "activation_1 = activation_ReLU()\n",
    "activation_softmax = ActivationSoftMax()\n",
    "\n",
    "first_layer.forward(X) # Inputs go to first layer\n",
    "activation_1.forward(first_layer.output) # Then to activation function\n",
    "second_layer.forward(activation_1.output) # Then to second layer\n",
    "\n",
    "# Finally, activation by softmax\n",
    "activation_softmax.forward(second_layer.output)\n",
    "print(activation_softmax.output) # And that is DONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Cross-Entropy Loss\n",
    "\n",
    "In the project's context of classification, we need a way to measure how wrong or right our neural network's predictions are. One of the most widely used loss functions for this purpose is **Categorical Cross-Entropy**.\n",
    "\n",
    "## What is Categorical Cross-Entropy?\n",
    "\n",
    "Categorical Cross-Entropy is a loss function designed for multi-class classification problems, where data can belong to one of several possible classes. Its primary purpose is to quantify the difference between the predicted probability distribution and the actual distribution (one-hot encoded labels).\n",
    "\n",
    "## How It Works\n",
    "\n",
    "Here's how Categorical Cross-Entropy works:\n",
    "\n",
    "- For each example, the network produces a probability distribution over all the possible classes using the softmax function in the output layer.\n",
    "\n",
    "- The actual label is represented as a one-hot encoded vector, where only one element is 1 (the correct class) and the rest are 0.\n",
    "\n",
    "- Categorical Cross-Entropy measures the dissimilarity between the predicted distribution and the actual label by summing up the differences.\n",
    "\n",
    "- It penalizes incorrect predictions more severely, encouraging the network to become more accurate in its classifications.\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "In essence, Categorical Cross-Entropy quantifies how \"surprised\" the network is by the actual class compared to its prediction. If the prediction is perfectly accurate, the loss is zero. The higher the loss, the more different the prediction is from the actual label.\n",
    "\n",
    "## Training Objective\n",
    "\n",
    "During the training process, our goal is to minimize the Categorical Cross-Entropy loss. This means finding the network's parameters (weights and biases) that result in the most accurate predictions.\n",
    "\n",
    "In the following sections, we will implement Categorical Cross-Entropy as the loss function for our neural network and use it to guide the training process, helping the network learn and make better classifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99752638e-01 2.42489607e-36 3.54493515e-46 2.15916838e-04\n",
      "  7.92027664e-26 3.14446724e-05 4.35590025e-11 2.12793591e-43\n",
      "  1.12855400e-60 6.08625859e-35]\n",
      " [3.18529382e-33 3.19146442e-28 2.83064991e-12 1.42726252e-20\n",
      "  1.28391390e-20 8.92143604e-04 1.69019781e-08 4.68454040e-02\n",
      "  1.36311083e-22 9.52262436e-01]\n",
      " [2.82193134e-12 7.09890074e-22 1.82950591e-17 1.02951965e-23\n",
      "  4.36150060e-11 1.06366587e-11 3.89786604e-16 1.42263235e-01\n",
      "  8.57736714e-01 5.06745797e-08]]\n"
     ]
    }
   ],
   "source": [
    "# First, I'll have to adapt my model as my objective is to give classifications to 10 different outcomes (all numbers)\n",
    "\n",
    "# Won't change what I had essencially\n",
    "X = np.array([train_X[i].ravel() for i in range(3)])\n",
    "np.random.seed(0)\n",
    "\n",
    "first_layer = LayerDense(784, 3) \n",
    "second_layer = LayerDense(3, 10) # Only now I will have 10 outputs from the second layer\n",
    "\n",
    "activation_1 = activation_ReLU()\n",
    "activation_softmax = ActivationSoftMax()\n",
    "\n",
    "first_layer.forward(X)\n",
    "activation_1.forward(first_layer.output) \n",
    "second_layer.forward(activation_1.output) \n",
    "\n",
    "# Finally, activation by softmax\n",
    "activation_softmax.forward(second_layer.output)\n",
    "print(activation_softmax.output) # And that is DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([train_y[i] for i in range(3)]) # This is my y vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.36728099 74.82676353 23.85561985]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array(activation_softmax.output)\n",
    "\n",
    "# I wish to get only the outputs related to the correct classification, hence:\n",
    "print(-np.log(softmax_outputs[range(len(softmax_outputs)), y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.34988812436632\n"
     ]
    }
   ],
   "source": [
    "# Which is:\n",
    "neg_loss = -np.log(softmax_outputs[range(len(softmax_outputs)), y])\n",
    "average_loss = np.mean(neg_loss)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue is due to happen on this case. If confidence of predicted value is 0, log(0) -> inf. And if there is an infinite \"value\", the mean of an infinite value will be infinite.\n",
    "\n",
    "Because of this, it is interesting to clip our confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.34988812436632\n"
     ]
    }
   ],
   "source": [
    "# Something of the sorts:\n",
    "predictions = np.clip(softmax_outputs, 1e-50, 1 - 1e-50)\n",
    "neg_loss = -np.log(predictions[range(len(softmax_outputs)), y])\n",
    "average_loss = np.mean(neg_loss)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement it as a class\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        batch_loss = np.mean(sample_losses)\n",
    "        return batch_loss\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Now a workaround if y_true is one hot encoded or not:\n",
    "        if len(y_true.shape) == 1:\n",
    "            confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        else:\n",
    "            confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        negative_log_likelihood = -np.log(confidences)\n",
    "        return negative_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1948777644199895\n"
     ]
    }
   ],
   "source": [
    "# IT IS BUILD TIME!\n",
    "\n",
    "X = np.array([train_X[i].ravel() for i in range(3)])\n",
    "y = np.array([train_y[i] for i in range(3)])\n",
    "\n",
    "first_layer = LayerDense(784, 3) \n",
    "second_layer = LayerDense(3, 10)\n",
    "\n",
    "activation_1 = activation_ReLU()\n",
    "activation_softmax = ActivationSoftMax()\n",
    "\n",
    "first_layer.forward(X)\n",
    "activation_1.forward(first_layer.output) \n",
    "second_layer.forward(activation_1.output) \n",
    "\n",
    "activation_softmax.forward(second_layer.output)\n",
    "\n",
    "# Calculate the loss:\n",
    "cross_entropy_loss = CrossEntropyLoss()\n",
    "\n",
    "loss_value = cross_entropy_loss.calculate(activation_softmax.output, y)\n",
    "print(loss_value) # And that is DONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization in Neural Networks\n",
    "\n",
    "Optimization is a crucial step in training neural networks to make accurate predictions or classifications. Key concepts include **backpropagation** and **gradient descent**, which are fundamental to improving the network's performance.\n",
    "\n",
    "## 1. Backpropagation\n",
    "\n",
    "**Backpropagation** is the process of computing the gradients of the loss function with respect to the network's parameters. It's a crucial step in the training process and works in reverse through the network's layers. Here are the key points:\n",
    "\n",
    "- **Forward Pass:** During the forward pass, input data is propagated through the network to make predictions.\n",
    "\n",
    "- **Loss Calculation:** The loss (error) between the predictions and the actual target values is calculated.\n",
    "\n",
    "- **Backward Pass:** In the backward pass, the gradients of the loss with respect to the network's parameters are calculated. This helps us understand how changes in each parameter affect the loss.\n",
    "\n",
    "- **Gradient Descent:** The gradients obtained from backpropagation guide the update of network parameters in the direction that reduces the loss. Gradient descent is a common optimization algorithm used for this purpose.\n",
    "\n",
    "## 2. Gradient Descent\n",
    "\n",
    "**Gradient Descent** is an iterative optimization algorithm that minimizes the loss function by adjusting the network's parameters in small steps. Key points to understand:\n",
    "\n",
    "- **Learning Rate:** The learning rate is a hyperparameter that determines the step size during parameter updates. It affects the speed and stability of training.\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD):** In standard gradient descent, all training examples are used in each iteration. In SGD, only a random subset (mini-batch) of examples is used, which speeds up training.\n",
    "\n",
    "- **Adaptive Methods:** Variants of gradient descent, such as Adam and RMSprop, adjust the learning rate during training to improve convergence.\n",
    "\n",
    "## 3. Mini-Batch Training\n",
    "\n",
    "Instead of processing all data at once, neural networks are often trained on smaller subsets called **mini-batches**. Mini-batch training provides several benefits, including improved efficiency, regularization, and better gradient estimates.\n",
    "\n",
    "## 4. Local Minima\n",
    "\n",
    "Optimization in neural networks can get stuck in local minima, where the loss function stops improving. Techniques like **momentum** and **learning rate schedules** are used to escape these local minima and reach better solutions.\n",
    "\n",
    "In summary, optimization in neural networks involves backpropagation to compute gradients and gradient descent to update parameters. Training typically uses mini-batches to improve efficiency. Handling local minima and choosing appropriate optimization techniques are also important for successful training.\n",
    "\n",
    "In the following sections, we will delve into the implementation of these optimization concepts to train our neural network effectively.\n",
    "\n",
    "\n",
    "## Calculus and Optimizing the Neural network\n",
    "\n",
    "Before I start experimenting with backpropagation, one thing I must really test is my calculus knowledge right now.\n",
    "\n",
    "The main objective here is quite simple: where should my optimizer adjust that will give conctrete results on reducing my loss function?\n",
    "\n",
    "The question is quite basic if I look at a single layer of a neural network and worry about how a specific output is being propagated.\n",
    "\n",
    "I know that on the low level of the model, what is being done is the following operation:\n",
    "\n",
    "$y_i = x_i * weight_i$\n",
    "\n",
    "But it's complexity goes further than that, as we sum the result of every x*weight:\n",
    "\n",
    "$\\sum y_i = \\sum x_i * w_i = w_i$\n",
    "\n",
    "As w is the result of the dot product of inputs and weights, we add the bias and the outcome is the output of the neuron:\n",
    "\n",
    "$\\sum x_i * w_i + bias$\n",
    "\n",
    "This is feeded into the ReLu function, which is basically a $max(x, 0)$ function:\n",
    "\n",
    "$z = ReLu(\\sum x_i * w_i + bias)$\n",
    "\n",
    "### The parcial derivatives\n",
    "\n",
    "Now that I've defined z as the output of the Rectified Linear Unit RELU, I'll delve into the partial derivatives of this function.\n",
    "\n",
    "First, the partial derivative of the ReLu function is defined as:\n",
    "\n",
    "$\\frac{\\partial}{\\partial z}f(z) = 1 (x > 0)$\n",
    "\n",
    "Also, $z = (\\sum x_i * w_i + bias)$ which means, by the chain rule:\n",
    "\n",
    "$\\frac{\\partial}{\\partial z}f(z) = \\frac{\\partial}{\\partial z}f(z) * \\frac{\\partial}{\\partial w}f(w + bias)$\n",
    "\n",
    "As w is a partial derivative of $\\sum y_i$, and partial derivative of a sum will always result on 1, we can simplify the expression by:\n",
    "\n",
    "$\\frac{\\partial}{\\partial z}f(z) = 1 * \\frac{\\partial}{\\partial z}f(z)$\n",
    "\n",
    "The result above is the result of the fist **backpropagation!** Let's keep up now by going further into the functions and calculate de partial derivative of the output of the neuron:\n",
    "\n",
    "$\\frac{\\partial}{\\partial x_i w_i}f(\\sum (x_i w_i) + bias) = 1$\n",
    "\n",
    "And we should also consider the partial derivative of the bias on this case:\n",
    "\n",
    "$\\frac{\\partial}{\\partial bias}f(\\sum (x_i w_i) + bias) = 1$\n",
    "\n",
    "The last function inside a layer is plainly simple:\n",
    "\n",
    "$\\frac{\\partial}{\\partial x_i}f(x_i * w_i) = w_i$\n",
    "\n",
    "$\\frac{\\partial}{\\partial w_i}f(x_i * w_i) = x_i$\n",
    "\n",
    "All of these combined make possible to calculate the gradient of the whole layer:\n",
    "\n",
    "$dx = \\frac{\\partial}{\\partial x_i}f(ReLu(z))$ \n",
    "\n",
    "$dw = \\frac{\\partial}{\\partial w_i}f(ReLu(z))$ \n",
    "\n",
    "$db = \\frac{\\partial}{\\partial b_i}f(ReLu(z))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything I have so far...\n",
    "class LayerDense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class activation_ReLU():\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "class ActivationSoftMax():\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis = 1, keepdims=True)\n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        batch_loss = np.mean(sample_losses)\n",
    "        return batch_loss\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Now a workaround if y_true is one hot encoded or not:\n",
    "        if len(y_true.shape) == 1:\n",
    "            confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        else:\n",
    "            confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        negative_log_likelihood = -np.log(confidences)\n",
    "        return negative_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing I'll have to add is a backward method for LayerDense class:\n",
    "class LayerDense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues): #dvalues are calculated on the ReLu activation phase\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.biases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "# And now for the ReLu class:\n",
    "class activation_ReLU():\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # This is the simple derivative where we nullify all values that were negative before the ReLu activation:\n",
    "        self.dinputs[inputs < 0 ] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of the Categorical Cross Entropy Loss function\n",
    "\n",
    "The Categorical Cross Entropy Loss function is defined by:\n",
    "\n",
    "$L_i = -\\sum y_{i, j} log(\\hat y_{i, j})$\n",
    "\n",
    "It's parcial derivative is:\n",
    "\n",
    "$\\frac{\\partial L_i}{\\partial \\hat y_{i, j}} = -\\sum y_{i, j} \\frac{\\partial}{\\partial \\hat y_{i, j}} log(\\hat y_{i, j}) = -\\sum y_{i, j} \\frac{1}{\\hat y_{i, j}} = - \\sum \\frac{y_{i, j}}{\\hat y_{i, j}}$\n",
    "\n",
    "As only one argument of this sum will be diffent than 0 (we will only consider the true label category):\n",
    "\n",
    "$\\frac{\\partial L_i}{\\partial \\hat y_{i, j}} = - \\frac{y_{i, j}}{\\hat y_{i, j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating this equation into a method:\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Now a workaround if y_true is one hot encoded or not:\n",
    "        if len(y_true.shape) == 1:\n",
    "            confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        else:\n",
    "            confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        negative_log_likelihood = -np.log(confidences)\n",
    "        return negative_log_likelihood\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        n_samples = len(dvalues)\n",
    "\n",
    "        # Number of labels\n",
    "        n_labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(n_labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = - y_true / dvalues\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / n_samples # This is important so when I implement the optimizer the gradient sum doesn't explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of the SoftMax activation function\n",
    "\n",
    "The SoftMax activation function is defined by:\n",
    "\n",
    "$S_{i, j} = \\frac{e^{z_{i j}}}{\\sum e^{z_{i l}}}$\n",
    "\n",
    "It's derivative is a bit complicated, and is as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_{i, j}}{\\partial z_{i, k}} = \\frac{\\partial \\frac{e^{z_{i j}}}{\\sum e^{z_{i l}}}}{\\partial z_{i, k}} = \\frac{\\frac{\\partial}{\\partial z_{i, k}}e^{z_{i j}}\\sum e^{z_{i l}}- e^{z_{i j}}\\frac{\\partial}{\\partial z_{i, k}} \\sum e^{z_{i l}}}{[\\sum e^{z_{i l}}]^2}\n",
    "$$\n",
    "\n",
    "As $\\frac{\\partial}{\\partial x} e^x = e^x$, and if $j = k \\rightarrow \\frac{\\partial}{\\partial z_{i, k}} \\sum e^{z_{i l}} = e^{z_{i l}}$. So:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_{i, j}}{\\partial z_{i, k}} = \\frac{e^{z_{i j}} \\sum e^{z_{i l}}- e^{z_{i j}} e^{z_{i l}}}{[\\sum e^{z_{i l}}]^2} = \\frac{e^{z_{i j}} (\\sum e^{z_{i l}} - e^{z_{i l}})}{\\sum e^{z_{i l}} \\sum e^{z_{i l}}}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_{i, j}}{\\partial z_{i, k}} = \\frac{e^{z_{i j}}}{\\sum e^{z_{i l}}} \\cdot \\frac{\\sum e^{z_{i l}} - e^{z_{i l}}}{\\sum e^{z_{i l}}} = S_{i, j} \\cdot (1 - S_{i, k})\n",
    "$$\n",
    "\n",
    "But if $j \\neq k \\rightarrow \\frac{\\partial}{\\partial z_{i, k}} \\sum e^{z_{i l}} = 0$. Then (skipping to end of simplification):\n",
    "\n",
    "$$\n",
    "-S_{i, j} \\cdot S_{i, k}\n",
    "$$\n",
    "\n",
    "\n",
    "But as there are 2 different scenarios to code now, it's not very interesting speedwise. We can morph both scenarios by using the following Kronecker delta:\n",
    "\n",
    "If $i = j \\rightarrow \\delta _{j,k} = 1$, else: $\\delta _{j,k} = 0$ \n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_{i, j}}{\\partial z_{i, k}} = S_{i, j} \\cdot (\\delta _{j,k} - S_{i, k}) = S_{i, j}\\delta _{j,k} - S_{i, j} S_{i, k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the softmax function is: [[0.7 0.1 0.2]]\n",
      "This is the output when delta = 1 and 0 otherwise:\n",
      "[[0.7 0.  0. ]\n",
      " [0.  0.1 0. ]\n",
      " [0.  0.  0.2]]\n",
      "This is the product of the right side of the equation:\n",
      "[[0.49 0.07 0.14]\n",
      " [0.07 0.01 0.02]\n",
      " [0.14 0.02 0.04]]\n",
      "This is the resulting gradient of the softmax function:\n",
      "[[ 0.21 -0.07 -0.14]\n",
      " [-0.07  0.09 -0.02]\n",
      " [-0.14 -0.02  0.16]]\n"
     ]
    }
   ],
   "source": [
    "# I'll try to reproduce a book problem on this now\n",
    "\n",
    "# Say we have the following\n",
    "softmax_output = np.array([0.7, 0.1, 0.2]).reshape(-1, 1)\n",
    "print(f\"The output of the softmax function is: {softmax_output.T}\")\n",
    "\n",
    "# Below is the same as softmax_output * np.eye(softmax_output.shape[0]) but faster\n",
    "print(\"This is the output when delta = 1 and 0 otherwise:\")\n",
    "print(np.diagflat(softmax_output))\n",
    "\n",
    "print(\"This is the product of the right side of the equation:\"),\n",
    "print(np.dot(softmax_output, softmax_output.T))\n",
    "\n",
    "print(\"This is the resulting gradient of the softmax function:\")\n",
    "print(np.diagflat(softmax_output) - np.dot(softmax_output, softmax_output.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix result of the equation and the array solution provided by the code is called the\n",
    "Jacobian matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.17315525e-62 -2.32094331e-30 -4.40044283e-42 -3.78565136e-67\n",
      "  -3.25928993e-25  8.95330501e-02 -3.36217519e-47 -8.95330501e-02\n",
      "  -6.02343956e-13 -1.36078008e-28]\n",
      " [-6.76665981e-08 -1.36219860e-06  6.49922806e-02 -2.04837552e-06\n",
      "  -2.72416100e-04 -3.68144946e-06 -5.50163737e-08 -5.02513102e-04\n",
      "  -6.08142136e-02 -3.39592306e-03]\n",
      " [-3.03576608e-18 -3.03576608e-18 -3.03576608e-18 -3.03576608e-18\n",
      "  -1.30104261e-18 -1.30104261e-18 -1.30104261e-18 -1.30104261e-18\n",
      "  -1.73472348e-18 -3.46944695e-18]]\n"
     ]
    }
   ],
   "source": [
    "# Again, I'll repeat the process to the previous output of the batch of 3 inputs I had:\n",
    "\n",
    "softmax_output = activation_softmax.output\n",
    "# print(softmax_output)\n",
    "\n",
    "dinputs = np.empty_like(softmax_output) # This creates the vector without the need to initialize its values.\n",
    "\n",
    "# Because it's now on a batch format, I'll loop over it:\n",
    "for index, (single_output, single_dvalues) in enumerate(zip(softmax_output, softmax_output)):\n",
    "    single_output = single_output.reshape(-1, 1)\n",
    "    jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "    dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "print(dinputs) # This is the jacobian matrix for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's code this solution on the SoftMax activation class\n",
    "class ActivationSoftMax():\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis = 1, keepdims=True)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            \n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, there is a efficient combination of the SoftMax activation and the loss function that improves a lot computing time. I won't delve into the mathematics of this derivative, but I'll implement it's solution either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "# Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = ActivationSoftMax()\n",
    "        self.loss = CrossEntropyLoss()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this is all I have so far:\n",
    "\n",
    "class LayerDense():\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues): #dvalues are calculated on the ReLu activation phase\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class activation_ReLU():\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs < 0 ] = 0\n",
    "    \n",
    "class ActivationSoftMax():\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis = 1, keepdims=True)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            \n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Now a workaround if y_true is one hot encoded or not:\n",
    "        if len(y_true.shape) == 1:\n",
    "            confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        else:\n",
    "            confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        negative_log_likelihood = -np.log(confidences)\n",
    "        return negative_log_likelihood\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        n_samples = len(dvalues)\n",
    "\n",
    "        # Number of labels\n",
    "        n_labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(n_labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = - y_true / dvalues\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / n_samples # This is important so when I implement the optimizer the gradient sum doesn't explode\n",
    "\n",
    "# Softmax classifier - combined Softmax activation and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "# Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = ActivationSoftMax()\n",
    "        self.loss = CrossEntropyLoss()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: combined loss and activation:\n",
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n",
      "Gradients: separate loss and activation:\n",
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n"
     ]
    }
   ],
   "source": [
    "# Lets test if both gradient calculations result in the same thing\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],[0.1, 0.5, 0.4],[0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# The gradient using the \"faster\" form\n",
    "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "softmax_loss.backward(softmax_outputs, class_targets)\n",
    "dvalues1 = softmax_loss.dinputs \n",
    "\n",
    "# Using the conventional form\n",
    "activation = ActivationSoftMax()\n",
    "activation.output = softmax_outputs\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "loss.backward(softmax_outputs, class_targets)\n",
    "activation.backward(loss.dinputs)\n",
    "dvalues2 = activation.dinputs\n",
    "\n",
    "\n",
    "print('Gradients: combined loss and activation:')\n",
    "print(dvalues1)\n",
    "print('Gradients: separate loss and activation:')\n",
    "print(dvalues2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[0.00221845 0.04072283 0.0087886 ]]\n",
      "[[ 1.16878064e-25  1.08538732e+02  6.06849177e-28  2.44024133e-04\n",
      "   1.06012239e-48 -1.08559656e+02  4.77552527e-39  2.06802600e-02\n",
      "   6.49818763e-09  4.97039408e-37]\n",
      " [ 2.16382372e+01  8.68177622e-03  7.07925615e-05  2.24179430e-02\n",
      "  -2.21475181e+01  9.41304371e-03  9.56548694e-05  3.63076290e-04\n",
      "   4.29498987e-05  4.68195696e-01]\n",
      " [ 3.91841144e-26  3.63883002e+01  2.03450046e-28  8.18106425e-05\n",
      "   3.55412775e-49 -3.63953152e+01  1.60102522e-39  6.93318870e-03\n",
      "   2.17855873e-09  1.66635623e-37]]\n",
      "[[ 0.02558238  0.36473206  0.0333344   0.03367139 -0.29991201 -0.29785699\n",
      "   0.03333477  0.03340191  0.03333398  0.0403781 ]]\n"
     ]
    }
   ],
   "source": [
    "# And it's time to test on real data!\n",
    "X = np.array([train_X[i].ravel() for i in range(3)])\n",
    "y = np.array([train_y[i] for i in range(3)])\n",
    "\n",
    "first_layer = LayerDense(784, 3) \n",
    "second_layer = LayerDense(3, 10)\n",
    "\n",
    "activation_1 = activation_ReLU()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform first forward pass into first layer\n",
    "first_layer.forward(X)\n",
    "activation_1.forward(first_layer.output) # Activate neurons based on ReLu activation function\n",
    "\n",
    "# Perform second forward pass into second layer\n",
    "second_layer.forward(activation_1.output) \n",
    "\n",
    "# Perform forward pass into activation/loss function:\n",
    "loss = loss_activation.forward(second_layer.output, y)\n",
    "\n",
    "# print(loss_activation.output[0]) # And that is done with forward pass.\n",
    "# print(\"loss:\", loss)\n",
    "\n",
    "# Now on the backward pass:\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "second_layer.backward(loss_activation.dinputs)\n",
    "activation_1.backward(second_layer.dinputs)\n",
    "first_layer.backward(activation_1.dinputs)\n",
    "\n",
    "# Print gradients\n",
    "print(first_layer.dweights)\n",
    "print(first_layer.dbiases)\n",
    "print(second_layer.dweights)\n",
    "print(second_layer.dbiases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Accuracy from model is \n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a popular optimization algorithm used in training neural networks. It's a variant of the gradient descent algorithm that offers several advantages, especially for large datasets. Here's a brief overview:\n",
    "\n",
    "## Key Features:\n",
    "\n",
    "1. **Minibatch Processing:** Instead of using the entire dataset in each iteration, SGD randomly selects a small subset (minibatch) of data samples. This speeds up training and often results in faster convergence.\n",
    "\n",
    "2. **Stochastic Updates:** In each iteration, SGD updates the model's parameters (weights and biases) based on the gradient computed for the current minibatch. This introduces randomness, which can help escape local minima.\n",
    "\n",
    "3. **Noisy but Efficient:** The stochastic nature of SGD introduces noise in parameter updates, but this noise can have a regularizing effect and help the model generalize better.\n",
    "\n",
    "4. **Learning Rate:** The learning rate in SGD determines the step size for parameter updates. It's a critical hyperparameter to fine-tune.\n",
    "\n",
    "5. **Convergence:** SGD may not converge to the global minimum but often finds a good solution in a reasonable time.\n",
    "\n",
    "## Benefits:\n",
    "\n",
    "- Well-suited for large datasets.\n",
    "- Faster convergence due to minibatch processing.\n",
    "- Effective regularization through noise.\n",
    "- Scalable to deep neural networks.\n",
    "\n",
    "## Challenges:\n",
    "\n",
    "- Noisy updates may require careful learning rate tuning.\n",
    "- May not converge to the global minimum.\n",
    "\n",
    "SGD is the foundation for many advanced optimization algorithms in deep learning, making it a crucial tool for training neural networks efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.082, loss: 14.479\n",
      "epoch: 100, acc: 0.290, loss: 1.925\n",
      "epoch: 200, acc: 0.359, loss: 1.729\n",
      "epoch: 300, acc: 0.445, loss: 1.563\n",
      "epoch: 400, acc: 0.518, loss: 1.377\n",
      "epoch: 500, acc: 0.566, loss: 1.247\n",
      "epoch: 600, acc: 0.581, loss: 1.196\n",
      "epoch: 700, acc: 0.592, loss: 1.155\n",
      "epoch: 800, acc: 0.596, loss: 1.133\n",
      "epoch: 900, acc: 0.603, loss: 1.105\n",
      "epoch: 1000, acc: 0.611, loss: 1.072\n",
      "epoch: 1100, acc: 0.620, loss: 1.048\n",
      "epoch: 1200, acc: 0.622, loss: 1.032\n",
      "epoch: 1300, acc: 0.627, loss: 1.010\n",
      "epoch: 1400, acc: 0.629, loss: 0.998\n",
      "epoch: 1500, acc: 0.632, loss: 0.983\n",
      "epoch: 1600, acc: 0.636, loss: 0.969\n",
      "epoch: 1700, acc: 0.639, loss: 0.957\n",
      "epoch: 1800, acc: 0.642, loss: 0.946\n",
      "epoch: 1900, acc: 0.644, loss: 0.935\n",
      "epoch: 2000, acc: 0.648, loss: 0.923\n",
      "epoch: 2100, acc: 0.651, loss: 0.914\n",
      "epoch: 2200, acc: 0.653, loss: 0.905\n",
      "epoch: 2300, acc: 0.655, loss: 0.896\n",
      "epoch: 2400, acc: 0.658, loss: 0.888\n",
      "epoch: 2500, acc: 0.661, loss: 0.879\n",
      "epoch: 2600, acc: 0.664, loss: 0.874\n",
      "epoch: 2700, acc: 0.665, loss: 0.867\n",
      "epoch: 2800, acc: 0.667, loss: 0.862\n",
      "epoch: 2900, acc: 0.669, loss: 0.856\n",
      "epoch: 3000, acc: 0.670, loss: 0.851\n",
      "epoch: 3100, acc: 0.671, loss: 0.847\n",
      "epoch: 3200, acc: 0.672, loss: 0.843\n",
      "epoch: 3300, acc: 0.671, loss: 0.838\n",
      "epoch: 3400, acc: 0.668, loss: 0.838\n",
      "epoch: 3500, acc: 0.673, loss: 0.831\n",
      "epoch: 3600, acc: 0.668, loss: 0.834\n",
      "epoch: 3700, acc: 0.676, loss: 0.824\n",
      "epoch: 3800, acc: 0.675, loss: 0.819\n",
      "epoch: 3900, acc: 0.678, loss: 0.815\n",
      "epoch: 4000, acc: 0.676, loss: 0.812\n",
      "epoch: 4100, acc: 0.678, loss: 0.808\n",
      "epoch: 4200, acc: 0.666, loss: 0.832\n",
      "epoch: 4300, acc: 0.682, loss: 0.801\n",
      "epoch: 4400, acc: 0.684, loss: 0.798\n",
      "epoch: 4500, acc: 0.683, loss: 0.802\n",
      "epoch: 4600, acc: 0.686, loss: 0.794\n",
      "epoch: 4700, acc: 0.686, loss: 0.791\n",
      "epoch: 4800, acc: 0.686, loss: 0.788\n",
      "epoch: 4900, acc: 0.687, loss: 0.784\n",
      "epoch: 5000, acc: 0.679, loss: 0.789\n",
      "epoch: 5100, acc: 0.689, loss: 0.784\n",
      "epoch: 5200, acc: 0.689, loss: 0.776\n",
      "epoch: 5300, acc: 0.690, loss: 0.773\n",
      "epoch: 5400, acc: 0.691, loss: 0.770\n",
      "epoch: 5500, acc: 0.683, loss: 0.786\n",
      "epoch: 5600, acc: 0.691, loss: 0.767\n",
      "epoch: 5700, acc: 0.691, loss: 0.766\n",
      "epoch: 5800, acc: 0.692, loss: 0.764\n",
      "epoch: 5900, acc: 0.686, loss: 0.770\n",
      "epoch: 6000, acc: 0.699, loss: 0.754\n",
      "epoch: 6100, acc: 0.701, loss: 0.746\n",
      "epoch: 6200, acc: 0.706, loss: 0.739\n",
      "epoch: 6300, acc: 0.708, loss: 0.733\n",
      "epoch: 6400, acc: 0.712, loss: 0.724\n",
      "epoch: 6500, acc: 0.719, loss: 0.710\n",
      "epoch: 6600, acc: 0.730, loss: 0.689\n",
      "epoch: 6700, acc: 0.732, loss: 0.682\n",
      "epoch: 6800, acc: 0.732, loss: 0.682\n",
      "epoch: 6900, acc: 0.751, loss: 0.644\n",
      "epoch: 7000, acc: 0.755, loss: 0.636\n",
      "epoch: 7100, acc: 0.757, loss: 0.631\n",
      "epoch: 7200, acc: 0.760, loss: 0.621\n",
      "epoch: 7300, acc: 0.762, loss: 0.617\n",
      "epoch: 7400, acc: 0.766, loss: 0.613\n",
      "epoch: 7500, acc: 0.763, loss: 0.609\n",
      "epoch: 7600, acc: 0.763, loss: 0.607\n",
      "epoch: 7700, acc: 0.764, loss: 0.605\n",
      "epoch: 7800, acc: 0.767, loss: 0.600\n",
      "epoch: 7900, acc: 0.765, loss: 0.597\n",
      "epoch: 8000, acc: 0.769, loss: 0.595\n",
      "epoch: 8100, acc: 0.764, loss: 0.594\n",
      "epoch: 8200, acc: 0.765, loss: 0.594\n",
      "epoch: 8300, acc: 0.770, loss: 0.591\n",
      "epoch: 8400, acc: 0.766, loss: 0.589\n",
      "epoch: 8500, acc: 0.768, loss: 0.587\n",
      "epoch: 8600, acc: 0.764, loss: 0.591\n",
      "epoch: 8700, acc: 0.768, loss: 0.582\n",
      "epoch: 8800, acc: 0.769, loss: 0.581\n",
      "epoch: 8900, acc: 0.772, loss: 0.579\n",
      "epoch: 9000, acc: 0.770, loss: 0.579\n",
      "epoch: 9100, acc: 0.769, loss: 0.578\n",
      "epoch: 9200, acc: 0.769, loss: 0.582\n",
      "epoch: 9300, acc: 0.764, loss: 0.582\n",
      "epoch: 9400, acc: 0.764, loss: 0.582\n",
      "epoch: 9500, acc: 0.773, loss: 0.575\n",
      "epoch: 9600, acc: 0.769, loss: 0.576\n",
      "epoch: 9700, acc: 0.773, loss: 0.575\n",
      "epoch: 9800, acc: 0.774, loss: 0.573\n",
      "epoch: 9900, acc: 0.775, loss: 0.572\n",
      "epoch: 10000, acc: 0.774, loss: 0.572\n"
     ]
    }
   ],
   "source": [
    "# And we use this class as another building block inside a loop.\n",
    "X = np.array([train_X[i].ravel() for i in range(5000)])\n",
    "y = np.array([train_y[i] for i in range(5000)])\n",
    "\n",
    "# Create layers\n",
    "first_layer = LayerDense(784, 37) \n",
    "second_layer = LayerDense(37, 10)\n",
    "\n",
    "# Initialize activation/loss\n",
    "activation_1 = activation_ReLU()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate = 0.01)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(0, 10001):\n",
    "\n",
    "    # Perform first forward pass into first layer\n",
    "    first_layer.forward(X)\n",
    "    activation_1.forward(first_layer.output) # Activate neurons based on ReLu activation function\n",
    "\n",
    "    # Perform second forward pass into second layer\n",
    "    second_layer.forward(activation_1.output) \n",
    "\n",
    "    # Perform forward pass into activation/loss function:\n",
    "    loss = loss_activation.forward(second_layer.output, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f}')\n",
    "\n",
    "    # Now on the backward pass:\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    second_layer.backward(loss_activation.dinputs)\n",
    "    activation_1.backward(second_layer.dinputs)\n",
    "    first_layer.backward(activation_1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(first_layer)\n",
    "    optimizer.update_params(second_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are already starting to take shape. I'll keep on progressing on how to improve with what I have so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbfbd03a87395819c26610a7c68c8cab9bc3d3efa5727999ef7714fdb2cb4f26"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit ('3.10.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
